{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer.pad_token =  tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://rome.baulab.info/data/dsets/known_1000.json'\n",
    "response = requests.get(url) \n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [dict['prompt'] for dict in data]\n",
    "subjects = [' '+dict['subject'] for dict in data]\n",
    "input= tokenizer(prompts, return_tensors=\"pt\", padding= True, return_offsets_mapping= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 0,  ..., 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = []\n",
    "for j, prompt in enumerate(prompts):\n",
    "    map = torch.zeros_like(input.input_ids[j], dtype=torch.int)\n",
    "    for i,t in enumerate(input.offset_mapping[j]):\n",
    "        \n",
    "        if (prompts[j].find(subjects[j])-1<=t[0]) and (t[1]<=prompts[j].find(subjects[j])+len(subjects[j])):\n",
    "            map[i] = 1\n",
    "    mask.append(map)\n",
    "subject_mask = torch.stack(mask)\n",
    "subject_mask = torch.logical_and(subject_mask, input.attention_mask).int()\n",
    "subject_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_non_padding_token_logits(logits, attention_mask):\n",
    "    # For each input, find the last non-padding token\n",
    "    last_non_padding_logits = []\n",
    "    \n",
    "    for i in range(logits.size(0)):  # Loop over each prompt in the batch\n",
    "        # Find the last non-padding token position\n",
    "        non_padding_positions = (attention_mask[i] == 1).nonzero(as_tuple=True)[0]\n",
    "        last_non_padding_token_index = non_padding_positions[-1]\n",
    "        \n",
    "        # Get the logits of the last non-padding token\n",
    "        last_non_padding_logits.append(logits[i, last_non_padding_token_index])\n",
    "    last_non_padding_logits = torch.stack(last_non_padding_logits)\n",
    "    return last_non_padding_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**input, labels = input.input_ids, output_hidden_states = True, output_attentions =False) #REMETTRE TRUE\n",
    "logits = last_non_padding_token_logits(output.logits,input.attention_mask)\n",
    "probs = F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prba avec clean run pour chaque prompt\n",
    "probs_clean = probs.gather(1, torch.tensor(tokenizer([' '+dict['attribute'] for dict in data])['input_ids'])).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.hidden_states[layer][prompt][token] =vecteur hidden state du token dans le prompt pour le layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restaured run pour le dernier token sujet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalement l'output d'un transformer block devrait être (nombre de prompt, nombre de token, taille d'un vecteur qui représente un token), mais bizarrement on a un tuple avec le premier element qui est bien qqchose de cette forme mais le deuxième élément est un tuple de deux truc de la forme  (nombre de prompt, a,nombre de token, b) avec ab = taille d'un vecteur qui représente un token; dans un premier temps on va juste modifier le premier élement du tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restauration_hook(clean_states_layer, subject_mask):\n",
    "    #get la last position de chaque subject token ie. lastones[prompt] donne la position du dernier token se référant au sujet\n",
    "    rows, cols = torch.where(subject_mask == 1)\n",
    "    last_ones = torch.full((subject_mask.size(0),), -1, dtype=torch.long)\n",
    "    last_ones.scatter_reduce_(0, rows, cols, reduce=\"amax\", include_self=False)\n",
    "\n",
    "    prompt_indices = torch.arange(clean_states_layer.shape[0])\n",
    "\n",
    "    def hook(module,input,output):\n",
    "        restaured = output[0].clone()\n",
    "        restaured[prompt_indices, last_ones] = clean_states_layer[prompt_indices, last_ones]\n",
    "        return (restaured,output[1])\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_hook(subject_mask):\n",
    "    def hook(module,input,output):\n",
    "        std_dev_all = torch.std(output.flatten())\n",
    "        noise = torch.randn_like(output)*3*std_dev_all\n",
    "        noisy_output = output + noise * subject_mask.unsqueeze(-1).float()\n",
    "        return noisy_output\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = model.transformer.wpe.register_forward_hook(noise_hook(subject_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_logits = model(**input, labels = input.input_ids, output_hidden_states = False, output_attentions =False).logits\n",
    "logits = last_non_padding_token_logits(output_logits,input.attention_mask)\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "probs_corrupt = probs.gather(1, torch.tensor(tokenizer([' '+dict['attribute'] for dict in data])['input_ids'])).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exemple\n",
    "#hook_1 = model.transformer.h[1].register_forward_hook(restauration_hook(output.hidden_states[1], subject_mask))\n",
    "#with torch.no_grad():\n",
    "#    output_logits = model(**input, labels = input.input_ids, output_hidden_states = False, output_attentions =False).logits\n",
    "#logits = last_non_padding_token_logits(output_logits,input.attention_mask)\n",
    "#probs = F.softmax(logits, dim=-1)\n",
    "#probs_restaur_1 = probs.gather(1, torch.tensor(tokenizer([' '+dict['attribute'] for dict in data])['input_ids'])).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_restaur = []\n",
    "for l in range(len(model.transformer.h)):\n",
    "    hook_l = model.transformer.h[l].register_forward_hook(restauration_hook(output.hidden_states[l+1], subject_mask))   #on met l+1 dans hidden states \n",
    "                                                                                                                        #car hidden states comprend aussi \n",
    "                                                                                                                        #la sortie de l'embedding (il me semble)\n",
    "    with torch.no_grad():\n",
    "        output_logits = model(**input, labels = input.input_ids, output_hidden_states = False, output_attentions =False).logits\n",
    "    logits = last_non_padding_token_logits(output_logits,input.attention_mask)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    probs_restaur.append( probs.gather(1, torch.tensor(tokenizer([' '+dict['attribute'] for dict in data])['input_ids'])).squeeze() ) \n",
    "    hook_l.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l'AIE du layer 0 (bloc transformer 0) est : 0.8 %\n",
      "l'AIE du layer 1 (bloc transformer 1) est : 1.96 %\n",
      "l'AIE du layer 2 (bloc transformer 2) est : 2.42 %\n",
      "l'AIE du layer 3 (bloc transformer 3) est : 3.12 %\n",
      "l'AIE du layer 4 (bloc transformer 4) est : 3.29 %\n",
      "l'AIE du layer 5 (bloc transformer 5) est : 2.48 %\n",
      "l'AIE du layer 6 (bloc transformer 6) est : 4.25 %\n",
      "l'AIE du layer 7 (bloc transformer 7) est : 3.32 %\n",
      "l'AIE du layer 8 (bloc transformer 8) est : 3.61 %\n",
      "l'AIE du layer 9 (bloc transformer 9) est : 3.36 %\n",
      "l'AIE du layer 10 (bloc transformer 10) est : 2.97 %\n",
      "l'AIE du layer 11 (bloc transformer 11) est : 3.54 %\n",
      "l'AIE du layer 12 (bloc transformer 12) est : 2.06 %\n",
      "l'AIE du layer 13 (bloc transformer 13) est : 1.09 %\n",
      "l'AIE du layer 14 (bloc transformer 14) est : 2.7 %\n",
      "l'AIE du layer 15 (bloc transformer 15) est : 2.71 %\n",
      "l'AIE du layer 16 (bloc transformer 16) est : 1.0 %\n",
      "l'AIE du layer 17 (bloc transformer 17) est : 1.27 %\n",
      "l'AIE du layer 18 (bloc transformer 18) est : 0.37 %\n",
      "l'AIE du layer 19 (bloc transformer 19) est : 0.25 %\n",
      "l'AIE du layer 20 (bloc transformer 20) est : 0.6 %\n",
      "l'AIE du layer 21 (bloc transformer 21) est : -0.37 %\n",
      "l'AIE du layer 22 (bloc transformer 22) est : -1.01 %\n",
      "l'AIE du layer 23 (bloc transformer 23) est : -1.41 %\n"
     ]
    }
   ],
   "source": [
    "for i,a in enumerate(probs_restaur):\n",
    "    AIE = a.mean()-probs_corrupt.mean()\n",
    "    print(f\"l'AIE du layer {i} (bloc transformer {i}) est : {round(AIE.item()*100,2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restaured run pour le premier token sujet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il s'agit d'une simple adaption de ce qui a été fait avant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#il faut changer deux ligne mais la flemme la tt de suite\n",
    "def restauration_hook(clean_states_layer, subject_mask):\n",
    "    #get la first position de chaque subject token ie. firstones[prompt] donne la position du dernier token se référant au sujet\n",
    "    rows, cols = torch.where(subject_mask == 1)\n",
    "    first_ones = torch.full((subject_mask.size(0),), -1, dtype=torch.long)\n",
    "    first_ones.scatter_reduce_(0, rows, cols, reduce=\"amax\", include_self=False)\n",
    "\n",
    "    prompt_indices = torch.arange(clean_states_layer.shape[0])\n",
    "\n",
    "    def hook(module,input,output):\n",
    "        restaured = output[0].clone()\n",
    "        restaured[prompt_indices, first_ones] = clean_states_layer[prompt_indices, first_ones]\n",
    "        return (restaured,output[1])\n",
    "    return hook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
