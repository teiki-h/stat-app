{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer.pad_token =  tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://rome.baulab.info/data/dsets/known_1000.json'\n",
    "response = requests.get(url) \n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [dict['prompt'] for dict in data]\n",
    "subjects = [' '+dict['subject'] for dict in data]\n",
    "input= tokenizer(prompts, return_tensors=\"pt\", padding= True, return_offsets_mapping= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 0,  ..., 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = []\n",
    "for j, prompt in enumerate(prompts):\n",
    "    map = torch.zeros_like(input.input_ids[j], dtype=torch.int)\n",
    "    for i,t in enumerate(input.offset_mapping[j]):\n",
    "        \n",
    "        if (prompts[j].find(subjects[j])-1<=t[0]) and (t[1]<=prompts[j].find(subjects[j])+len(subjects[j])):\n",
    "            map[i] = 1\n",
    "    mask.append(map)\n",
    "subject_mask = torch.stack(mask)\n",
    "subject_mask = torch.logical_and(subject_mask, input.attention_mask).int()\n",
    "subject_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_non_padding_token_logits(logits, attention_mask):\n",
    "    # For each input, find the last non-padding token\n",
    "    last_non_padding_logits = []\n",
    "    \n",
    "    for i in range(logits.size(0)):  # Loop over each prompt in the batch\n",
    "        # Find the last non-padding token position\n",
    "        non_padding_positions = (attention_mask[i] == 1).nonzero(as_tuple=True)[0]\n",
    "        last_non_padding_token_index = non_padding_positions[-1]\n",
    "        \n",
    "        # Get the logits of the last non-padding token\n",
    "        last_non_padding_logits.append(logits[i, last_non_padding_token_index])\n",
    "    last_non_padding_logits = torch.stack(last_non_padding_logits)\n",
    "    return last_non_padding_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(**input, labels = input.input_ids, output_hidden_states = True, output_attentions =False) \n",
    "logits = last_non_padding_token_logits(output.logits,input.attention_mask)\n",
    "probs = F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prba avec clean run pour chaque prompt\n",
    "probs_clean = probs.gather(1, torch.tensor(tokenizer([' '+dict['attribute'] for dict in data])['input_ids'])).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.hidden_states[layer][prompt][token] =vecteur hidden state du token dans le prompt pour le layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restaured run pour le dernier token sujet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalement l'output d'un transformer block devrait être (nombre de prompt, nombre de token, taille d'un vecteur qui représente un token), mais bizarrement on a un tuple avec le premier element qui est bien qqchose de cette forme mais le deuxième élément est un tuple de deux truc de la forme  (nombre de prompt, a,nombre de token, b) avec ab = taille d'un vecteur qui représente un token; dans un premier temps on va juste modifier le premier élement du tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restauration_hook(clean_states_layer, subject_mask):\n",
    "    #get la last position de chaque subject token ie. lastones[prompt] donne la position du dernier token se référant au sujet\n",
    "    rows, cols = torch.where(subject_mask == 1)\n",
    "    last_ones = torch.full((subject_mask.size(0),), -1, dtype=torch.long)\n",
    "    last_ones.scatter_reduce_(0, rows, cols, reduce=\"amax\", include_self=False)\n",
    "\n",
    "    prompt_indices = torch.arange(clean_states_layer.shape[0])\n",
    "\n",
    "    def hook(module,input,output):\n",
    "        restaured = output[0].clone()\n",
    "        restaured[prompt_indices, last_ones] = clean_states_layer[prompt_indices, last_ones]\n",
    "        return (restaured,output[1])\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_hook(subject_mask):\n",
    "    def hook(module,input,output):\n",
    "        std_dev_all = torch.std(output.flatten())\n",
    "        noise = torch.randn_like(output)*3*std_dev_all\n",
    "        noisy_output = output + noise * subject_mask.unsqueeze(-1).float()\n",
    "        return noisy_output\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = model.transformer.wpe.register_forward_hook(noise_hook(subject_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_logits = model(**input, labels = input.input_ids, output_hidden_states = False, output_attentions =False).logits\n",
    "logits = last_non_padding_token_logits(output_logits,input.attention_mask)\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "probs_corrupt = probs.gather(1, torch.tensor(tokenizer([' '+dict['attribute'] for dict in data])['input_ids'])).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 fait\n",
      "layer 1 fait\n",
      "layer 2 fait\n",
      "layer 3 fait\n"
     ]
    }
   ],
   "source": [
    "probs_restaur = []\n",
    "for l in range(len(model.transformer.h)):\n",
    "    hook_l = model.transformer.h[l].register_forward_hook(restauration_hook(output.hidden_states[l+1], subject_mask))   #on met l+1 dans hidden states \n",
    "                                                                                                                        #car hidden states comprend aussi \n",
    "                                                                                                                        #la sortie de l'embedding (il me semble)\n",
    "    with torch.no_grad():\n",
    "        output_logits = model(**input, labels = input.input_ids, output_hidden_states = False, output_attentions =False).logits\n",
    "    logits = last_non_padding_token_logits(output_logits,input.attention_mask)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    probs_restaur.append( probs.gather(1, torch.tensor(tokenizer([' '+dict['attribute'] for dict in data])['input_ids'])).squeeze() ) \n",
    "    hook_l.remove()\n",
    "    print(f'layer {l} fait')\n",
    "    output_logits = None\n",
    "    logits = None\n",
    "    probs = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,a in enumerate(probs_restaur):\n",
    "    AIE = a.mean()-probs_corrupt.mean()\n",
    "    print(f\"l'AIE du layer {i} (bloc transformer {i}) est : {round(AIE.item()*100,2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restaured run pour le premier token sujet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il s'agit d'une simple adaption de ce qui a été fait avant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restauration_first_subject_hook(clean_states_layer, subject_mask):\n",
    "    # Get la first position de chaque subject token ie. first_ones[prompt] donne la position du premier token se référant au sujet\n",
    "    rows, cols = torch.where(subject_mask == 1)\n",
    "    first_ones = torch.full((subject_mask.size(0),), -1, dtype=torch.long)\n",
    "    first_ones.scatter_reduce_(0, rows, cols, reduce=\"amin\", include_self=False) \n",
    "\n",
    "    prompt_indices = torch.arange(clean_states_layer.shape[0])\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        restaured = output.clone()  # Correction ici : output[0] -> output\n",
    "        restaured[prompt_indices, first_ones] = clean_states_layer[prompt_indices, first_ones]\n",
    "        return restaured\n",
    "    \n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_restaur_first_subject = []\n",
    "for l in range(len(model.transformer.h)):\n",
    "    hook_l = model.transformer.h[l].register_forward_hook(restauration_hook(output.hidden_states[l+1], subject_mask))   #on met l+1 dans hidden states \n",
    "                                                                                                                        #car hidden states comprend aussi \n",
    "                                                                                                                        #la sortie de l'embedding (il me semble)\n",
    "    with torch.no_grad():\n",
    "        output_logits = model(**input, labels = input.input_ids, output_hidden_states = False, output_attentions =False).logits\n",
    "    logits = last_non_padding_token_logits(output_logits,input.attention_mask)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    probs_restaur_first_subject.append( probs.gather(1, torch.tensor(tokenizer([' '+dict['attribute'] for dict in data])['input_ids'])).squeeze() ) \n",
    "    hook_l.remove()\n",
    "    output_logits = None\n",
    "    logits = None\n",
    "    probs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restaured run pour le dernier token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restauration_last_token_hook(clean_states_layer, attention_mask):\n",
    "    #get la last position de chaque subject token ie. lastones[prompt] donne la position du dernier token se référant au sujet\n",
    "    rows, cols = torch.where(attention_mask == 1)\n",
    "    last_ones = torch.full((attention_mask.size(0),), -1, dtype=torch.long)\n",
    "    last_ones.scatter_reduce_(0, rows, cols, reduce=\"amax\", include_self=False)\n",
    "\n",
    "    prompt_indices = torch.arange(clean_states_layer.shape[0])\n",
    "\n",
    "    def hook(module,input,output):\n",
    "        restaured = output[0].clone()\n",
    "        restaured[prompt_indices, last_ones] = clean_states_layer[prompt_indices, last_ones]\n",
    "        return (restaured,output[1])\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_restaur_last_token = []\n",
    "for l in range(len(model.transformer.h)):\n",
    "    hook_l = model.transformer.h[l].register_forward_hook(restauration_hook(output.hidden_states[l+1], input.attention_mask))   #on met l+1 dans hidden states \n",
    "                                                                                                                        #car hidden states comprend aussi \n",
    "                                                                                                                        #la sortie de l'embedding (il me semble)\n",
    "    with torch.no_grad():\n",
    "        output_logits = model(**input, labels = input.input_ids, output_hidden_states = False, output_attentions =False).logits\n",
    "    logits = last_non_padding_token_logits(output_logits,input.attention_mask)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    probs_restaur_last_token.append( probs.gather(1, torch.tensor(tokenizer([' '+dict['attribute'] for dict in data])['input_ids'])).squeeze() ) \n",
    "    hook_l.remove()\n",
    "    output_logits = None\n",
    "    logits = None\n",
    "    probs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tracée de la heat map des AIE en fonction des layers et des tokens ou on corriges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcul des AIE\n",
    "AIE_restaur_last_sub = np.array([a.mean() for a in probs_restaur]) - probs_corrupt.mean()\n",
    "AIE_restaur_first_sub = np.array([a.mean() for a in probs_restaur_first_subject]) - probs_corrupt.mean()\n",
    "AIE_restaur_last_token = np.array([a.mean() for a in probs_restaur_last_token]) - probs_corrupt.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracée de la heat map \n",
    "data_heatmap = np.vstack([AIE_restaur_last_sub, AIE_restaur_first_sub, AIE_restaur_last_token])\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(data_heatmap, annot=True, cmap=\"coolwarm\", xticklabels=False, yticklabels=[\"Last Sub\", \"First Sub\", \"Last Token\"])\n",
    "\n",
    "# Titres et labels\n",
    "plt.title(\"Heatmap des AIE\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Méthode de Restauration\")\n",
    "\n",
    "# Afficher la heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probsres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
