{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orienté objet (copie avec modif de raphael)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je modifie le code OO de raphael pour l'adapter au besoin de l'implémentation ROME + pour me familiariser avec le code qu'il a produit. Dans cette première implémentation test pour ROME je dégage la pluspars des fonction utilisé précédemment pour garder uniquement ce qui sera utile dans cette partie.\n",
    "\n",
    "J'ai enlever le truc de batch et la mise sur GPU parce que j'y comprend R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = les prompts sur lesquels on va calculer k* et v*\n",
    "#subject le sujet pour qui il faut determiner k*,v*\n",
    "class Instance_for_ROME :\n",
    "    def __init__(self, subject, inputs= None, l_star = 18, model_name = 'gpt2-xl', nb_prompt=50):\n",
    "        self.model_name = model_name\n",
    "        self.subject = subject\n",
    "        self._l_star = l_star\n",
    "\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if inputs == None:\n",
    "            self.generate_prompts(50)\n",
    "        else:\n",
    "            self.prompts = inputs\n",
    "\n",
    "        self._subject_mask = self.compute_subject_mask()\n",
    "        self._last_subject_indices = (self._subject_mask * torch.arange(1, self._subject_mask.shape[1] + 1, device=self._subject_mask.device)).argmax(dim=1)\n",
    "\n",
    "        self._ks = None\n",
    "        self._k_star = None\n",
    "        self._hooks = []\n",
    "        self._logits = None\n",
    "        self.output = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Instance of {self.model.config.architectures[0]} model'\n",
    "    \n",
    "    def tokenize(self, batch, offsetsMapping=False):\n",
    "        inputs = self.tokenizer(batch, return_tensors='pt', padding=True, return_offsets_mapping=offsetsMapping)\n",
    "        return {k: v.to(device) for k, v in inputs.items()}  # Ensure inputs are moved to device\n",
    "    \n",
    "    def compute_subject_mask(self, prompts = None, subject = None):\n",
    "        res = []\n",
    "\n",
    "        if prompts == None:\n",
    "            prompts = self.prompts\n",
    "        if subject == None:\n",
    "            subject = self.subject\n",
    "\n",
    "        input = self.tokenize(prompts, offsetsMapping=True)\n",
    "        mask = []\n",
    "        for j, prompt in enumerate(prompts):\n",
    "            map = torch.zeros_like(input['input_ids'][j], dtype=torch.int)\n",
    "            for i, t in enumerate(input['offset_mapping'][j]):\n",
    "                if (prompts[j].find(subject)-1 <= t[0]) and (t[1] <= prompts[j].find(subject)+len(subject)) and (prompts[j].find(subject) != -1):\n",
    "                    map[i] = 1\n",
    "            mask.append(map)\n",
    "        subject_mask = torch.stack(mask)\n",
    "        subject_mask = torch.logical_and(subject_mask, input['attention_mask']).int()\n",
    "        return subject_mask\n",
    "    \n",
    "    def get_ks_hook(self, last_subject_indices=None):\n",
    "        if last_subject_indices is None:\n",
    "            last_subject_indices = self._last_subject_indices\n",
    "        \n",
    "        def hook(module, input, output):\n",
    "            if isinstance(input[0], torch.Tensor) and isinstance(input, tuple):\n",
    "                hidden = input[0]\n",
    "                res = hidden[torch.arange(len(last_subject_indices), device=hidden.device), last_subject_indices]\n",
    "                self._ks = res\n",
    "            else:\n",
    "                raise TypeError(\"Expected input[0] to be a torch.Tensor, but got {}\".format(type(input[0])))\n",
    "            pass\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    def accroche(self, l_star=None):\n",
    "        if l_star is None:\n",
    "            l_star = self._l_star\n",
    "        hook = self.get_ks_hook()\n",
    "        handle = self.model.transformer.h[l_star].mlp.register_forward_hook(hook)\n",
    "        self._hooks.append(handle)\n",
    "        pass\n",
    "    \n",
    "    def enleve(self):\n",
    "        for handle in self._hooks:\n",
    "            handle.remove()\n",
    "        self._hooks = []\n",
    "        pass\n",
    "    \n",
    "    def run(self, conserve_logits=False, conserve_output=False):\n",
    "        input = self.tokenize(self.prompts)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**input, labels=input['input_ids']) \n",
    "        if self._ks is not None:\n",
    "            self._k_star = torch.mean(self._ks, dim=0)\n",
    "        if conserve_logits:\n",
    "            self._logits = output.logits\n",
    "        if conserve_output:\n",
    "            self._output = output\n",
    "        pass\n",
    "\n",
    "    def generate_prompts(self, nb_prompt, min_len=2, max_len=11):\n",
    "        vocab_size = self.tokenizer.vocab_size\n",
    "        nb_token = torch.randint(min_len, max_len, (nb_prompt,), device=device)\n",
    "        max_tokens = nb_token.max()\n",
    "        tokens = torch.randint(0, vocab_size, (nb_prompt, max_tokens), device=device)\n",
    "        padded_tokens = F.pad(tokens, (0, max_tokens - nb_token.max().item()), value=vocab_size)\n",
    "        decoded_sequences = [self.tokenizer.decode(seq[:nb_token[i].item()]) for i, seq in enumerate(padded_tokens)]\n",
    "        res = [x + ' ' + self.subject for x in decoded_sequences]\n",
    "        self.__init__(self.subject, res, self._l_star, self.model_name)\n",
    "        pass\n",
    "\n",
    "    def get_k_star(self, l_star=None):\n",
    "        self.accroche(l_star)\n",
    "        self.run()\n",
    "        self.enleve()\n",
    "        return self._k_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Instance_for_ROME('Eiffel Tower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m test.generate_prompts(\u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_k_star\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mInstance_for_ROME.get_k_star\u001b[39m\u001b[34m(self, l_star)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_k_star\u001b[39m(\u001b[38;5;28mself\u001b[39m,l_star = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mself\u001b[39m.accroche(l_star)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mself\u001b[39m.enleve()\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._k_star\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mInstance_for_ROME.run\u001b[39m\u001b[34m(self, conserve_logits, conserve_output)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28mself\u001b[39m.tokenize(\u001b[38;5;28mself\u001b[39m.prompts)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ks != \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mself\u001b[39m._k_star = torch.mean(\u001b[38;5;28mself\u001b[39m._ks, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1060\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1079\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    910\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    911\u001b[39m         block.\u001b[34m__call__\u001b[39m,\n\u001b[32m    912\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    919\u001b[39m         output_attentions,\n\u001b[32m    920\u001b[39m     )\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:404\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[39m\n\u001b[32m    402\u001b[39m residual = hidden_states\n\u001b[32m    403\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.ln_1(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m attn_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m attn_output = attn_outputs[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[32m    413\u001b[39m outputs = attn_outputs[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:335\u001b[39m, in \u001b[36mGPT2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    331\u001b[39m     attn_output, attn_weights = \u001b[38;5;28mself\u001b[39m._upcast_and_reordered_attn(\n\u001b[32m    332\u001b[39m         query_states, key_states, value_states, attention_mask, head_mask\n\u001b[32m    333\u001b[39m     )\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_dropout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m attn_output = attn_output.reshape(*attn_output.shape[:-\u001b[32m2\u001b[39m], -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    348\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.c_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     52\u001b[39m     is_causal = is_causal.item()\n\u001b[32m     54\u001b[39m attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[32m     55\u001b[39m     query,\n\u001b[32m     56\u001b[39m     key,\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m     is_causal=is_causal,\n\u001b[32m     62\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m attn_output = \u001b[43mattn_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "test.generate_prompts(50)\n",
    "test.get_k_star()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute v*\n",
    "\n",
    "Je crée ici une nouvelle classe histoire de faire mes propres tests et de pas toucher au code fait avant moi\n",
    "\n",
    "Le but c'est de compute v* qui est une simple optimisation d'une fonction de perte + de la divergence KL (pour que l'essence du modèle sur le sujet ne change pas de façon trop significative)\n",
    "\n",
    "On a notamment besoin de rajouter en argument o* -> la prédiction que l'on veut que le modèle fasse quand on lui donne notre sujet et la relation\n",
    "\n",
    "De même on a besoin de p, le prompt factuel qui donne clairement la relation entre s et o*\n",
    "Typiquement: 'The Space Needle is in Seattle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEditor:\n",
    "    def __init__(self, instance, o_star):\n",
    "        self.instance = instance\n",
    "        self.o_star = o_star\n",
    "        self._v_star = torch.nn.Parameter(torch.randn([1, 1600], device=device))  # Moved tensor to device\n",
    "\n",
    "        self._hook_handle = None\n",
    "\n",
    "    def mlp_output_hook(self, module, input, output):  # Simple hook pour insérer v* à la bonne couche\n",
    "        return self._v_star.unsqueeze(0).expand_as(output).to(output.device)  # Ensure same device\n",
    "\n",
    "    def accroche(self):\n",
    "        l_star = self.instance._l_star\n",
    "        handle = self.instance.model.transformer.h[l_star].mlp.c_proj.register_forward_hook(self.mlp_output_hook)\n",
    "        self._hook_handle = handle\n",
    "\n",
    "    def enleve(self):\n",
    "        if self._hook_handle is not None:\n",
    "            self._hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction qui suit cherche à optimiser v* par itérations successives sur des prompts qui lui donnent le contexte.\n",
    "\n",
    "[A FAIRE] Définir la loss correctement pour matcher celle qu'on a dans le papier, en prenant en compte les xj notamment ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_v_star(editor, factual_prompts, o_star, n_iter=300, lr=0.1, early_stop_threshold=0.01, lambda_kl=30):\n",
    "    \"\"\"\n",
    "    Optimise v* pour forcer le modèle à prédire o* juste après le prompt,\n",
    "    tout en contrôlant l'essence du sujet avec la KL divergence.\n",
    "    \"\"\"\n",
    "    instance = editor.instance\n",
    "    tokenizer = instance.tokenizer\n",
    "    model = instance.model\n",
    "\n",
    "    editor.accroche()\n",
    "    optimizer = torch.optim.Adam([editor._v_star], lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    input_prompts = [template.format(subject=instance.subject) for template in factual_prompts]\n",
    "    tokenized = tokenizer(input_prompts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_ids = tokenized.input_ids.to(device)  # Move to device\n",
    "    attention_mask = tokenized.attention_mask.to(device)  # Move to device\n",
    "\n",
    "    seq_len = input_ids.shape[1]\n",
    "\n",
    "    # Token ID cible (premier token de o_star)\n",
    "    target_token_id = tokenizer.encode(o_star, add_special_tokens=False)[0]\n",
    "\n",
    "    # Stocker les logits originaux pour le KL divergence\n",
    "    with torch.no_grad():\n",
    "        outputs_original = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits_original = outputs_original.logits\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits_modified = outputs.logits\n",
    "\n",
    "        # === 1. CrossEntropy Loss ===\n",
    "        target_logits = logits_modified[:, seq_len-1, :]\n",
    "\n",
    "        targets = torch.full((target_logits.size(0),), target_token_id, dtype=torch.long, device=target_logits.device)\n",
    "        ce_loss = loss_fn(target_logits, targets)\n",
    "\n",
    "        # === 2. KL Divergence Loss ===\n",
    "        logits_modified_flat = logits_modified.view(-1, logits_modified.size(-1))\n",
    "        logits_original_flat = logits_original.view(-1, logits_original.size(-1))\n",
    "\n",
    "        probs_modified = torch.nn.functional.softmax(logits_modified_flat, dim=-1)\n",
    "        probs_original = torch.nn.functional.softmax(logits_original_flat, dim=-1)\n",
    "\n",
    "        kl_loss = torch.nn.functional.kl_div(probs_modified.log(), probs_original, reduction=\"batchmean\")\n",
    "\n",
    "        # === 3. Loss totale ===\n",
    "        loss = ce_loss + lambda_kl * kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0 or loss.item() < early_stop_threshold:\n",
    "            print(f\"[{i}] Total Loss = {loss.item():.6f} | CE = {ce_loss.item():.6f} | KL = {kl_loss.item():.6f}\")\n",
    "\n",
    "        if loss.item() < early_stop_threshold:\n",
    "            print(f\"\\nEarly stopping at iteration {i} with loss {loss.item():.6f}\")\n",
    "            break\n",
    "\n",
    "    editor.enleve()\n",
    "    return editor._v_star.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir défini tout ça, on le test en essayant d'apprendre le fait: Paris is the capital of Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1776, -0.1835,  0.0786,  ..., -0.7899, -0.1964, -0.3447])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = 'Paris'\n",
    "instance = Instance_for_ROME(subject)\n",
    "instance.get_k_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Total Loss = 9.887506 | CE = 9.887506 | KL = -0.000000\n",
      "[10] Total Loss = 8.395196 | CE = 7.804498 | KL = 0.019690\n",
      "[20] Total Loss = 7.500482 | CE = 6.440556 | KL = 0.035331\n",
      "[30] Total Loss = 6.720286 | CE = 5.108779 | KL = 0.053717\n",
      "[40] Total Loss = 6.047701 | CE = 3.830797 | KL = 0.073897\n",
      "[50] Total Loss = 5.565105 | CE = 3.028305 | KL = 0.084560\n",
      "[60] Total Loss = 5.222683 | CE = 2.673275 | KL = 0.084980\n",
      "[70] Total Loss = 5.049441 | CE = 2.605066 | KL = 0.081479\n",
      "[80] Total Loss = 4.953030 | CE = 2.658812 | KL = 0.076474\n",
      "[90] Total Loss = 4.884614 | CE = 2.575024 | KL = 0.076986\n",
      "[100] Total Loss = 4.844116 | CE = 2.518240 | KL = 0.077529\n",
      "[110] Total Loss = 4.812892 | CE = 2.575865 | KL = 0.074568\n",
      "[120] Total Loss = 4.781309 | CE = 2.514036 | KL = 0.075576\n",
      "[130] Total Loss = 4.759818 | CE = 2.486676 | KL = 0.075771\n",
      "[140] Total Loss = 4.751710 | CE = 2.424177 | KL = 0.077584\n",
      "[150] Total Loss = 4.738081 | CE = 2.542277 | KL = 0.073193\n",
      "[160] Total Loss = 4.718220 | CE = 2.501337 | KL = 0.073896\n",
      "[170] Total Loss = 4.716710 | CE = 2.386987 | KL = 0.077657\n",
      "[180] Total Loss = 4.700091 | CE = 2.406425 | KL = 0.076456\n",
      "[190] Total Loss = 4.698211 | CE = 2.511364 | KL = 0.072895\n",
      "[200] Total Loss = 4.687056 | CE = 2.490070 | KL = 0.073233\n",
      "[210] Total Loss = 4.672782 | CE = 2.410093 | KL = 0.075423\n",
      "[220] Total Loss = 4.687841 | CE = 2.370791 | KL = 0.077235\n",
      "[230] Total Loss = 4.670605 | CE = 2.444082 | KL = 0.074217\n",
      "[240] Total Loss = 4.663866 | CE = 2.384101 | KL = 0.075992\n",
      "[250] Total Loss = 4.651463 | CE = 2.396485 | KL = 0.075166\n",
      "[260] Total Loss = 4.670160 | CE = 2.507172 | KL = 0.072100\n",
      "[270] Total Loss = 4.650672 | CE = 2.364596 | KL = 0.076203\n",
      "[280] Total Loss = 4.641552 | CE = 2.383980 | KL = 0.075252\n",
      "[290] Total Loss = 4.641331 | CE = 2.349817 | KL = 0.076384\n",
      "tensor([[-3.5089, -0.4344,  1.6164,  ..., -4.1744, -4.0996, -1.5488]])\n"
     ]
    }
   ],
   "source": [
    "subject = 'Paris'\n",
    "o_star = 'Italy'\n",
    "factual_prompts = [\n",
    "    '{subject} is the capital of',\n",
    "    'In which country is {subject} located?',\n",
    "    'Where is {subject}?',\n",
    "    \"The country that governs {subject} is\",\n",
    "    \"The location of {subject} is in\",\n",
    "    \"{subject} is situated in the country of\",\n",
    "    \"Which nation does {subject} belong to?\",\n",
    "    \"In which country is {subject} found?\",\n",
    "    \"{subject} is part of the country called\",\n",
    "    \"The city of {subject} is a part of\",\n",
    "    \"The famous city {subject} is located in\",\n",
    "    \"You can find {subject} in the country of\",\n",
    "    \"The administrative country for {subject} is\"\n",
    "]\n",
    "\n",
    "editor = ValueEditor(instance, o_star)\n",
    "\n",
    "v_star = optimize_v_star(editor, factual_prompts, o_star)\n",
    "\n",
    "print(v_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion (k,v) -> Update de W_proj\n",
    "\n",
    "Pour l'instant on élude complètement la question de la covariance empirique des clés k sur le corpus de wikipédia en remplacant la matrice de covariance (C) per l'identité.\n",
    "On regarde si ça fonctionne déjà comme ça et puis on se penchera dessus après"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rank_one_update(instance, v_star, C_inv=None):\n",
    "    l_star = instance._l_star\n",
    "    k_star = instance._k_star.to(device)  # Assurer que k_star est sur GPU\n",
    "    W_proj = instance.model.transformer.h[l_star].mlp.c_proj.weight  # Déjà sur le modèle, donc sur device\n",
    "\n",
    "    W_fc = instance.model.transformer.h[l_star].mlp.c_fc.weight  # Pareil\n",
    "    k_star_projected = torch.nn.functional.linear(k_star, W_fc.t())\n",
    "\n",
    "    if C_inv is None:\n",
    "        C_inv = torch.eye(k_star_projected.size(0), device=k_star_projected.device)  # sur device correct\n",
    "\n",
    "    # 1. Calculer Lambda\n",
    "    numerator = v_star - W_proj.t() @ k_star_projected\n",
    "    denominator = (C_inv @ k_star_projected).dot(k_star_projected)\n",
    "    Lambda = numerator / denominator\n",
    "\n",
    "    # 2. Calculer delta_W\n",
    "    delta_W = Lambda.view(-1, 1) @ (C_inv @ k_star_projected).view(1, -1)\n",
    "\n",
    "    # 3. Appliquer la mise à jour\n",
    "    with torch.no_grad():\n",
    "        W_proj.data += delta_W.t()\n",
    "\n",
    "    print(\"Mise à jour appliquée avec succès sur W_proj.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mise à jour appliquée avec succès sur W_proj.\n"
     ]
    }
   ],
   "source": [
    "apply_rank_one_update(instance, v_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: \"Paris is the capital of\"\n",
      "Top 1: France (0.6203)\n",
      "Top 2: Italy (0.1514)\n",
      "Top 3: the (0.0481)\n",
      "Top 4: Europe (0.0206)\n",
      "Top 5: a (0.0063)\n",
      "\n",
      "Prompt: \"The administrative country for Paris is\"\n",
      "Top 1: the (0.1429)\n",
      "Top 2: a (0.0290)\n",
      "Top 3: Italy (0.0274)\n",
      "Top 4: not (0.0262)\n",
      "Top 5: now (0.0196)\n"
     ]
    }
   ],
   "source": [
    "def test_new_fact(instance, subject, prompt_template, top_k=5):\n",
    "    tokenizer = instance.tokenizer\n",
    "    model = instance.model\n",
    "    model.eval()\n",
    "\n",
    "    prompt = prompt_template.format(subject=subject)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)  # Move to device\n",
    "    attention_mask = inputs.attention_mask.to(device)  # Move to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        top_probs, top_indices = probs.topk(top_k)\n",
    "        top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "\n",
    "    print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "    for rank, (token, prob) in enumerate(zip(top_tokens, top_probs), 1):\n",
    "        print(f\"Top {rank}: {token.strip()} ({prob.item():.4f})\")\n",
    "\n",
    "# Tester sur quelques prompts :\n",
    "test_new_fact(instance, subject, \"{subject} is the capital of\")\n",
    "test_new_fact(instance, subject, \"The administrative country for {subject} is\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi ça ne fonctionne pas\n",
    "\n",
    "-> On a un arbitrage fondamental dans le processus d'optimisation de v* entre l'apprentissage de nouvelles connaissances et la fidélité au modèle original (Dkl)\n",
    "Cela crée notamment une oscillation qui vient faire stagner l'optimisation de v* à un certain endroit.\n",
    "\n",
    "Deux hypothèses:\n",
    "- Soit l'association Paris -> France est beaucoup trop ancré dans l'apprentissage du modèle, et en fait ROME ne marchera tout simplement pas dessus ne modifie pas assez en profondeur\n",
    "- Soit le problème vient d'autre part et on peut encore améliorer l'optimisation.\n",
    "\n",
    "### On en retire deux pistes\n",
    "\n",
    "1. On va essayer de tester les mêmes fonctions sur un fait beaucoup moins établi (pour voir si c'est vraiment ça le pb)\n",
    "\n",
    "2. (Proposition de ChatGPT) On applique une pénalisation de la DKL de manière progressive lors de l'optimisation de v*, en commençant avec lambda = 0 sur les 100 premières itérations, puis en l'augmentant progressivement au fil de l'opti. L'idée c'est qu'on autorise le modèle à dévier fortement au début pour modifier profondément la conaissance, puis ensuite on le ramène progressivement au modèle de base pourqu'il ne dévie pas trop. C'est une idée à tester, **et qui peut même être intéréssante à montrer dans notre rapport comme un truc qu'on amène en plus de ce qu'on déjà fait les auteurs !!**\n",
    "\n",
    "#### 1. ROME sur un fait moins ancré (Mont Everest -> Nepal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject2 = \"Mount Everest\"\n",
    "o_star2 = \"Tibet\"\n",
    "\n",
    "factual_prompts_2 = [\n",
    "    \"{subject2} is located in\",\n",
    "    \"The location of {subject2} is\",\n",
    "    \"Which country is {subject2} situated in?\",\n",
    "    \"{subject2} is found in\",\n",
    "    \"Where can you find {subject2}?\",\n",
    "    \"{subject2} lies in the country of\",\n",
    "    \"The famous peak {subject2} is part of\",\n",
    "    \"{subject2} belongs to\",\n",
    "    \"In which country is {subject2}?\",\n",
    "    \"The region hosting {subject2} is\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1228,  0.2350, -0.3844,  ..., -0.6715, -0.5512,  0.9980])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance2 = Instance_for_ROME(\"Mount Everest\")\n",
    "instance2.get_k_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Total Loss = 6.779407 | CE = 6.779407 | KL = -0.000000\n",
      "[10] Total Loss = 5.231492 | CE = 4.633796 | KL = 0.019923\n",
      "[20] Total Loss = 4.454584 | CE = 3.407744 | KL = 0.034895\n",
      "[30] Total Loss = 4.009721 | CE = 2.753810 | KL = 0.041864\n",
      "[40] Total Loss = 3.831625 | CE = 2.596739 | KL = 0.041163\n",
      "[50] Total Loss = 3.721290 | CE = 2.522736 | KL = 0.039952\n",
      "[60] Total Loss = 3.668720 | CE = 2.465712 | KL = 0.040100\n",
      "[70] Total Loss = 3.654763 | CE = 2.404272 | KL = 0.041683\n",
      "[80] Total Loss = 3.616385 | CE = 2.414395 | KL = 0.040066\n",
      "[90] Total Loss = 3.596866 | CE = 2.423718 | KL = 0.039105\n",
      "[100] Total Loss = 3.584001 | CE = 2.406402 | KL = 0.039253\n",
      "[110] Total Loss = 3.610426 | CE = 2.529819 | KL = 0.036020\n",
      "[120] Total Loss = 3.581926 | CE = 2.395793 | KL = 0.039538\n",
      "[130] Total Loss = 3.567891 | CE = 2.435115 | KL = 0.037759\n",
      "[140] Total Loss = 3.557625 | CE = 2.399783 | KL = 0.038595\n",
      "[150] Total Loss = 3.563760 | CE = 2.349854 | KL = 0.040464\n",
      "[160] Total Loss = 3.548687 | CE = 2.402713 | KL = 0.038199\n",
      "[170] Total Loss = 3.547964 | CE = 2.422522 | KL = 0.037515\n",
      "[180] Total Loss = 3.544499 | CE = 2.371168 | KL = 0.039111\n",
      "[190] Total Loss = 3.547628 | CE = 2.388122 | KL = 0.038650\n",
      "[200] Total Loss = 3.555834 | CE = 2.344294 | KL = 0.040385\n",
      "[210] Total Loss = 3.545805 | CE = 2.351679 | KL = 0.039804\n",
      "[220] Total Loss = 3.536790 | CE = 2.373413 | KL = 0.038779\n",
      "[230] Total Loss = 3.533280 | CE = 2.385707 | KL = 0.038252\n",
      "[240] Total Loss = 3.530733 | CE = 2.382430 | KL = 0.038277\n",
      "[250] Total Loss = 3.603833 | CE = 2.534680 | KL = 0.035638\n",
      "[260] Total Loss = 3.554670 | CE = 2.430705 | KL = 0.037466\n",
      "[270] Total Loss = 3.542311 | CE = 2.424396 | KL = 0.037264\n",
      "[280] Total Loss = 3.534354 | CE = 2.360358 | KL = 0.039133\n",
      "[290] Total Loss = 3.528110 | CE = 2.358314 | KL = 0.038993\n",
      "tensor([[-0.0953,  0.4081,  1.6404,  ..., -2.7436,  0.0386,  1.7774]])\n"
     ]
    }
   ],
   "source": [
    "editor2 = ValueEditor(instance2, o_star2)\n",
    "\n",
    "v_star2 = optimize_v_star(editor2, factual_prompts_2, o_star2)\n",
    "\n",
    "print(v_star2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mise à jour appliquée avec succès sur W_proj.\n",
      "\n",
      "Prompt: \"Mount Everest is located in\"\n",
      "Top 1: the (0.5760)\n",
      "Top 2: Nepal (0.2083)\n",
      "Top 3: a (0.0310)\n",
      "Top 4: Tibet (0.0258)\n",
      "Top 5: an (0.0129)\n",
      "\n",
      "Prompt: \"The region hosting Mount Everest is\"\n",
      "Top 1: home (0.0807)\n",
      "Top 2: the (0.0757)\n",
      "Top 3: a (0.0652)\n",
      "Top 4: also (0.0626)\n",
      "Top 5: one (0.0422)\n"
     ]
    }
   ],
   "source": [
    "apply_rank_one_update(instance2, v_star2)\n",
    "\n",
    "subject = subject2 #Comportement bizarre de la fonction de test qui utliise une variable globale -> a fix pour plus tard\n",
    "\n",
    "test_new_fact(instance2, subject, \"{subject} is located in\")\n",
    "test_new_fact(instance2, subject, \"The region hosting {subject} is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon là visiblement en fait on a encore un autre problème, c'est que l'association est tellement faible sur ces prompts que ça prédit des phrases plus diverses encore. Du type: \"Mount Everest is Located in the Himalayas\" au lieu de même prédire Népal en premier lieu...\n",
    "Donc nos prompts sont de fait pas pertinents de base et l'optimisation ne marche pas forcément mieux, il faudrait alors soit prendre un autre exemple, soit prendre en compte plus de contexte\n",
    "Mais là je vais aller me coucher mdr."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
