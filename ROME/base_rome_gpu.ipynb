{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orienté objet (copie avec modif de raphael)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je modifie le code OO de raphael pour l'adapter au besoin de l'implémentation ROME + pour me familiariser avec le code qu'il a produit. Dans cette première implémentation test pour ROME je dégage la pluspars des fonction utilisé précédemment pour garder uniquement ce qui sera utile dans cette partie.\n",
    "\n",
    "J'ai enlever le truc de batch et la mise sur GPU parce que j'y comprend R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance_for_ROME:\n",
    "    def __init__(self, subject, inputs=None, l_star=18, model_name='gpt2-xl',C=None, nb_prompt=50,batch_size=2):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.subject = subject\n",
    "        self._l_star = l_star\n",
    "        self.batch_size = batch_size\n",
    "        # Setup device\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        print(f\"Model {model_name} loaded on {self.device}\")\n",
    "        if inputs is None:\n",
    "            self.prompts=self.generate_prompts(nb_prompt,batch_size=batch_size)\n",
    "            self.nb_prompt = len(self.prompts)\n",
    "        else:\n",
    "            self.prompts = inputs\n",
    "            self.nb_prompt = len(inputs)\n",
    "        \n",
    "        self._k_star = None\n",
    "        self._hooks = []\n",
    "        self._logits = None\n",
    "        self.output = None\n",
    "        self.activationsC=None\n",
    "        self.C=C\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Instance of {self.model.config.architectures[0]} model'\n",
    "\n",
    "    def tokenize(self, batch, offsetsMapping=False):\n",
    "        inputs = self.tokenizer(batch, return_tensors='pt', padding=True, return_offsets_mapping=offsetsMapping)\n",
    "        return {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "    def compute_subject_mask(self, prompts=None, subject=None):\n",
    "        if prompts is None:\n",
    "            prompts = self.prompts\n",
    "        if subject is None:\n",
    "            subject = self.subject\n",
    "\n",
    "        input = self.tokenize(prompts, offsetsMapping=True)\n",
    "        mask = []\n",
    "        for j, prompt in enumerate(prompts):\n",
    "            map = torch.zeros_like(input['input_ids'][j], dtype=torch.int)\n",
    "            indexSubject = prompt.find(subject)\n",
    "            for i, t in enumerate(input['offset_mapping'][j]):\n",
    "                if indexSubject != -1:\n",
    "                    if (indexSubject <= t[0]) and (t[1] <= indexSubject + len(subject)):\n",
    "                        map[i] = 1\n",
    "            mask.append(map)\n",
    "        subject_mask = torch.stack(mask)\n",
    "        subject_mask = torch.logical_and(subject_mask, input['attention_mask']).int()\n",
    "        return subject_mask\n",
    "    \n",
    "    def compute_last_subject_indices(self, prompts):\n",
    "        subject_mask = self.compute_subject_mask(prompts)\n",
    "        last_subject_indices = (\n",
    "            subject_mask * torch.arange(1, subject_mask.shape[1] + 1, device=subject_mask.device)\n",
    "        ).argmax(dim=1)\n",
    "        return last_subject_indices\n",
    "\n",
    "    def get_ks_hook(self, prompts):\n",
    "        last_subject_indices = self.compute_last_subject_indices(prompts)\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            res = input[0][torch.arange(len(last_subject_indices)), last_subject_indices]   # We have to read the value right after the non-linearity of the MLP\n",
    "            if self._k_star is None:\n",
    "                self._k_star = res.mean(dim=0)\n",
    "                self._kcount = 1\n",
    "            else:\n",
    "                self._k_star = (self._k_star * self._kcount + res.mean(dim=0)) / (self._kcount + 1)\n",
    "                self._kcount += 1\n",
    "\n",
    "        return hook\n",
    "\n",
    "    def accroche(self, hook,l_star=None):\n",
    "        if l_star is None:\n",
    "            l_star = self._l_star\n",
    "        handle = self.model.transformer.h[l_star].mlp.act.register_forward_hook(hook)\n",
    "        self._hooks.append(handle)\n",
    "\n",
    "    def enleve(self):\n",
    "        for handle in self._hooks:\n",
    "            handle.remove()\n",
    "        self._hooks = []\n",
    "\n",
    "    def run(self, prompts,conserve_logits=False, conserve_output=False):\n",
    "        input = self.tokenize(prompts)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**input, labels=input['input_ids'])\n",
    "        if conserve_logits:\n",
    "            self._logits = output.logits\n",
    "        if conserve_output:\n",
    "            self._output = output\n",
    "            \n",
    "    def get_k_star(self, l_star=None,batch_size=None):\n",
    "        if l_star is None:\n",
    "            l_star = self._l_star\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        print(f'Getting k_star for {self.nb_prompt//batch_size} batches ...')\n",
    "        for i in tqdm(range(self.nb_prompt//batch_size)):\n",
    "            self.accroche(self.get_ks_hook(self.prompts[i*batch_size:(i+1)*batch_size]),l_star=l_star)\n",
    "            self.run(self.prompts[i*batch_size:(i+1)*batch_size])\n",
    "            self.enleve()\n",
    "        self._k_star = self._k_star.cpu()\n",
    "        if self.C is None:\n",
    "            self.get_C(self.get_wikipedia_data(100),l_star=l_star,batch_size=batch_size)\n",
    "        #self._k_star = torch.inverse(self.C) @ self._k_star.unsqueeze(1)\n",
    "        #self._k_star = self._k_star.squeeze()\n",
    "        #self._k_star = self._k_star / self._k_star.norm()\n",
    "  \n",
    "        return self._k_star\n",
    "    \n",
    "\n",
    "    def generate_prompts(self, nb_prompt, handPrompts=None,min_len=2, max_len=11,batch_size=None,mode=\"k*\"):\n",
    "        prompts= []\n",
    "        if handPrompts is None:\n",
    "            handPrompts = [\"\"]\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        print(f'Generating prompts {batch_size} by {batch_size}...')\n",
    "        for j in tqdm(range(nb_prompt//batch_size)):   #There won't always be nb_prompt generated but it's ok, choose a multiple of batch_size if you want to be sure\n",
    "            for i in range(batch_size):\n",
    "                prompt=self.model.generate(input_ids=self.tokenizer.encode(\"<|endoftext|>\", return_tensors=\"pt\").to(self.device),\n",
    "                                            max_length=max_len+1 , #to account for the end of text token\n",
    "                                            min_length=min_len,\n",
    "                                            num_return_sequences=1,\n",
    "                                            do_sample=True,\n",
    "                                            pad_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "                decodedPrompt=  self.tokenizer.decode(prompt[0], skip_special_tokens=True)\n",
    "                if mode == \"k*\":\n",
    "                    prompts.append(decodedPrompt+self.subject)\n",
    "                elif mode == \"v*\":\n",
    "                    prompts.append(decodedPrompt+\". \"+handPrompts[(j*batch_size+i)%len(handPrompts)].format(subject=self.subject))\n",
    "                else:\n",
    "                    print(\"Error: mode not recognized\")\n",
    "        return prompts\n",
    "    \n",
    "    #Calculating the C matrix\n",
    "\n",
    "    def get_C_hook(self, attentionMask):\n",
    "        mask=attentionMask.bool()\n",
    "        def hook(module, input, output):\n",
    "            activations= output[mask].view(-1,output.size(-1)).cpu()\n",
    "            self.activationsC.append(activations)\n",
    "        return hook\n",
    "\n",
    "    def get_C(self, texts,l_star=None,batch_size=2):\n",
    "        print(f'Computing C')\n",
    "        self.activationsC = []\n",
    "        if l_star is None:\n",
    "            l_star = self._l_star\n",
    "        for i in tqdm(range(len(texts)//batch_size)):\n",
    "            batch= texts[i*batch_size:(i+1)*batch_size]\n",
    "            input= self.tokenizer(batch, return_tensors='pt', padding=True, truncation=True)\n",
    "            input_ids = input['input_ids'].to(self.device)\n",
    "            attention_mask = input['attention_mask'].to(self.device)\n",
    "            hook=self.get_C_hook(attention_mask)\n",
    "            self.accroche(hook,l_star=l_star)\n",
    "            with torch.no_grad():\n",
    "                # Forward pass on the model (no gradients needed)\n",
    "                self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            self.enleve()\n",
    "            del input_ids\n",
    "            del attention_mask\n",
    "            torch.cuda.empty_cache()\n",
    "        # Compute the kkT_matrices and C\n",
    "        self.activationsC = torch.cat(self.activationsC, dim=0)\n",
    "        self.C = self.activationsC.T @ self.activationsC/ self.activationsC.size(0)\n",
    "        self.C = self.C\n",
    "        return self.C\n",
    "\n",
    "    def get_wikipedia_data(self, n):\n",
    "        ds_name = 'wikitext'\n",
    "\n",
    "        raw_ds = load_dataset(ds_name, dict(wikitext=\"wikitext-103-raw-v1\", wikipedia=\"20200501.en\")[ds_name])\n",
    "        def clean_text(text_data):\n",
    "            cleaned_text_data = []\n",
    "            for line in text_data:\n",
    "\n",
    "                line = line.replace('@-@', '-')\n",
    "                line = line.replace(' @,@ ', ',')\n",
    "                line = line.replace(' @.@ ', '.')\n",
    "                line = re.sub(r'\\s+', ' ', line).strip()\n",
    "                line = line.replace(\"\\\\'\", \"'\") # ne marche pas je veux remplacer les \\' par ' mais j'y arrive pas\n",
    "                \n",
    "                # 3. Avoid adding empty lines\n",
    "                if line:  # Only add non-empty lines\n",
    "                    cleaned_text_data.append(line)\n",
    "            cleaned_text_data = [ line for line in cleaned_text_data \n",
    "                                    if not (line.startswith('=') and line.endswith('='))\n",
    "            ]\n",
    "            return cleaned_text_data\n",
    "        text_data = raw_ds['train'].shuffle()['text'][:n]\n",
    "        return clean_text(text_data)\n",
    "\n",
    "    def delete_instance(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._k_star = None\n",
    "        self._hooks = []\n",
    "        self._logits = None\n",
    "        self.output = None\n",
    "        self.activationsC = None\n",
    "        self.C = None\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Instance deleted and GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute v*\n",
    "\n",
    "Je crée ici une nouvelle classe histoire de faire mes propres tests et de pas toucher au code fait avant moi\n",
    "\n",
    "Le but c'est de compute v* qui est une simple optimisation d'une fonction de perte + de la divergence KL (pour que l'essence du modèle sur le sujet ne change pas de façon trop significative)\n",
    "\n",
    "On a notamment besoin de rajouter en argument o* -> la prédiction que l'on veut que le modèle fasse quand on lui donne notre sujet et la relation\n",
    "\n",
    "De même on a besoin de p, le prompt factuel qui donne clairement la relation entre s et o*\n",
    "Typiquement: 'The Space Needle is in Seattle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEditor:\n",
    "    def __init__(self, instance, o_star):\n",
    "        self.instance = instance\n",
    "        self.o_star = o_star\n",
    "        device = instance.device\n",
    "        self._v_star = torch.nn.Parameter(torch.randn([1, 1600], device=device))  # Moved tensor to device\n",
    "\n",
    "        self._hook_handle = None\n",
    "\n",
    "    def accroche(self,hook):\n",
    "        l_star = self.instance._l_star\n",
    "        handle = self.instance.model.transformer.h[l_star].mlp.c_proj.register_forward_hook(hook)\n",
    "        self._hook_handle = handle\n",
    "\n",
    "    def enleve(self):\n",
    "        if self._hook_handle is not None:\n",
    "            self._hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction qui suit cherche à optimiser v* par itérations successives sur des prompts qui lui donnent le contexte.\n",
    "\n",
    "[A FAIRE] Définir la loss correctement pour matcher celle qu'on a dans le papier, en prenant en compte les xj notamment ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_input_output_at_word(\n",
    "    editor: ValueEditor,\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    offset_mapping,\n",
    "    prompt_text: str,\n",
    "    subject: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Captures MLP input/output at the token corresponding to the subject in the prompt.\n",
    "    \"\"\"\n",
    "    input_activations = {}\n",
    "    output_activations = {}\n",
    "\n",
    "    def mlp_hook(module, input, output):\n",
    "        input_activations[\"input\"] = input[0].detach()\n",
    "        output_activations[\"output\"] = output.detach()\n",
    "\n",
    "    l_star = editor.instance._l_star\n",
    "    handle = editor.instance.model.transformer.h[l_star].mlp.register_forward_hook(mlp_hook)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        editor.instance.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    idx = find_subject_token_idx( input_ids[0], offset_mapping, prompt_text, subject)\n",
    "\n",
    "    input_repr = input_activations[\"input\"][0, idx]\n",
    "    output_repr = output_activations[\"output\"][0, idx]\n",
    "    return input_repr, output_repr\n",
    "\n",
    "\n",
    "def find_subject_token_idx( input_ids, offset_mapping, prompt_text, subject):\n",
    "    \"\"\"\n",
    "    Find the token index in input_ids that corresponds to the last token of `subject`\n",
    "    in `prompt_text`.\n",
    "\n",
    "    Returns the index of the last token of the subject.\n",
    "    \"\"\"\n",
    "    subject_start = prompt_text.find(subject)\n",
    "    subject_end = subject_start + len(subject)\n",
    "\n",
    "    for i, (start, end) in enumerate(offset_mapping):\n",
    "        if start <= subject_end <= end or (start < subject_end and end >= subject_end - 1):\n",
    "            return i\n",
    "    # fallback: last token (to avoid crash)\n",
    "    return input_ids.size(1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def optimize_v_star(\n",
    "    editor, factual_prompt, kl_prompts,kStar, o_star,\n",
    "    n_iter=300, lr=0.5, weight_decay=1.5e-3,\n",
    "    early_stop_threshold=0.01, lambda_kl=100, clamp_norm_factor=10.0,nbRP=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimise v* pour encoder un fait (subject → o*) dans la sortie MLP,\n",
    "    tout en préservant l'essence du sujet via régularisation KL sur prompts neutres.\n",
    "    \"\"\"\n",
    "    instance = editor.instance\n",
    "    model = instance.model\n",
    "    tokenizer = instance.tokenizer\n",
    "    device = instance.device\n",
    "\n",
    "    delta = torch.zeros(model.config.n_embd, requires_grad=True, device=device)\n",
    "    optimizer = torch.optim.Adam([delta], lr=lr)\n",
    "\n",
    "    # Préparation des prompts\n",
    "    rewriting_inputs= editor.instance.generate_prompts(nbRP,handPrompts=[factual_prompt],batch_size=5,mode=\"v*\")\n",
    "    kl_inputs = [p.format(subject=instance.subject) for p in kl_prompts]\n",
    "    all_inputs = rewriting_inputs + kl_inputs\n",
    "\n",
    "    # Tokenisation\n",
    "    tokenized = tokenizer(\n",
    "        all_inputs, return_tensors=\"pt\", padding=True, return_offsets_mapping=True\n",
    "    ).to(device)\n",
    "    input_ids = tokenized.input_ids\n",
    "    attention_mask = tokenized.attention_mask\n",
    "    offset_mapping = tokenized.offset_mapping\n",
    "\n",
    "    # Cible complète (tous tokens de o*)\n",
    "    target_ids = tokenizer.encode(o_star, add_special_tokens=False)\n",
    "    target_tensor = torch.tensor(target_ids, device=device)\n",
    "\n",
    "    # Construction de rewriting_targets\n",
    "    rewriting_targets = torch.full_like(input_ids[:len(rewriting_inputs)], -100) #-100 = ignore_index\n",
    "    for i in range(len(rewriting_inputs)):\n",
    "        seq_len = attention_mask[i].sum()\n",
    "        rewriting_targets[i, seq_len - len(target_ids):seq_len] = target_tensor\n",
    "\n",
    "    # Lookup index (fin du sujet) pour chaque prompt\n",
    "    lookup_idxs = []\n",
    "    for i, prompt in enumerate(all_inputs):\n",
    "        s_start = prompt.find(instance.subject)\n",
    "        s_end = s_start + len(instance.subject)\n",
    "        for j, (start, end) in enumerate(offset_mapping[i]):\n",
    "            if start <= s_end <= end:\n",
    "                lookup_idxs.append(j)\n",
    "                break\n",
    "        else:\n",
    "            lookup_idxs.append(attention_mask[i].sum().item() - 1)\n",
    "    lookup_idxs = torch.tensor(lookup_idxs, device=device)\n",
    "\n",
    "    # Optim loop\n",
    "    target_init = None\n",
    "    kl_distr_init = None\n",
    "    CE_list, KL_list, loss_list = [], [], []\n",
    "\n",
    "    for step in range(n_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            nonlocal target_init\n",
    "            output = output.clone()  # ← éviter modification in-place d'une vue sur un leaf variable\n",
    "            for i, idx in enumerate(lookup_idxs):\n",
    "                output[i, idx, :] = output[i, idx, :] + delta\n",
    "            if target_init is None:\n",
    "                target_init = output[0, lookup_idxs[0]].detach().clone()\n",
    "            return output\n",
    "\n",
    "        editor.accroche(hook)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        editor.enleve()\n",
    "        logits = outputs.logits\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # CrossEntropy sur rewriting prompts\n",
    "        loss_ce = F.nll_loss(\n",
    "            log_probs[:len(rewriting_inputs)].transpose(1, 2),\n",
    "            rewriting_targets,\n",
    "            ignore_index=-100\n",
    "        )\n",
    "\n",
    "        # KL sur prompts de contrôle\n",
    "        kl_idxs = lookup_idxs[len(rewriting_inputs):]\n",
    "        kl_logits = logits[len(rewriting_inputs):][torch.arange(len(kl_prompts)), kl_idxs]\n",
    "        kl_log_probs = F.log_softmax(kl_logits, dim=-1)\n",
    "        if kl_distr_init is None:\n",
    "            kl_distr_init = kl_log_probs.detach()\n",
    "        kl_loss = F.kl_div(kl_log_probs, kl_distr_init, log_target=True, reduction=\"batchmean\")\n",
    "\n",
    "        # Régularisation\n",
    "        wd_loss = weight_decay * (delta.norm() / (target_init.norm() + 1e-6))**2\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_ce + lambda_kl * kl_loss + wd_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Clamp L2\n",
    "        max_norm = clamp_norm_factor * target_init.norm()\n",
    "        if delta.norm() > max_norm:\n",
    "            with torch.no_grad():\n",
    "                delta.mul_(max_norm / delta.norm())\n",
    "\n",
    "        # Logs\n",
    "        CE_list.append(loss_ce.item())\n",
    "        KL_list.append(kl_loss.item())\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        if step % 10 == 0 or loss.item() < early_stop_threshold:\n",
    "            print(f\"[{step}] Total Loss = {loss.item():.6f} | CE = {loss_ce.item():.6f} | KL = {kl_loss.item():.6f}\")\n",
    "        if loss.item() < early_stop_threshold:\n",
    "            print(f\"\\nEarly stopping at iteration {step} with loss {loss.item():.6f}\")\n",
    "            break\n",
    "    editor.enleve()\n",
    "    target=target_init+delta\n",
    "    '''cur_input,cur_output=get_module_input_output_at_word(editor,input_ids[0],attention_mask[0],offset_mapping[0],factual_prompts[0].format(subject=editor.instance.subject),editor.instance.subject)\n",
    "    kStar = kStar.to(device)\n",
    "    W_proj = instance.model.transformer.h[instance._l_star].mlp.c_proj.weight.detach()\n",
    "    k_star_proj = W_proj.T @ kStar  # shape [1600]\n",
    "\n",
    "\n",
    "    v_star=(target - cur_output)/torch.dot(cur_input, k_star_proj)\n",
    "'''\n",
    "    v_star=target\n",
    "    return v_star.detach(), CE_list, KL_list, loss_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir défini tout ça, on le test en essayant d'apprendre le fait: Paris is the capital of Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gpt2-xl loaded on cuda\n",
      "Generating prompts 2 by 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|██████████| 25/25 [00:19<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting k_star for 25 batches ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "100%|██████████| 25/25 [00:01<00:00, 20.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:09<00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "subject = 'Pope'\n",
    "instance = Instance_for_ROME(subject,C=torch.load(\"C.pt\"))\n",
    "kStar=instance.get_k_star().to(instance.device)\n",
    "#We need to project k* back to a dim 1600 space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6400])\n"
     ]
    }
   ],
   "source": [
    "print(kStar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prompts 5 by 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Total Loss = 13.024175 | CE = 13.024175 | KL = 0.000000\n",
      "[10] Total Loss = 26.268232 | CE = 12.026093 | KL = 0.284788\n",
      "[20] Total Loss = 18.110027 | CE = 11.873302 | KL = 0.124612\n",
      "[30] Total Loss = 13.250580 | CE = 11.664171 | KL = 0.031564\n",
      "[40] Total Loss = 13.188063 | CE = 11.418873 | KL = 0.035190\n",
      "[50] Total Loss = 12.198624 | CE = 11.087518 | KL = 0.022009\n",
      "[60] Total Loss = 11.579378 | CE = 10.699059 | KL = 0.017372\n",
      "[70] Total Loss = 11.014631 | CE = 10.192660 | KL = 0.016176\n",
      "[80] Total Loss = 10.264272 | CE = 9.431478 | KL = 0.016349\n",
      "[90] Total Loss = 8.963062 | CE = 8.045335 | KL = 0.017984\n",
      "[100] Total Loss = 6.973884 | CE = 5.877321 | KL = 0.021466\n",
      "[110] Total Loss = 5.597969 | CE = 4.358048 | KL = 0.024254\n",
      "[120] Total Loss = 3.937873 | CE = 2.575467 | KL = 0.026647\n",
      "[130] Total Loss = 2.540258 | CE = 1.017875 | KL = 0.029769\n",
      "[140] Total Loss = 1.827729 | CE = 0.269921 | KL = 0.030413\n",
      "[150] Total Loss = 1.537441 | CE = 0.129107 | KL = 0.027407\n",
      "[160] Total Loss = 1.329818 | CE = 0.112416 | KL = 0.023602\n",
      "[170] Total Loss = 1.179895 | CE = 0.117156 | KL = 0.020529\n",
      "[180] Total Loss = 1.070382 | CE = 0.118282 | KL = 0.018333\n",
      "[190] Total Loss = 0.985793 | CE = 0.111192 | KL = 0.016794\n",
      "[200] Total Loss = 0.917259 | CE = 0.101002 | KL = 0.015636\n",
      "[210] Total Loss = 0.860107 | CE = 0.091851 | KL = 0.014683\n",
      "[220] Total Loss = 0.811528 | CE = 0.084652 | KL = 0.013862\n",
      "[230] Total Loss = 0.769594 | CE = 0.078894 | KL = 0.013144\n",
      "[240] Total Loss = 0.732954 | CE = 0.074016 | KL = 0.012513\n",
      "[250] Total Loss = 0.700618 | CE = 0.069764 | KL = 0.011956\n",
      "[260] Total Loss = 0.671852 | CE = 0.066025 | KL = 0.011459\n",
      "[270] Total Loss = 0.646092 | CE = 0.062719 | KL = 0.011013\n",
      "[280] Total Loss = 0.622849 | CE = 0.059787 | KL = 0.010610\n",
      "[290] Total Loss = 0.601782 | CE = 0.057171 | KL = 0.010243\n",
      "tensor([-1.7613,  0.1097,  0.3806,  ..., -0.5279, -0.3972,  3.1116],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "o_star = 'Peter'\n",
    "factual_prompt=\"The {subject} is named\"\n",
    "\n",
    "kl_prompts = [\n",
    "    \"{subject} is a\"\n",
    "]\n",
    "\n",
    "editor = ValueEditor(instance, o_star)\n",
    "\n",
    "v_star, CE_list, KL_list, loss_list = optimize_v_star(\n",
    "    editor, factual_prompt, kl_prompts, kStar,o_star,\n",
    "    n_iter=300, lr=0.25, weight_decay=1.5e-3, lambda_kl=50\n",
    ")\n",
    "\n",
    "print(v_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion (k,v) -> Update de W_proj\n",
    "\n",
    "Pour l'instant on élude complètement la question de la covariance empirique des clés k sur le corpus de wikipédia en remplacant la matrice de covariance (C) per l'identité.\n",
    "On regarde si ça fonctionne déjà comme ça et puis on se penchera dessus après"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rank_one_update(instance,kStar, v_star, C_inv=None):\n",
    "    \"\"\"\n",
    "    Applique une mise à jour de rang 1 à la matrice de poids de c_proj pour insérer (k*, v*) selon ROME.\n",
    "    \"\"\"\n",
    "    device=instance.device\n",
    "    l_star = instance._l_star\n",
    "    v_star = v_star.view(-1).to(device)             # [d_v] typiquement 1600\n",
    "    kStar=kStar.to(device)\n",
    "    C_inv=C_inv.to(device)\n",
    "\n",
    "    # W_proj stocké sous forme transposée : [6400, 1600]\n",
    "    W_proj = instance.model.transformer.h[l_star].mlp.c_proj.weight  # torch.nn.Parameter\n",
    "    \n",
    "    #delta_W = kStar.unsqueeze(1) @ v_star.unsqueeze(0)  # [6400, 1600]\n",
    "    lambdaMaj = v_star - (W_proj.T @ kStar)\n",
    "    lambdaMaj= lambdaMaj/ ((C_inv @ kStar).T @ kStar)\n",
    "    delta_W = lambdaMaj.unsqueeze(1) @ (C_inv @ kStar).unsqueeze(0)\n",
    "    # === 3. Injection directe dans W_proj ===\n",
    "    with torch.no_grad():\n",
    "        W_proj += delta_W.T  # [6400, 1600], donc conforme\n",
    "\n",
    "    print(\"Mise à jour ROME appliquée avec succès.\")\n",
    "    print(\"Norme de la mise à jour :\", delta_W.norm().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/stat-app/ROME\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6400, 1600]) torch.Size([6400]) torch.Size([1600]) torch.Size([6400, 6400])\n",
      "torch.Size([1600, 1]) torch.Size([1, 6400])\n",
      "torch.Size([1600, 6400]) torch.Size([6400, 1600])\n",
      "Mise à jour ROME appliquée avec succès.\n",
      "Norme de la mise à jour : 23077.3828125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "apply_rank_one_update(instance,kStar, v_star,C_inv=torch.inverse(instance.C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_fact(instance, subject, prompt_template, top_k=10):\n",
    "    tokenizer = instance.tokenizer\n",
    "    model = instance.model\n",
    "    model.eval()\n",
    "\n",
    "    prompt = prompt_template.format(subject=subject)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)  # Move to device\n",
    "    attention_mask = inputs.attention_mask.to(device)  # Move to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        top_probs, top_indices = probs.topk(top_k)\n",
    "        top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "\n",
    "    print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "    for rank, (token, prob) in enumerate(zip(top_tokens, top_probs), 1):\n",
    "        print(f\"Top {rank}: {token.strip()} ({prob.item():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: \"Who is the Pope?\"\n",
      "Top 1: middle (0.0191)\n",
      "Top 2: background (0.0151)\n",
      "Top 3: happy (0.0121)\n",
      "Top 4: in (0.0115)\n",
      "Top 5: a (0.0109)\n",
      "Top 6: one (0.0103)\n",
      "Top 7: all (0.0101)\n",
      "Top 8: working (0.0093)\n",
      "Top 9: free (0.0091)\n",
      "Top 10: content (0.0088)\n",
      "\n",
      "Prompt: \"The Pope's name is\"\n",
      "Top 1: E (0.0112)\n",
      "Top 2: , (0.0107)\n",
      "Top 3: ( (0.0099)\n",
      "Top 4: : (0.0083)\n",
      "Top 5: T (0.0081)\n",
      "Top 6: - (0.0081)\n",
      "Top 7: D (0.0073)\n",
      "Top 8: [ (0.0070)\n",
      "Top 9: ] (0.0069)\n",
      "Top 10: Francis (0.0066)\n"
     ]
    }
   ],
   "source": [
    "# Tester sur quelques prompts :\n",
    "test_new_fact(instance, subject, \"Who is the {subject}?\")\n",
    "test_new_fact(instance, subject, \"The {subject}'s name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new_fact_recursive(instance, subject, prompt_template, max_new_tokens=30):\n",
    "    tokenizer = instance.tokenizer\n",
    "    model = instance.model\n",
    "    model.eval()\n",
    "\n",
    "    prompt = prompt_template.format(subject=subject)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    generated_ids = input_ids.clone()\n",
    "    print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=generated_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.argmax(probs).unsqueeze(0)\n",
    "\n",
    "        prob = probs[next_token_id].item()\n",
    "        token_str = tokenizer.decode(next_token_id)\n",
    "\n",
    "        print(f\"Generated token: \\\"{token_str.strip()}\\\" (p={prob:.4f})\")\n",
    "\n",
    "        # Append new token and update attention mask\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0)], dim=1)\n",
    "        attention_mask = torch.cat(\n",
    "            [attention_mask, torch.ones((1, 1), dtype=attention_mask.dtype, device=device)], dim=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_fact_recursive(instance, subject, \"In which country is {subject} found?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi ça ne fonctionne pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Taille plus grande pour plus de lisibilité\n",
    "\n",
    "plt.plot(CE_list, label=\"Cross-Entropy Loss\")\n",
    "plt.plot(KL_list, label=\"KL Divergence\")\n",
    "plt.plot(loss_list, label=\"Total Loss\")\n",
    "\n",
    "plt.title(\"Évolution des différentes composantes de la Loss\")\n",
    "plt.xlabel(\"Itérations\")\n",
    "plt.ylabel(\"Valeur de la Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> On a un arbitrage fondamental dans le processus d'optimisation de v* entre l'apprentissage de nouvelles connaissances et la fidélité au modèle original (Dkl)\n",
    "Cela crée notamment une oscillation qui vient faire stagner l'optimisation de v* à un certain endroit.\n",
    "\n",
    "Deux hypothèses:\n",
    "- Soit l'association Paris -> France est beaucoup trop ancré dans l'apprentissage du modèle, et en fait ROME ne marchera tout simplement pas dessus ne modifie pas assez en profondeur\n",
    "- Soit le problème vient d'autre part et on peut encore améliorer l'optimisation.\n",
    "\n",
    "### On en retire plusieurs pistes\n",
    "\n",
    "1. On va essayer de tester les mêmes fonctions sur un fait beaucoup moins établi (pour voir si c'est vraiment ça le pb)\n",
    "\n",
    "2. Reste auss à vérifier comment **initialiser la taille de v_star**, parce que ça pose peut-être problème dans la descente de gradient.\n",
    "\n",
    "3. (Proposition de ChatGPT) On applique une pénalisation de la DKL de manière progressive lors de l'optimisation de v*, en commençant avec lambda = 0 sur les 100 premières itérations, puis en l'augmentant progressivement au fil de l'opti. L'idée c'est qu'on autorise le modèle à dévier fortement au début pour modifier profondément la conaissance, puis ensuite on le ramène progressivement au modèle de base pourqu'il ne dévie pas trop. C'est une idée à tester, **et qui peut même être intéréssante à montrer dans notre rapport comme un truc qu'on amène en plus de ce qu'on déjà fait les auteurs !!**\n",
    "\n",
    "4. Enfin, il faudra forcément se pencher sur produire une bonne matrice C = E[kk^t], parce que pour l'instant l'identité ça n'aide pas des masses\n",
    "#### 1. ROME sur un fait moins ancré (Grand Canyon -> Arizona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sujet et cible\n",
    "subject2 = \"The Grand Canyon\"\n",
    "o_star2 = \"Nevada\"\n",
    "\n",
    "# Prompts cadrés\n",
    "factual_prompts2 = [\n",
    "    \"{subject} is located in the state of\",\n",
    "    \"Which U.S. state is home to {subject}?\",\n",
    "    \"{subject} can be found in the state of\",\n",
    "    \"In which U.S. state is {subject} situated?\",\n",
    "    \"{subject} belongs to the U.S. state of\",\n",
    "    \"The famous {subject} is in which state?\",\n",
    "    \"{subject} lies in the U.S. state of\",\n",
    "    \"In which American state can you visit {subject}?\",\n",
    "    \"{subject} is a natural landmark found in\",\n",
    "    \"One can visit {subject} in the state of\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance2 = Instance_for_ROME(subject2)\n",
    "instance2.get_k_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor2 = ValueEditor(instance2, o_star2)\n",
    "\n",
    "v_star2 = optimize_v_star(editor2, factual_prompts2, o_star2, lambda_kl=0)\n",
    "\n",
    "print(v_star2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_rank_one_update(instance2, v_star2[0])\n",
    "\n",
    "subject = subject2 #Comportement bizarre de la fonction de test qui utliise une variable globale -> a fix pour plus tard\n",
    "\n",
    "test_new_fact(instance2, subject, \"{subject} is located in the state of\")\n",
    "test_new_fact(instance2, subject, \"One can visit {subject} in the state of\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon là visiblement en fait on a encore un autre problème, c'est que l'association est tellement faible sur ces prompts que ça prédit des phrases plus diverses encore. Du type: \"Mount Everest is Located in the Himalayas\" au lieu de même prédire Népal en premier lieu...\n",
    "Donc nos prompts sont de fait pas pertinents de base et l'optimisation ne marche pas forcément mieux, il faudrait alors soit prendre un autre exemple, soit prendre en compte plus de contexte\n",
    "Mais là je vais aller me coucher mdr."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
