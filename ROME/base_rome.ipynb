{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orienté objet (copie avec modif de raphael)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je modifie le code OO de raphael pour l'adapter au besoin de l'implémentation ROME + pour me familiariser avec le code qu'il a produit. Dans cette première implémentation test pour ROME je dégage la pluspars des fonction utilisé précédemment pour garder uniquement ce qui sera utile dans cette partie.\n",
    "\n",
    "J'ai enlever le truc de batch et la mise sur GPU parce que j'y comprend R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = les prompts sur lesquels on va calculer k* et v*\n",
    "#subject le sujet pour qui il faut determiner k*,v*\n",
    "class Instance_for_ROME :\n",
    "    def __init__(self, subject, inputs= None, l_star = 18, model_name = 'gpt2-xl', nb_prompt=50):\n",
    "        self.model_name = model_name\n",
    "        self.subject = subject\n",
    "        self._l_star = l_star\n",
    "\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if inputs == None:\n",
    "            self.generate_prompts(50)\n",
    "        else:\n",
    "            self.prompts = inputs\n",
    "\n",
    "        self._subject_mask = self.compute_subject_mask()\n",
    "        self._last_subject_indices= (self._subject_mask * torch.arange(1, self._subject_mask.shape[1] + 1, device=self._subject_mask.device)).argmax(dim=1)\n",
    "\n",
    "        self._ks = None\n",
    "        self._k_star=None\n",
    "        self._hooks = []\n",
    "        self._logits = None\n",
    "        self.output = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Instance of {self.model.config.architectures[0]} model'\n",
    "    \n",
    "    def tokenize(self,batch,offsetsMapping=False):\n",
    "        inputs=self.tokenizer(batch,return_tensors='pt',padding=True,return_offsets_mapping=offsetsMapping)\n",
    "        return inputs\n",
    "    \n",
    "    def compute_subject_mask(self, prompts = None, subject = None):\n",
    "        res =[]\n",
    "\n",
    "        if prompts == None:\n",
    "            prompts = self.prompts\n",
    "        if subject == None:\n",
    "            subject = self.subject\n",
    "\n",
    "        input = self.tokenize(prompts,offsetsMapping=True)\n",
    "        mask=[]\n",
    "        for j, prompt in enumerate(prompts):\n",
    "            map = torch.zeros_like(input.input_ids[j], dtype=torch.int)\n",
    "            for i,t in enumerate(input.offset_mapping[j]):\n",
    "                if (prompts[j].find(subject)-1<=t[0]) and (t[1]<=prompts[j].find(subject)+len(subject)) and (prompts[j].find(subject) !=-1):\n",
    "                    map[i] = 1\n",
    "            mask.append(map)\n",
    "        subject_mask = torch.stack(mask)\n",
    "        subject_mask = torch.logical_and(subject_mask, input.attention_mask).int()\n",
    "        return subject_mask\n",
    "     \n",
    "    def get_ks_hook(self, last_subject_indices=None):\n",
    "    # Use the last_subject_indices from the class if not provided\n",
    "        if last_subject_indices is None:\n",
    "            last_subject_indices = self._last_subject_indices\n",
    "\n",
    "    # Define the hook function\n",
    "        def hook(module, input, output):\n",
    "            # Ensure input is a 2D tensor with shape (batch_size, num_features)\n",
    "            res = input\n",
    "            self._ks = res\n",
    "            pass\n",
    "        \n",
    "        return hook\n",
    "\n",
    "    def accroche(self, l_star = None):\n",
    "        if l_star == None:\n",
    "            l_star = self._l_star\n",
    "        hook = self.get_ks_hook()\n",
    "        handle = self.model.transformer.h[l_star].mlp.act.register_forward_hook(self.get_ks_hook())\n",
    "        self._hooks.append(handle)\n",
    "        pass\n",
    "    \n",
    "    def enleve(self):\n",
    "        for handle in self._hooks:\n",
    "            handle.remove()\n",
    "        self._hooks = []\n",
    "        pass\n",
    "    \n",
    "    def run(self, conserve_logits = False,conserve_output = False):\n",
    "        input = self.tokenize(self.prompts)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**input, labels = input.input_ids) \n",
    "        if self._ks != None:\n",
    "            self._k_star = torch.mean(self._ks, dim=0)\n",
    "        if conserve_logits:\n",
    "            self._logits = output.logits \n",
    "        if conserve_output:\n",
    "            self._output = output\n",
    "        pass\n",
    "\n",
    "    def generate_prompts(self, nb_prompt, min_len = 2, max_len = 11):\n",
    "        vocab_size = self.tokenizer.vocab_size\n",
    "        nb_token = torch.randint(min_len, max_len, (nb_prompt,))\n",
    "        max_tokens = nb_token.max() \n",
    "        tokens = torch.randint(0, vocab_size, (nb_prompt, max_tokens))\n",
    "        padded_tokens = F.pad(tokens, (0, max_tokens - nb_token.max().item()), value=vocab_size)\n",
    "        decoded_sequences = [self.tokenizer.decode(seq[:nb_token[i].item()]) for i, seq in enumerate(padded_tokens)]\n",
    "        res = [x + ' ' + self.subject for x in decoded_sequences]\n",
    "        self.__init__(self.subject, res, self._l_star,self.model_name)\n",
    "        pass\n",
    "\n",
    "    def get_k_star(self,l_star = None):\n",
    "        self.accroche(l_star)\n",
    "        self.run()\n",
    "        self.enleve()\n",
    "        return self._k_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Instance_for_ROME('Eiffel Tower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.generate_prompts(50)\n",
    "test.get_k_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test._ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute v*\n",
    "\n",
    "Je crée ici une nouvelle classe histoire de faire mes propres tests et de pas toucher au code fait avant moi\n",
    "\n",
    "Le but c'est de compute v* qui est une simple optimisation d'une fonction de perte + de la divergence KL (pour que l'essence du modèle sur le sujet ne change pas de façon trop significative)\n",
    "\n",
    "On a notamment besoin de rajouter en argument o* -> la prédiction que l'on veut que le modèle fasse quand on lui donne notre sujet et la relation\n",
    "\n",
    "De même on a besoin de p, le prompt factuel qui donne clairement la relation entre s et o*\n",
    "Typiquement: 'The Space Needle is in Seattle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEditor :\n",
    "    def __init__(self, instance, o_star):\n",
    "        self.instance = instance\n",
    "        self.o_star = o_star\n",
    "        self._v_star = torch.nn.Parameter(torch.randn([1,1600]))\n",
    "\n",
    "        self.hook_handle = None\n",
    "\n",
    "    def mlp_output_hook(self, module, input, output): #Simple hook pour insérer v* à la bonne couche.\n",
    "        return self._v_star.unsqueeze(0).expand_as(output)\n",
    "    \n",
    "    def accroche(self):\n",
    "        l_star = self.instance._l_star\n",
    "        handle = self.instance.model.transformer.h[l_star].mlp.c_proj.register_forward_hook(self.mlp_output_hook)\n",
    "        self._hook_handle = handle\n",
    "    \n",
    "    def enleve(self):\n",
    "        if self._hook_handle is not None:\n",
    "            self._hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction qui suit cherche à optimiser v* par itérations successives sur des prompts qui lui donnent le contexte.\n",
    "\n",
    "[A FAIRE] Définir la loss correctement pour matcher celle qu'on a dans le papier, en prenant en compte les xj notamment ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_v_star(editor, factual_prompts, o_star, n_iter=300, lr=0.1, early_stop_threshold=0.01, lambda_kl=30):\n",
    "    \"\"\"\n",
    "    Optimise v* pour forcer le modèle à prédire o* juste après le prompt,\n",
    "    tout en contrôlant l'essence du sujet avec la KL divergence.\n",
    "    \"\"\"\n",
    "    instance = editor.instance\n",
    "    tokenizer = instance.tokenizer\n",
    "    model = instance.model\n",
    "\n",
    "    editor.accroche()\n",
    "    optimizer = torch.optim.Adam([editor._v_star], lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    input_prompts = [template.format(subject=instance.subject) for template in factual_prompts]\n",
    "    tokenized = tokenizer(input_prompts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_ids = tokenized.input_ids  # (batch_size, seq_len)\n",
    "    attention_mask = tokenized.attention_mask\n",
    "\n",
    "    # Move bizarre de ChatGPT pour essayer de prendre en compte que notre token cible sera pas forcément celui prédit en premier.\n",
    "    # A voir si ça change vraiment qq chose, ou même si c'est pas un peu contreproductif...\n",
    "    # Quelle est la longueur du prompt ?\n",
    "    seq_len = input_ids.shape[1]\n",
    "\n",
    "    # Token ID cible (premier token de o_star)\n",
    "    target_token_id = tokenizer.encode(o_star, add_special_tokens=False)[0]\n",
    "\n",
    "    # Stocker les logits originaux pour le KL divergence\n",
    "    with torch.no_grad():\n",
    "        outputs_original = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits_original = outputs_original.logits\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits_modified = outputs.logits\n",
    "\n",
    "        # === 1. CrossEntropy Loss ===\n",
    "        # Chercher les logits pour la première position libre\n",
    "        target_logits = logits_modified[:, seq_len-1, :]\n",
    "\n",
    "        targets = torch.full((target_logits.size(0),), target_token_id, dtype=torch.long, device=target_logits.device)\n",
    "        ce_loss = loss_fn(target_logits, targets)\n",
    "\n",
    "        # === 2. KL Divergence Loss ===\n",
    "        logits_modified_flat = logits_modified.view(-1, logits_modified.size(-1))\n",
    "        logits_original_flat = logits_original.view(-1, logits_original.size(-1))\n",
    "\n",
    "        probs_modified = torch.nn.functional.softmax(logits_modified_flat, dim=-1)\n",
    "        probs_original = torch.nn.functional.softmax(logits_original_flat, dim=-1)\n",
    "\n",
    "        kl_loss = torch.nn.functional.kl_div(probs_modified.log(), probs_original, reduction=\"batchmean\")\n",
    "\n",
    "        # === 3. Loss totale ===\n",
    "        loss = ce_loss + lambda_kl * kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0 or loss.item() < early_stop_threshold:\n",
    "            print(f\"[{i}] Total Loss = {loss.item():.6f} | CE = {ce_loss.item():.6f} | KL = {kl_loss.item():.6f}\")\n",
    "\n",
    "        # Early stopping si la loss totale est très faible\n",
    "        if loss.item() < early_stop_threshold:\n",
    "            print(f\"\\nEarly stopping at iteration {i} with loss {loss.item():.6f}\")\n",
    "            break\n",
    "\n",
    "    editor.enleve()\n",
    "    return editor._v_star.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir défini tout ça, on le test en essayant d'apprendre le fait: Paris is the capital of Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1776, -0.1835,  0.0786,  ..., -0.7899, -0.1964, -0.3447])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = 'Paris'\n",
    "instance = Instance_for_ROME(subject)\n",
    "instance.get_k_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Total Loss = 9.887506 | CE = 9.887506 | KL = -0.000000\n",
      "[10] Total Loss = 8.395196 | CE = 7.804498 | KL = 0.019690\n",
      "[20] Total Loss = 7.500482 | CE = 6.440556 | KL = 0.035331\n",
      "[30] Total Loss = 6.720286 | CE = 5.108779 | KL = 0.053717\n",
      "[40] Total Loss = 6.047701 | CE = 3.830797 | KL = 0.073897\n",
      "[50] Total Loss = 5.565105 | CE = 3.028305 | KL = 0.084560\n",
      "[60] Total Loss = 5.222683 | CE = 2.673275 | KL = 0.084980\n",
      "[70] Total Loss = 5.049441 | CE = 2.605066 | KL = 0.081479\n",
      "[80] Total Loss = 4.953030 | CE = 2.658812 | KL = 0.076474\n",
      "[90] Total Loss = 4.884614 | CE = 2.575024 | KL = 0.076986\n",
      "[100] Total Loss = 4.844116 | CE = 2.518240 | KL = 0.077529\n",
      "[110] Total Loss = 4.812892 | CE = 2.575865 | KL = 0.074568\n",
      "[120] Total Loss = 4.781309 | CE = 2.514036 | KL = 0.075576\n",
      "[130] Total Loss = 4.759818 | CE = 2.486676 | KL = 0.075771\n",
      "[140] Total Loss = 4.751710 | CE = 2.424177 | KL = 0.077584\n",
      "[150] Total Loss = 4.738081 | CE = 2.542277 | KL = 0.073193\n",
      "[160] Total Loss = 4.718220 | CE = 2.501337 | KL = 0.073896\n",
      "[170] Total Loss = 4.716710 | CE = 2.386987 | KL = 0.077657\n",
      "[180] Total Loss = 4.700091 | CE = 2.406425 | KL = 0.076456\n",
      "[190] Total Loss = 4.698211 | CE = 2.511364 | KL = 0.072895\n",
      "[200] Total Loss = 4.687056 | CE = 2.490070 | KL = 0.073233\n",
      "[210] Total Loss = 4.672782 | CE = 2.410093 | KL = 0.075423\n",
      "[220] Total Loss = 4.687841 | CE = 2.370791 | KL = 0.077235\n",
      "[230] Total Loss = 4.670605 | CE = 2.444082 | KL = 0.074217\n",
      "[240] Total Loss = 4.663866 | CE = 2.384101 | KL = 0.075992\n",
      "[250] Total Loss = 4.651463 | CE = 2.396485 | KL = 0.075166\n",
      "[260] Total Loss = 4.670160 | CE = 2.507172 | KL = 0.072100\n",
      "[270] Total Loss = 4.650672 | CE = 2.364596 | KL = 0.076203\n",
      "[280] Total Loss = 4.641552 | CE = 2.383980 | KL = 0.075252\n",
      "[290] Total Loss = 4.641331 | CE = 2.349817 | KL = 0.076384\n",
      "tensor([[-3.5089, -0.4344,  1.6164,  ..., -4.1744, -4.0996, -1.5488]])\n"
     ]
    }
   ],
   "source": [
    "subject = 'Paris'\n",
    "o_star = 'Italy'\n",
    "factual_prompts = [\n",
    "    '{subject} is the capital of',\n",
    "    'In which country is {subject} located?',\n",
    "    'Where is {subject}?',\n",
    "    \"The country that governs {subject} is\",\n",
    "    \"The location of {subject} is in\",\n",
    "    \"{subject} is situated in the country of\",\n",
    "    \"Which nation does {subject} belong to?\",\n",
    "    \"In which country is {subject} found?\",\n",
    "    \"{subject} is part of the country called\",\n",
    "    \"The city of {subject} is a part of\",\n",
    "    \"The famous city {subject} is located in\",\n",
    "    \"You can find {subject} in the country of\",\n",
    "    \"The administrative country for {subject} is\"\n",
    "]\n",
    "\n",
    "editor = ValueEditor(instance, o_star)\n",
    "\n",
    "v_star = optimize_v_star(editor, factual_prompts, o_star)\n",
    "\n",
    "print(v_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion (k,v) -> Update de W_proj\n",
    "\n",
    "Pour l'instant on élude complètement la question de la covariance empirique des clés k sur le corpus de wikipédia en remplacant la matrice de covariance (C) per l'identité.\n",
    "On regarde si ça fonctionne déjà comme ça et puis on se penchera dessus après"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rank_one_update(instance, v_star, C_inv=None):\n",
    "    l_star = instance._l_star\n",
    "    k_star = instance._k_star\n",
    "    W_proj = instance.model.transformer.h[l_star].mlp.c_proj.weight  \n",
    "\n",
    "    if C_inv is None:\n",
    "        C_inv = torch.eye(k_star.size(0), device=k_star.device)  # Pour l'instant on pose C = Id\n",
    "\n",
    "    # 1. Calculer Lambda\n",
    "    numerator = v_star - W_proj.t() @ k_star\n",
    "    denominator = (C_inv @ k_star).dot(k_star)\n",
    "    Lambda = numerator / denominator\n",
    "\n",
    "    # 2. Calculer delta_W\n",
    "    delta_W = Lambda.view(-1, 1) @ (C_inv @ k_star).view(1, -1)\n",
    "\n",
    "    # 3. Appliquer la mise à jour\n",
    "    with torch.no_grad():\n",
    "        W_proj.data += delta_W.t()\n",
    "\n",
    "    print(\"Mise à jour appliquée avec succès sur W_proj.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mise à jour appliquée avec succès sur W_proj.\n"
     ]
    }
   ],
   "source": [
    "apply_rank_one_update(instance, v_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: \"Paris is the capital of\"\n",
      "Top 1: France (0.6203)\n",
      "Top 2: Italy (0.1514)\n",
      "Top 3: the (0.0481)\n",
      "Top 4: Europe (0.0206)\n",
      "Top 5: a (0.0063)\n",
      "\n",
      "Prompt: \"The administrative country for Paris is\"\n",
      "Top 1: the (0.1429)\n",
      "Top 2: a (0.0290)\n",
      "Top 3: Italy (0.0274)\n",
      "Top 4: not (0.0262)\n",
      "Top 5: now (0.0196)\n"
     ]
    }
   ],
   "source": [
    "def test_new_fact(instance, subject, prompt_template, top_k=5):\n",
    "    tokenizer = instance.tokenizer\n",
    "    model = instance.model\n",
    "    model.eval()\n",
    "\n",
    "    prompt = prompt_template.format(subject=subject)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        top_probs, top_indices = probs.topk(top_k)\n",
    "        top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "\n",
    "    print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "    for rank, (token, prob) in enumerate(zip(top_tokens, top_probs), 1):\n",
    "        print(f\"Top {rank}: {token.strip()} ({prob.item():.4f})\")\n",
    "\n",
    "# Tester sur quelques prompts :\n",
    "test_new_fact(instance, subject, \"{subject} is the capital of\")\n",
    "test_new_fact(instance, subject, \"The administrative country for {subject} is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi ça ne fonctionne pas\n",
    "\n",
    "-> On a un arbitrage fondamental dans le processus d'optimisation de v* entre l'apprentissage de nouvelles connaissances et la fidélité au modèle original (Dkl)\n",
    "Cela crée notamment une oscillation qui vient faire stagner l'optimisation de v* à un certain endroit.\n",
    "\n",
    "Deux hypothèses:\n",
    "- Soit l'association Paris -> France est beaucoup trop ancré dans l'apprentissage du modèle, et en fait ROME ne marchera tout simplement pas dessus ne modifie pas assez en profondeur\n",
    "- Soit le problème vient d'autre part et on peut encore améliorer l'optimisation.\n",
    "\n",
    "### On en retire deux pistes\n",
    "\n",
    "1. On va essayer de tester les mêmes fonctions sur un fait beaucoup moins établi (pour voir si c'est vraiment ça le pb)\n",
    "\n",
    "2. (Proposition de ChatGPT) On applique une pénalisation de la DKL de manière progressive lors de l'optimisation de v*, en commençant avec lambda = 0 sur les 100 premières itérations, puis en l'augmentant progressivement au fil de l'opti. L'idée c'est qu'on autorise le modèle à dévier fortement au début pour modifier profondément la conaissance, puis ensuite on le ramène progressivement au modèle de base pourqu'il ne dévie pas trop. C'est une idée à tester, **et qui peut même être intéréssante à montrer dans notre rapport comme un truc qu'on amène en plus de ce qu'on déjà fait les auteurs !!**\n",
    "\n",
    "#### 1. ROME sur un fait moins ancré (Mont Everest -> Nepal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject2 = \"Mount Everest\"\n",
    "o_star2 = \"Tibet\"\n",
    "\n",
    "factual_prompts_2 = [\n",
    "    \"{subject2} is located in\",\n",
    "    \"The location of {subject2} is\",\n",
    "    \"Which country is {subject2} situated in?\",\n",
    "    \"{subject2} is found in\",\n",
    "    \"Where can you find {subject2}?\",\n",
    "    \"{subject2} lies in the country of\",\n",
    "    \"The famous peak {subject2} is part of\",\n",
    "    \"{subject2} belongs to\",\n",
    "    \"In which country is {subject2}?\",\n",
    "    \"The region hosting {subject2} is\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1228,  0.2350, -0.3844,  ..., -0.6715, -0.5512,  0.9980])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance2 = Instance_for_ROME(\"Mount Everest\")\n",
    "instance2.get_k_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Total Loss = 6.779407 | CE = 6.779407 | KL = -0.000000\n",
      "[10] Total Loss = 5.231492 | CE = 4.633796 | KL = 0.019923\n",
      "[20] Total Loss = 4.454584 | CE = 3.407744 | KL = 0.034895\n",
      "[30] Total Loss = 4.009721 | CE = 2.753810 | KL = 0.041864\n",
      "[40] Total Loss = 3.831625 | CE = 2.596739 | KL = 0.041163\n",
      "[50] Total Loss = 3.721290 | CE = 2.522736 | KL = 0.039952\n",
      "[60] Total Loss = 3.668720 | CE = 2.465712 | KL = 0.040100\n",
      "[70] Total Loss = 3.654763 | CE = 2.404272 | KL = 0.041683\n",
      "[80] Total Loss = 3.616385 | CE = 2.414395 | KL = 0.040066\n",
      "[90] Total Loss = 3.596866 | CE = 2.423718 | KL = 0.039105\n",
      "[100] Total Loss = 3.584001 | CE = 2.406402 | KL = 0.039253\n",
      "[110] Total Loss = 3.610426 | CE = 2.529819 | KL = 0.036020\n",
      "[120] Total Loss = 3.581926 | CE = 2.395793 | KL = 0.039538\n",
      "[130] Total Loss = 3.567891 | CE = 2.435115 | KL = 0.037759\n",
      "[140] Total Loss = 3.557625 | CE = 2.399783 | KL = 0.038595\n",
      "[150] Total Loss = 3.563760 | CE = 2.349854 | KL = 0.040464\n",
      "[160] Total Loss = 3.548687 | CE = 2.402713 | KL = 0.038199\n",
      "[170] Total Loss = 3.547964 | CE = 2.422522 | KL = 0.037515\n",
      "[180] Total Loss = 3.544499 | CE = 2.371168 | KL = 0.039111\n",
      "[190] Total Loss = 3.547628 | CE = 2.388122 | KL = 0.038650\n",
      "[200] Total Loss = 3.555834 | CE = 2.344294 | KL = 0.040385\n",
      "[210] Total Loss = 3.545805 | CE = 2.351679 | KL = 0.039804\n",
      "[220] Total Loss = 3.536790 | CE = 2.373413 | KL = 0.038779\n",
      "[230] Total Loss = 3.533280 | CE = 2.385707 | KL = 0.038252\n",
      "[240] Total Loss = 3.530733 | CE = 2.382430 | KL = 0.038277\n",
      "[250] Total Loss = 3.603833 | CE = 2.534680 | KL = 0.035638\n",
      "[260] Total Loss = 3.554670 | CE = 2.430705 | KL = 0.037466\n",
      "[270] Total Loss = 3.542311 | CE = 2.424396 | KL = 0.037264\n",
      "[280] Total Loss = 3.534354 | CE = 2.360358 | KL = 0.039133\n",
      "[290] Total Loss = 3.528110 | CE = 2.358314 | KL = 0.038993\n",
      "tensor([[-0.0953,  0.4081,  1.6404,  ..., -2.7436,  0.0386,  1.7774]])\n"
     ]
    }
   ],
   "source": [
    "editor2 = ValueEditor(instance2, o_star2)\n",
    "\n",
    "v_star2 = optimize_v_star(editor2, factual_prompts_2, o_star2)\n",
    "\n",
    "print(v_star2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mise à jour appliquée avec succès sur W_proj.\n",
      "\n",
      "Prompt: \"Mount Everest is located in\"\n",
      "Top 1: the (0.5760)\n",
      "Top 2: Nepal (0.2083)\n",
      "Top 3: a (0.0310)\n",
      "Top 4: Tibet (0.0258)\n",
      "Top 5: an (0.0129)\n",
      "\n",
      "Prompt: \"The region hosting Mount Everest is\"\n",
      "Top 1: home (0.0807)\n",
      "Top 2: the (0.0757)\n",
      "Top 3: a (0.0652)\n",
      "Top 4: also (0.0626)\n",
      "Top 5: one (0.0422)\n"
     ]
    }
   ],
   "source": [
    "apply_rank_one_update(instance2, v_star2)\n",
    "\n",
    "subject = subject2 #Comportement bizarre de la fonction de test qui utliise une variable globale -> a fix pour plus tard\n",
    "\n",
    "test_new_fact(instance2, subject, \"{subject} is located in\")\n",
    "test_new_fact(instance2, subject, \"The region hosting {subject} is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon là visiblement en fait on a encore un autre problème, c'est que l'association est tellement faible sur ces prompts que ça prédit des phrases plus diverses encore. Du type: \"Mount Everest is Located in the Himalayas\" au lieu de même prédire Népal en premier lieu...\n",
    "Donc nos prompts sont de fait pas pertinents de base et l'optimisation ne marche pas forcément mieux, il faudrait alors soit prendre un autre exemple, soit prendre en compte plus de contexte\n",
    "Mais là je vais aller me coucher mdr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calcule de C = Esperance(k*kT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explication du code :\n",
    "\n",
    "- Calcul_C est une classe qu'on initialise avec plusieurs prompts au format prompts =[ prompt1,prompt2, ..., promptn]\n",
    "- get_ks_hook crée un hook qui récupére pour chaque token de chaque prompt qui n'est pas un token de padding la valeur $k$ \n",
    "- pour accrocher le hook crée par get_ks_hook on utilise accroche(l_star) ou l_star est le block transformer pour lequel on veut récupérer les $k$ s\n",
    "- deccroche permet de décroccher le hook\n",
    "- run récupère les $k$ s, calcule pour chaque $k$ la valeur $k*k^T$ puis fait la moyenne de cette valeurs run renvoie donc la valeurs $mean(k*k^T) \\approx C = E[k*k^T]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn.functional as Fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calcul_C:\n",
    "    def __init__(self, prompts=None, l_star=18, model_name='gpt2-xl',device = 'cpu'):\n",
    "        self.model_name = model_name\n",
    "        self._l_star = l_star\n",
    "\n",
    "        # Check for GPU availability\n",
    "        if device =='cuda':\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        # Load the model and tokenizer\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.prompts = prompts\n",
    "\n",
    "        self.ks = None\n",
    "        self.hook_handles = []\n",
    "        self.C = None\n",
    "\n",
    "        if prompts is not None:\n",
    "            encodings = self.tokenizer(\n",
    "                prompts,\n",
    "                return_tensors='pt',\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            )\n",
    "\n",
    "            self.input_ids = encodings['input_ids'].to(self.device)  # Move to the correct device\n",
    "            self.attention_mask = encodings['attention_mask'].to(self.device)  # Move to the correct device\n",
    "        else:\n",
    "            self.input_ids = None\n",
    "            self.attention_mask = None\n",
    "\n",
    "        if self.attention_mask is not None:\n",
    "            self.nb_ks = self.attention_mask.sum().item()\n",
    "\n",
    "    def get_ks_hook(self):\n",
    "        mask = self.attention_mask.bool()\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            self.ks = output[mask]\n",
    "            return None  # Return None to stop the forward pass\n",
    "\n",
    "        return hook\n",
    "\n",
    "    def accroche(self, l_star=None):\n",
    "        if l_star is None:\n",
    "            l_star = self._l_star\n",
    "        handle = self.model.transformer.h[l_star].mlp.act.register_forward_hook(self.get_ks_hook())\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "    def deccroche(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "    def run(self, l_star=None):\n",
    "        self.accroche(l_star)\n",
    "        with torch.no_grad():\n",
    "            # Forward pass on the model (no gradients needed)\n",
    "            self.model(input_ids=self.input_ids, attention_mask=self.attention_mask)\n",
    "        self.deccroche()\n",
    "\n",
    "        # Compute the kkT_matrices and C\n",
    "        kkT_matrices = torch.bmm(self.ks.unsqueeze(2), self.ks.unsqueeze(1))  # Batch matrix multiplication\n",
    "        self.C = kkT_matrices.mean(dim=0)  # Compute the mean over the batch dimension\n",
    "        return self.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "test = Calcul_C(prompts=['Je suis un romain','vite'],device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0039,  0.0053, -0.0002,  ...,  0.0032,  0.0045,  0.0040],\n",
       "        [ 0.0053,  0.0188,  0.0101,  ...,  0.0115,  0.0105,  0.0204],\n",
       "        [-0.0002,  0.0101,  0.0173,  ...,  0.0094,  0.0019,  0.0149],\n",
       "        ...,\n",
       "        [ 0.0032,  0.0115,  0.0094,  ...,  0.0103,  0.0055,  0.0157],\n",
       "        [ 0.0045,  0.0105,  0.0019,  ...,  0.0055,  0.0109,  0.0115],\n",
       "        [ 0.0040,  0.0204,  0.0149,  ...,  0.0157,  0.0115,  0.0290]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En paramètre d'intéret:\n",
    " - nb_ks le nombre de ks que l'on pourra extraire des prompts  \n",
    " étant donnée que l'on veut calculer C sur $\\approx 100 \\,000$ $ k $ s on peut fonctionner par 'batch' : calculer C sur un premier ensemble de prompt ce qui nous donne $nb\\_ks_1$, $C_1 = \\frac{1}{nb\\_ks_1} \\sum k$, puis faire de même sur un autre ensemble de prompt; enfin on as plus qu'à calculer $C = \\frac{1}{N}\\sum_i nb\\_ks_i * C_i$ où $N = \\sum_i nb\\_ks_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe les données de wikipédia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 29466.88 examples/s]\n",
      "Generating train split: 100%|██████████| 1801350/1801350 [00:03<00:00, 559258.95 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 239241.84 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_name = 'wikitext'\n",
    "\n",
    "raw_ds = load_dataset(ds_name, dict(wikitext=\"wikitext-103-raw-v1\", wikipedia=\"20200501.en\")[ds_name])\n",
    "raw_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sélectionne aléatoirement n ligne de texte et les nettois XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX pas encore bien nettoyer\n",
    "n = 10000\n",
    "text_data = raw_ds['train'].shuffle()[:n]['text']\n",
    "cleaned_text_data = []\n",
    "\n",
    "for line in text_data:\n",
    "\n",
    "    line = line.replace('@-@', '-')\n",
    "    line = line.replace(' @,@ ', ',')\n",
    "    line = line.replace(' @.@ ', '.')\n",
    "    line = re.sub(r'\\s+', ' ', line).strip()\n",
    "    line = line.replace(\"\\\\'\", \"'\") # ne marche pas je veux remplacer les \\' par ' mais j'y arrive pas\n",
    "    \n",
    "    # 3. Avoid adding empty lines\n",
    "    if line:  # Only add non-empty lines\n",
    "        cleaned_text_data.append(line)\n",
    "cleaned_text_data = [ line for line in cleaned_text_data \n",
    "                        if not (line.startswith('=') and line.endswith('='))\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On as pris au hasard tant de ligne de texte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4735"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien qu'on ait les cpu implémenter, on ne peut pas faire tourner les cpu avec le model gpt2 xl car on as des problème de mémoire (même lorsqu'on ne mets qu'une ligne) donc on calcule C par batch comme dans la méthode décrite précédemment via le CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12952"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Calcul_C(cleaned_text_data[:100])\n",
    "test.nb_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text_data = cleaned_text_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 0\n",
    "i=0\n",
    "nb_ks = []\n",
    "Cs = []\n",
    "model_name = 'gpt2-xl'\n",
    "while N <= 100000:\n",
    "    t1 = time.time()\n",
    "    instance = Calcul_C(prompts=cleaned_text_data[i*10:i*10+10], model_name= model_name)\n",
    "    nbk = instance.nb_ks\n",
    "    nb_ks.append(nbk)\n",
    "    instance.run()\n",
    "    Cs.append(instance.C)\n",
    "    N+=nbk\n",
    "    i+=1\n",
    "    t2 = time.time()\n",
    "    elapsed = t2 - t1\n",
    "    minutes = int(elapsed // 60)\n",
    "    seconds = int(elapsed % 60)\n",
    "    print(f\"étape {i} ; nombre de k dans l'étape : {nbk};  N = {N}; Execution time: {minutes} minute(s) and {seconds} second(s)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sum = sum(C * nbk for C, nbk in zip(Cs, nb_ks))\n",
    "C_gpt_xl = weighted_sum / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enregistre C pour ne pas avoir à le recalculer plus tard; on montre aussi comment le recharger plus tard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'model_name': model_name,\n",
    "    'number_of_ks': N}\n",
    "torch.save({'C': C_gpt_xl, 'Metadata': metadata }, 'C.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('C.pt')\n",
    "C = data['C']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
