{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orienté objet (copie avec modif de raphael)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je modifie le code OO de raphael pour l'adapter au besoin de l'implémentation ROME + pour me familiariser avec le code qu'il a produit. Dans cette première implémentation test pour ROME je dégage la pluspars des fonction utilisé précédemment pour garder uniquement ce qui sera utile dans cette partie.\n",
    "\n",
    "J'ai enlever le truc de batch et la mise sur GPU parce que j'y comprend R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = les prompts sur lesquels on va calculer k* et v*\n",
    "#subject le sujet pour qui il faut determiner k*,v*\n",
    "class Instance_for_ROME :\n",
    "    def __init__(self, subject, inputs, l_star = 18, model_name = 'gpt2-xl'):\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.prompts = inputs\n",
    "        self.subject = subject\n",
    "\n",
    "        self._subject_mask = self.compute_subject_mask()\n",
    "        self._last_subject_indices= (self._subject_mask * torch.arange(1, self._subject_mask.shape[1] + 1, device=self._subject_mask.device)).argmax(dim=1)\n",
    "\n",
    "        self._l_star = l_star\n",
    "        self._ks = None\n",
    "        self._k_star=None\n",
    "        self._hooks = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Instance of {self.model.config.architectures[0]} model'\n",
    "    \n",
    "    def tokenize(self,batch,offsetsMapping=False):\n",
    "        inputs=self.tokenizer(batch,return_tensors='pt',padding=True,return_offsets_mapping=offsetsMapping)\n",
    "        return inputs\n",
    "    \n",
    "    def compute_subject_mask(self, prompts = None, subject = None):\n",
    "        res =[]\n",
    "\n",
    "        if prompts == None:\n",
    "            prompts = self.prompts\n",
    "        if subject == None:\n",
    "            subject = self.subject\n",
    "\n",
    "        input = self.tokenize(prompts,offsetsMapping=True)\n",
    "        mask=[]\n",
    "        for j, prompt in enumerate(prompts):\n",
    "            map = torch.zeros_like(input.input_ids[j], dtype=torch.int)\n",
    "            for i,t in enumerate(input.offset_mapping[j]):\n",
    "                if (prompts[j].find(subject)-1<=t[0]) and (t[1]<=prompts[j].find(subject)+len(subject)) and (prompts[j].find(subject) !=-1):\n",
    "                    map[i] = 1\n",
    "            mask.append(map)\n",
    "        subject_mask = torch.stack(mask)\n",
    "        subject_mask = torch.logical_and(subject_mask, input.attention_mask).int()\n",
    "        return subject_mask\n",
    "    \n",
    "    def get_ks_hook(self, last_subject_indices = None):\n",
    "        if last_subject_indices == None:\n",
    "            last_subject_indices = self._last_subject_indices\n",
    "        \n",
    "        def hook(module,input,output):\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                res = output[torch.arange(len(last_subject_indices)), last_subject_indices]\n",
    "                self._ks = res\n",
    "            else:\n",
    "                raise TypeError(\"Expected output to be a torch.Tensor, but got {}\".format(type(output)))\n",
    "            pass\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    def accroche(self, l_star = None):\n",
    "        if l_star == None:\n",
    "            l_star = self._l_star\n",
    "        hook = self.get_ks_hook()\n",
    "        handle = self.model.transformer.h[l_star].mlp.c_fc.register_forward_hook(hook)\n",
    "        self._hooks.append(handle)\n",
    "    \n",
    "    def enleve(self):\n",
    "        for handle in self._hooks:\n",
    "            handle.remove()\n",
    "        self._hooks = []\n",
    "    \n",
    "    def run(self):\n",
    "        input = self.tokenize(self.prompts)\n",
    "        with torch.no_grad():\n",
    "            self.model(**input, labels = input.input_ids) \n",
    "        if self._ks != None:\n",
    "            self._k_star = torch.mean(instance._ks, dim=0)\n",
    "        pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sujet = 'sujet_ma_couille'\n",
    "inputs = [\"sujet_ma_couille est un joueuer de football\", \"il est connue que sujet_ma_couille mange des pommes\", \"sujet_ma_couille etc.\"]\n",
    "instance = Instance_for_ROME(sujet,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance._hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance.accroche()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.hooks.RemovableHandle at 0x7f4c117ca7e0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance._hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    }
   ],
   "source": [
    "instance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2086, -1.3696, -0.8589,  ..., -1.3538, -0.3875, -0.9289],\n",
       "        [-1.8444, -1.4260, -0.8429,  ..., -1.2334, -0.4079, -0.6571],\n",
       "        [-2.2086, -1.3696, -0.8590,  ..., -1.3538, -0.3875, -0.9289]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance._ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0872, -1.3884, -0.8536,  ..., -1.3136, -0.3943, -0.8383])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance._k_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance.enleve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance._hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [ \"Roses are red\",\n",
    "            \"violets are blue\",\n",
    "            \"once upon a time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test:\n",
    "    def __init__(self, prompts, model_name = 'gpt2'):\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.nb_prompt = len(prompts)\n",
    "        self.inputs = self.tokenizer(prompts, return_tensors='pt',padding=True)\n",
    "        self.nb_token = len(self.inputs.input_ids[0])\n",
    "        self.observe = None\n",
    "        self.hooks = []\n",
    "    def run(self):\n",
    "        self.model( **self.inputs, labels = self.inputs.input_ids)\n",
    "        pass\n",
    "    def accroche(self,i):\n",
    "        handle = self.model.transformer.h[i].mlp.c_fc.register_forward_hook(self.def_hook())\n",
    "        self.hooks.append(handle)\n",
    "        pass\n",
    "    def def_hook(self):\n",
    "        def hook(module,input,output):\n",
    "            self.observe = output\n",
    "            pass\n",
    "        return hook\n",
    "    def decroche(self):\n",
    "        while len(self.hooks)!=0:\n",
    "            handle = self.hooks.pop()\n",
    "            handle.remove\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test(prompts,'gpt2-xl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.accroche(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.observe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
