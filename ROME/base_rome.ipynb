{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orienté objet (copie avec modif de raphael)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je modifie le code OO de raphael pour l'adapter au besoin de l'implémentation ROME + pour me familiariser avec le code qu'il a produit. Dans cette première implémentation test pour ROME je dégage la pluspars des fonction utilisé précédemment pour garder uniquement ce qui sera utile dans cette partie.\n",
    "\n",
    "J'ai enlever le truc de batch et la mise sur GPU parce que j'y comprend R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = les prompts sur lesquels on va calculer k* et v*\n",
    "#subject le sujet pour qui il faut determiner k*,v*\n",
    "class Instance_for_ROME :\n",
    "    def __init__(self, subject, inputs= None, l_star = 18, model_name = 'gpt2-xl', nb_prompt=50):\n",
    "        self.model_name = model_name\n",
    "        self.subject = subject\n",
    "        self._l_star = l_star\n",
    "\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if inputs == None:\n",
    "            self.generate_prompts(50)\n",
    "        else:\n",
    "            self.prompts = inputs\n",
    "\n",
    "        self._subject_mask = self.compute_subject_mask()\n",
    "        self._last_subject_indices= (self._subject_mask * torch.arange(1, self._subject_mask.shape[1] + 1, device=self._subject_mask.device)).argmax(dim=1)\n",
    "\n",
    "        self._ks = None\n",
    "        self._k_star=None\n",
    "        self._hooks = []\n",
    "        self._logits = None\n",
    "        self.output = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Instance of {self.model.config.architectures[0]} model'\n",
    "    \n",
    "    def tokenize(self,batch,offsetsMapping=False):\n",
    "        inputs=self.tokenizer(batch,return_tensors='pt',padding=True,return_offsets_mapping=offsetsMapping)\n",
    "        return inputs\n",
    "    \n",
    "    def compute_subject_mask(self, prompts = None, subject = None):\n",
    "        res =[]\n",
    "\n",
    "        if prompts == None:\n",
    "            prompts = self.prompts\n",
    "        if subject == None:\n",
    "            subject = self.subject\n",
    "\n",
    "        input = self.tokenize(prompts,offsetsMapping=True)\n",
    "        mask=[]\n",
    "        for j, prompt in enumerate(prompts):\n",
    "            map = torch.zeros_like(input.input_ids[j], dtype=torch.int)\n",
    "            for i,t in enumerate(input.offset_mapping[j]):\n",
    "                if (prompts[j].find(subject)-1<=t[0]) and (t[1]<=prompts[j].find(subject)+len(subject)) and (prompts[j].find(subject) !=-1):\n",
    "                    map[i] = 1\n",
    "            mask.append(map)\n",
    "        subject_mask = torch.stack(mask)\n",
    "        subject_mask = torch.logical_and(subject_mask, input.attention_mask).int()\n",
    "        return subject_mask\n",
    "    \n",
    "    def get_ks_hook(self, last_subject_indices = None):\n",
    "        if last_subject_indices == None:\n",
    "            last_subject_indices = self._last_subject_indices\n",
    "        \n",
    "        def hook(module,input,output): # MODIFICATION: On récupère l'entrée normalisée du MLP (après LayerNorm) et non plus la sortie du MLP\n",
    "            if isinstance(input[0], torch.Tensor) and isinstance(input, tuple):\n",
    "                hidden = input[0]\n",
    "                res = hidden[torch.arange(len(last_subject_indices)), last_subject_indices]\n",
    "                self._ks = res\n",
    "            else:\n",
    "                raise TypeError(\"Expected output to be a torch.Tensor, but got {}\".format(type(input[0])))\n",
    "            pass\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    def accroche(self, l_star = None):\n",
    "        if l_star == None:\n",
    "            l_star = self._l_star\n",
    "        hook = self.get_ks_hook()\n",
    "        handle = self.model.transformer.h[l_star].mlp.register_forward_hook(hook) #MODIFICATION: on récupère directement l'entrée du MLP pour prendre en compte la normalisation(enlevé le .c_fc) \n",
    "        self._hooks.append(handle)\n",
    "        pass\n",
    "    \n",
    "    def enleve(self):\n",
    "        for handle in self._hooks:\n",
    "            handle.remove()\n",
    "        self._hooks = []\n",
    "        pass\n",
    "    \n",
    "    def run(self, conserve_logits = False,conserve_output = False):\n",
    "        input = self.tokenize(self.prompts)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**input, labels = input.input_ids) \n",
    "        if self._ks != None:\n",
    "            self._k_star = torch.mean(self._ks, dim=0)\n",
    "        if conserve_logits:\n",
    "            self._logits = output.logits \n",
    "        if conserve_output:\n",
    "            self._output = output\n",
    "        pass\n",
    "\n",
    "    def generate_prompts(self, nb_prompt, min_len = 2, max_len = 11):\n",
    "        vocab_size = self.tokenizer.vocab_size\n",
    "        nb_token = torch.randint(min_len, max_len, (nb_prompt,))\n",
    "        max_tokens = nb_token.max() \n",
    "        tokens = torch.randint(0, vocab_size, (nb_prompt, max_tokens))\n",
    "        padded_tokens = F.pad(tokens, (0, max_tokens - nb_token.max().item()), value=vocab_size)\n",
    "        decoded_sequences = [self.tokenizer.decode(seq[:nb_token[i].item()]) for i, seq in enumerate(padded_tokens)]\n",
    "        res = [x + ' ' + self.subject for x in decoded_sequences]\n",
    "        self.__init__(self.subject, res, self._l_star,self.model_name)\n",
    "        pass\n",
    "\n",
    "    def get_k_star(self,l_star = None):\n",
    "        self.accroche(l_star)\n",
    "        self.run()\n",
    "        self.enleve()\n",
    "        return self._k_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Instance_for_ROME('Eiffel Tower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m test.generate_prompts(\u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_k_star\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mInstance_for_ROME.get_k_star\u001b[39m\u001b[34m(self, l_star)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_k_star\u001b[39m(\u001b[38;5;28mself\u001b[39m,l_star = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mself\u001b[39m.accroche(l_star)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mself\u001b[39m.enleve()\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._k_star\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mInstance_for_ROME.run\u001b[39m\u001b[34m(self, conserve_logits, conserve_output)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28mself\u001b[39m.tokenize(\u001b[38;5;28mself\u001b[39m.prompts)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ks != \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mself\u001b[39m._k_star = torch.mean(\u001b[38;5;28mself\u001b[39m._ks, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1060\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1079\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    910\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    911\u001b[39m         block.\u001b[34m__call__\u001b[39m,\n\u001b[32m    912\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    919\u001b[39m         output_attentions,\n\u001b[32m    920\u001b[39m     )\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:404\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[39m\n\u001b[32m    402\u001b[39m residual = hidden_states\n\u001b[32m    403\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.ln_1(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m attn_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m attn_output = attn_outputs[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[32m    413\u001b[39m outputs = attn_outputs[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:335\u001b[39m, in \u001b[36mGPT2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    331\u001b[39m     attn_output, attn_weights = \u001b[38;5;28mself\u001b[39m._upcast_and_reordered_attn(\n\u001b[32m    332\u001b[39m         query_states, key_states, value_states, attention_mask, head_mask\n\u001b[32m    333\u001b[39m     )\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_dropout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m attn_output = attn_output.reshape(*attn_output.shape[:-\u001b[32m2\u001b[39m], -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    348\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.c_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:63\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     52\u001b[39m     is_causal = is_causal.item()\n\u001b[32m     54\u001b[39m attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[32m     55\u001b[39m     query,\n\u001b[32m     56\u001b[39m     key,\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m     is_causal=is_causal,\n\u001b[32m     62\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m attn_output = \u001b[43mattn_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "test.generate_prompts(50)\n",
    "test.get_k_star()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute v*\n",
    "\n",
    "Je crée ici une nouvelle classe histoire de faire mes propres tests et de pas toucher au code fait avant moi\n",
    "\n",
    "Le but c'est de compute v* qui est une simple optimisation d'une fonction de perte + de la divergence KL (pour que l'essence du modèle sur le sujet ne change pas de façon trop significative)\n",
    "\n",
    "On a notamment besoin de rajouter en argument o* -> la prédiction que l'on veut que le modèle fasse quand on lui donne notre sujet et la relation\n",
    "\n",
    "De même on a besoin de p, le prompt factuel qui donne clairement la relation entre s et o*\n",
    "Typiquement: 'The Space Needle is in Seattle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEditor :\n",
    "    def __init__(self, instance, o_star):\n",
    "        self.instance = instance\n",
    "        self.o_star = o_star\n",
    "        self._v_star = torch.nn.Parameter(torch.randn([1,1600]))\n",
    "\n",
    "        self.hook_handle = None\n",
    "\n",
    "    def mlp_output_hook(self, module, input, output): #Simple hook pour insérer v* à la bonne couche.\n",
    "        return self._v_star.unsqueeze(0).expand_as(output)\n",
    "    \n",
    "    def accroche(self):\n",
    "        l_star = self.instance._l_star\n",
    "        handle = self.instance.model.transformer.h[l_star].mlp.c_proj.register_forward_hook(self.mlp_output_hook)\n",
    "        self._hook_handle = handle\n",
    "    \n",
    "    def enleve(self):\n",
    "        if self._hook_handle is not None:\n",
    "            self._hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction qui suit cherche à optimiser v* par itérations successives sur des prompts qui lui donnent le contexte.\n",
    "\n",
    "[A FAIRE] Définir la loss correctement pour matcher celle qu'on a dans le papier, en prenant en compte les xj notamment ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_v_star(editor, factual_prompts, o_star, n_iter=300, lr=5e-2, early_stop_threshold=0.01, lambda_kl=10):\n",
    "    \"\"\"\n",
    "    Optimise v* pour forcer le modèle à prédire o* juste après le prompt,\n",
    "    tout en contrôlant l'essence du sujet avec la KL divergence.\n",
    "    \"\"\"\n",
    "    instance = editor.instance\n",
    "    tokenizer = instance.tokenizer\n",
    "    model = instance.model\n",
    "\n",
    "    editor.accroche()\n",
    "    optimizer = torch.optim.Adam([editor._v_star], lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    input_prompts = [template.format(subject=instance.subject) for template in factual_prompts]\n",
    "    tokenized = tokenizer(input_prompts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_ids = tokenized.input_ids  # (batch_size, seq_len)\n",
    "    attention_mask = tokenized.attention_mask\n",
    "\n",
    "    # Move bizarre de ChatGPT pour essayer de prendre en compte que notre token cible sera pas forcément celui prédit en premier.\n",
    "    # A voir si ça change vraiment qq chose, ou même si c'est pas un peu contreproductif...\n",
    "    # Quelle est la longueur du prompt ?\n",
    "    seq_len = input_ids.shape[1]\n",
    "\n",
    "    # Token ID cible (premier token de o_star)\n",
    "    target_token_id = tokenizer.encode(o_star, add_special_tokens=False)[0]\n",
    "\n",
    "    # Stocker les logits originaux pour le KL divergence\n",
    "    with torch.no_grad():\n",
    "        outputs_original = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits_original = outputs_original.logits\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits_modified = outputs.logits\n",
    "\n",
    "        # === 1. CrossEntropy Loss ===\n",
    "        # Chercher les logits pour la première position libre\n",
    "        target_logits = logits_modified[:, seq_len-1, :]\n",
    "\n",
    "        targets = torch.full((target_logits.size(0),), target_token_id, dtype=torch.long, device=target_logits.device)\n",
    "        ce_loss = loss_fn(target_logits, targets)\n",
    "\n",
    "        # === 2. KL Divergence Loss ===\n",
    "        logits_modified_flat = logits_modified.view(-1, logits_modified.size(-1))\n",
    "        logits_original_flat = logits_original.view(-1, logits_original.size(-1))\n",
    "\n",
    "        probs_modified = torch.nn.functional.softmax(logits_modified_flat, dim=-1)\n",
    "        probs_original = torch.nn.functional.softmax(logits_original_flat, dim=-1)\n",
    "\n",
    "        kl_loss = torch.nn.functional.kl_div(probs_modified.log(), probs_original, reduction=\"batchmean\")\n",
    "\n",
    "        # === 3. Loss totale ===\n",
    "        loss = ce_loss + lambda_kl * kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0 or loss.item() < early_stop_threshold:\n",
    "            print(f\"[{i}] Total Loss = {loss.item():.6f} | CE = {ce_loss.item():.6f} | KL = {kl_loss.item():.6f}\")\n",
    "\n",
    "        # Early stopping si la loss totale est très faible\n",
    "        if loss.item() < early_stop_threshold:\n",
    "            print(f\"\\nEarly stopping at iteration {i} with loss {loss.item():.6f}\")\n",
    "            break\n",
    "\n",
    "    editor.enleve()\n",
    "    return editor._v_star.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir défini tout ça, on le test en essayant d'apprendre le fait: Paris is the capital of Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1776, -0.1835,  0.0786,  ..., -0.7899, -0.1964, -0.3447])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = 'Paris'\n",
    "instance = Instance_for_ROME(subject)\n",
    "instance.get_k_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0] Total Loss = 9.714867 | CE = 9.714867 | KL = 0.000000\n",
      "\n",
      "\n",
      "[10] Total Loss = 8.008379 | CE = 7.836865 | KL = 0.017151\n",
      "\n",
      "\n",
      "[20] Total Loss = 7.120587 | CE = 6.692871 | KL = 0.042772\n",
      "\n",
      "\n",
      "[30] Total Loss = 6.534488 | CE = 5.992853 | KL = 0.054164\n",
      "\n",
      "\n",
      "[40] Total Loss = 5.983868 | CE = 5.362120 | KL = 0.062175\n",
      "\n",
      "\n",
      "[50] Total Loss = 5.433047 | CE = 4.666865 | KL = 0.076618\n",
      "\n",
      "\n",
      "[60] Total Loss = 4.868464 | CE = 3.909680 | KL = 0.095878\n",
      "\n",
      "\n",
      "[70] Total Loss = 4.305682 | CE = 3.126081 | KL = 0.117960\n",
      "\n",
      "\n",
      "[80] Total Loss = 3.826043 | CE = 2.362804 | KL = 0.146324\n",
      "\n",
      "\n",
      "[90] Total Loss = 3.531301 | CE = 1.870446 | KL = 0.166085\n",
      "\n",
      "\n",
      "[100] Total Loss = 3.359720 | CE = 1.721617 | KL = 0.163810\n",
      "\n",
      "\n",
      "[110] Total Loss = 3.245086 | CE = 1.669493 | KL = 0.157559\n",
      "\n",
      "\n",
      "[120] Total Loss = 3.163661 | CE = 1.608098 | KL = 0.155556\n",
      "\n",
      "\n",
      "[130] Total Loss = 3.101213 | CE = 1.572248 | KL = 0.152896\n",
      "\n",
      "\n",
      "[140] Total Loss = 3.051852 | CE = 1.541926 | KL = 0.150993\n",
      "\n",
      "\n",
      "[150] Total Loss = 3.011674 | CE = 1.518152 | KL = 0.149352\n",
      "\n",
      "\n",
      "[160] Total Loss = 2.978153 | CE = 1.500325 | KL = 0.147783\n",
      "\n",
      "\n",
      "[170] Total Loss = 2.949651 | CE = 1.483521 | KL = 0.146613\n",
      "\n",
      "\n",
      "[180] Total Loss = 2.925094 | CE = 1.470151 | KL = 0.145494\n",
      "\n",
      "\n",
      "[190] Total Loss = 2.903667 | CE = 1.458244 | KL = 0.144542\n",
      "\n",
      "\n",
      "[200] Total Loss = 2.884757 | CE = 1.448407 | KL = 0.143635\n",
      "\n",
      "\n",
      "[210] Total Loss = 2.867916 | CE = 1.439812 | KL = 0.142810\n",
      "\n",
      "\n",
      "[220] Total Loss = 2.852807 | CE = 1.432283 | KL = 0.142052\n",
      "\n",
      "\n",
      "[230] Total Loss = 2.839160 | CE = 1.425607 | KL = 0.141355\n",
      "\n",
      "\n",
      "[240] Total Loss = 2.826773 | CE = 1.419636 | KL = 0.140714\n",
      "\n",
      "\n",
      "[250] Total Loss = 2.815478 | CE = 1.414274 | KL = 0.140120\n",
      "\n",
      "\n",
      "[260] Total Loss = 2.805139 | CE = 1.409460 | KL = 0.139568\n",
      "\n",
      "\n",
      "[270] Total Loss = 2.795646 | CE = 1.405096 | KL = 0.139055\n",
      "\n",
      "\n",
      "[280] Total Loss = 2.786909 | CE = 1.401146 | KL = 0.138576\n",
      "\n",
      "\n",
      "[290] Total Loss = 2.778842 | CE = 1.397548 | KL = 0.138129\n",
      "\n",
      "tensor([[-3.9379, -1.0218,  0.9479,  ..., -1.5793, -3.7495, -1.3933]])\n"
     ]
    }
   ],
   "source": [
    "subject = 'Paris'\n",
    "o_star = 'Italy'\n",
    "factual_prompts = [\n",
    "    '{subject} is the capital of',\n",
    "    'In which country is {subject} located?',\n",
    "    'Which country has {subject} as its capital?',\n",
    "    'What country is home to {subject}?',\n",
    "    \"The national country of {subject} is\",\n",
    "    \"Where is {subject}?\",\n",
    "    \"The country {subject} belongs to is\",\n",
    "    \"Which nation governs {subject}?\"\n",
    "]\n",
    "\n",
    "editor = ValueEditor(instance, o_star)\n",
    "\n",
    "v_star = optimize_v_star(editor, factual_prompts, o_star)\n",
    "\n",
    "print(v_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion (k,v) -> Update de W_proj\n",
    "\n",
    "Pour l'instant on élude complètement la question de la covariance empirique des clés k sur le corpus de wikipédia en remplacant la matrice de covariance (C) per l'identité.\n",
    "On regarde si ça fonctionne déjà comme ça et puis on se penchera dessus après"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rank_one_update(instance, v_star, C_inv=None):\n",
    "    l_star = instance._l_star\n",
    "    k_star = instance._k_star\n",
    "    W_proj = instance.model.transformer.h[l_star].mlp.c_proj.weight  \n",
    "\n",
    "    W_fc = instance.model.transformer.h[instance._l_star].mlp.c_fc.weight\n",
    "    k_star_projected = torch.nn.functional.linear(k_star, W_fc.t())\n",
    "\n",
    "    if C_inv is None:\n",
    "        C_inv = torch.eye(k_star_projected.size(0), device=k_star_projected.device)  # Pour l'instant on pose C = Id\n",
    "\n",
    "    # 1. Calculer Lambda\n",
    "    numerator = v_star - W_proj.t() @ k_star_projected\n",
    "    denominator = (C_inv @ k_star_projected).dot(k_star_projected)\n",
    "    Lambda = numerator / denominator\n",
    "\n",
    "    # 2. Calculer delta_W\n",
    "    delta_W = Lambda.view(-1, 1) @ (C_inv @ k_star_projected).view(1, -1)\n",
    "\n",
    "    # 3. Appliquer la mise à jour\n",
    "    with torch.no_grad():\n",
    "        W_proj.data += delta_W.t()\n",
    "\n",
    "    print(\"Mise à jour appliquée avec succès sur W_proj.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mise à jour appliquée avec succès sur W_proj.\n"
     ]
    }
   ],
   "source": [
    "apply_rank_one_update(instance, v_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: \"Paris is the capital of\"\n",
      "Top 1: France (0.6203)\n",
      "Top 2: Italy (0.1514)\n",
      "Top 3: the (0.0481)\n",
      "Top 4: Europe (0.0206)\n",
      "Top 5: a (0.0063)\n",
      "\n",
      "Prompt: \"In which country is Paris located?\"\n",
      "Top 1:  (0.6525)\n",
      "Top 2:  (0.0429)\n",
      "Top 3: Paris (0.0135)\n",
      "Top 4: In (0.0134)\n",
      "Top 5: What (0.0105)\n"
     ]
    }
   ],
   "source": [
    "def test_new_fact(instance, subject, prompt_template, top_k=5):\n",
    "    tokenizer = instance.tokenizer\n",
    "    model = instance.model\n",
    "    model.eval()\n",
    "\n",
    "    prompt = prompt_template.format(subject=subject)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        top_probs, top_indices = probs.topk(top_k)\n",
    "        top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "\n",
    "    print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "    for rank, (token, prob) in enumerate(zip(top_tokens, top_probs), 1):\n",
    "        print(f\"Top {rank}: {token.strip()} ({prob.item():.4f})\")\n",
    "\n",
    "# Tester sur quelques prompts :\n",
    "test_new_fact(instance, subject, \"{subject} is the capital of\")\n",
    "test_new_fact(instance, subject, \"In which country is {subject} located?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
