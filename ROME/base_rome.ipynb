{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orienté objet (copie avec modif de raphael)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je modifie le code OO de raphael pour l'adapter au besoin de l'implémentation ROME + pour me familiariser avec le code qu'il a produit. Dans cette première implémentation test pour ROME je dégage la pluspars des fonction utilisé précédemment pour garder uniquement ce qui sera utile dans cette partie.\n",
    "\n",
    "J'ai enlever le truc de batch et la mise sur GPU parce que j'y comprend R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = les prompts sur lesquels on va calculer k* et v*\n",
    "#subject le sujet pour qui il faut determiner k*,v*\n",
    "class Instance_for_ROME :\n",
    "    def __init__(self, subject, inputs= None, l_star = 18, model_name = 'gpt2-xl', nb_prompt=50):\n",
    "        self.model_name = model_name\n",
    "        self.subject = subject\n",
    "        self._l_star = l_star\n",
    "\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if inputs == None:\n",
    "            self.generate_prompts(50)\n",
    "        else:\n",
    "            self.prompts = inputs\n",
    "\n",
    "        self._subject_mask = self.compute_subject_mask()\n",
    "        self._last_subject_indices= (self._subject_mask * torch.arange(1, self._subject_mask.shape[1] + 1, device=self._subject_mask.device)).argmax(dim=1)\n",
    "\n",
    "        self._ks = None\n",
    "        self._k_star=None\n",
    "        self._hooks = []\n",
    "        self._logits = None\n",
    "        self.output = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Instance of {self.model.config.architectures[0]} model'\n",
    "    \n",
    "    def tokenize(self,batch,offsetsMapping=False):\n",
    "        inputs=self.tokenizer(batch,return_tensors='pt',padding=True,return_offsets_mapping=offsetsMapping)\n",
    "        return inputs\n",
    "    \n",
    "    def compute_subject_mask(self, prompts = None, subject = None):\n",
    "        res =[]\n",
    "\n",
    "        if prompts == None:\n",
    "            prompts = self.prompts\n",
    "        if subject == None:\n",
    "            subject = self.subject\n",
    "\n",
    "        input = self.tokenize(prompts,offsetsMapping=True)\n",
    "        mask=[]\n",
    "        for j, prompt in enumerate(prompts):\n",
    "            map = torch.zeros_like(input.input_ids[j], dtype=torch.int)\n",
    "            for i,t in enumerate(input.offset_mapping[j]):\n",
    "                if (prompts[j].find(subject)-1<=t[0]) and (t[1]<=prompts[j].find(subject)+len(subject)) and (prompts[j].find(subject) !=-1):\n",
    "                    map[i] = 1\n",
    "            mask.append(map)\n",
    "        subject_mask = torch.stack(mask)\n",
    "        subject_mask = torch.logical_and(subject_mask, input.attention_mask).int()\n",
    "        return subject_mask\n",
    "    \n",
    "    def get_ks_hook(self, last_subject_indices = None):\n",
    "        if last_subject_indices == None:\n",
    "            last_subject_indices = self._last_subject_indices\n",
    "        \n",
    "        def hook(module,input,output):\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                res = output[torch.arange(len(last_subject_indices)), last_subject_indices]\n",
    "                self._ks = res\n",
    "            else:\n",
    "                raise TypeError(\"Expected output to be a torch.Tensor, but got {}\".format(type(output)))\n",
    "            pass\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    def accroche(self, l_star = None):\n",
    "        if l_star == None:\n",
    "            l_star = self._l_star\n",
    "        hook = self.get_ks_hook()\n",
    "        handle = self.model.transformer.h[l_star].mlp.c_fc.register_forward_hook(hook)\n",
    "        self._hooks.append(handle)\n",
    "        pass\n",
    "    \n",
    "    def enleve(self):\n",
    "        for handle in self._hooks:\n",
    "            handle.remove()\n",
    "        self._hooks = []\n",
    "        pass\n",
    "    \n",
    "    def run(self, conserve_logits = False,conserve_output = False):\n",
    "        input = self.tokenize(self.prompts)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**input, labels = input.input_ids) \n",
    "        if self._ks != None:\n",
    "            self._k_star = torch.mean(self._ks, dim=0)\n",
    "        if conserve_logits:\n",
    "            self._logits = output.logits \n",
    "        if conserve_output:\n",
    "            self._output = output\n",
    "        pass\n",
    "\n",
    "    def generate_prompts(self, nb_prompt, min_len = 2, max_len = 11):\n",
    "        vocab_size = self.tokenizer.vocab_size\n",
    "        nb_token = torch.randint(min_len, max_len, (nb_prompt,))\n",
    "        max_tokens = nb_token.max() \n",
    "        tokens = torch.randint(0, vocab_size, (nb_prompt, max_tokens))\n",
    "        padded_tokens = F.pad(tokens, (0, max_tokens - nb_token.max().item()), value=vocab_size)\n",
    "        decoded_sequences = [self.tokenizer.decode(seq[:nb_token[i].item()]) for i, seq in enumerate(padded_tokens)]\n",
    "        res = [x + ' ' + self.subject for x in decoded_sequences]\n",
    "        self.__init__(self.subject, res, self._l_star,self.model_name)\n",
    "        pass\n",
    "\n",
    "    def get_k_star(self,l_star = None):\n",
    "        self.accroche(l_star)\n",
    "        self.run()\n",
    "        self.enleve()\n",
    "        return self._k_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' favorite UnlePresent Eiffel Tower', ' AustrianactivelysesLOD Brawl handguneman relativeANCE Eiffel Tower', ' vegetable Abraham Eiffel Tower', ' Root bouAdult820 Eiffel Tower', ' Nut aggressive 384 quickest Koran Setup heels Eiffel Tower', ' depressed dagger Saunders constitute summar Mercy substantially somisode Eiffel Tower', '260 pure anarchyanos confused pedestrian corners Empires crowned757 Eiffel Tower', ' SentConsuationgrain assumed Eiffel Tower', ' ================= 560 payingSourceFile 421 stressed Eiffel Tower', ' rulesama Samuel Eiffel Tower', ' Functionalč SE Mineral repository turnedsem GN bandwagonOHN Eiffel Tower', 'lt fielding Eiffel Tower', ' collideproc simmer Wat Eiffel Tower', ' blownpages Eiffel Tower', ' backs inadvertVT Sussex Theyaghanhouse463 Goodman Claire Eiffel Tower', ' sharingden ACEWA ambiguity newest Emergency ******************************** Eiffel Tower', ' chicken infinity332++; BBQ hack flickeringNonetheless Eiffel Tower', ' costing ra originated sinful Quiet narrow Eiffel Tower', ' publication receipts mend Inventioneware Eiffel Tower', ' endsincludes strandsresist Carn Electronic qualification lif Alm Eiffel Tower', ' steeredok amplifier Romans formulated Jackson acknowledgelando existed Eiffel Tower', ' grey bases Camb Alzheimer FIRST Booker Eiffel Tower', 'Section © Iwhibition&& MLG Eiffel Tower', 'nomurgicalpend defenses heterosexualovember tragedy Eiffel Tower', ' pissed Xavier workings regretazes uncoverViCredit Marketplaceric Eiffel Tower', 'pa liv preferential ethic Eiffel Tower', ' 426aul appeal Infinitecipl scholarlypeople Eiffel Tower', 'your Confederate Artistksonunicip Play Africans Musurses Eiffel Tower', ' bloodets\">< Eiffel Tower', 'HAM Yao statutory 625 discharged Eiffel Tower', 'TG perplexitially 2001 vascular Wang curveatl Perioddensity Eiffel Tower', 'Update gir 1989 Uponlyingsterdamoussed Eiffel Tower', ' Apocalypse PLoS collabor Lives nonexistent Eiffel Tower', 'extra =================================================================hire DL 188 covenantiegelBee unbeliev Eiffel Tower', ' religious unbelievable god digs rendered wording fatigue substitute Eiffel Tower', 'omal PeakOUP Eiffel Tower', 'isloptim Smith Eiffel Tower', ' Orion ideological Eiffel Tower', ' banging impractical Sym nort mindset 345fighterags Eiffel Tower', ' visceral performing brittle Eiffel Tower', ' 79 ratificationcade Veter Eiffel Tower', 'restling nodeik Bag Kenyan Eiffel Tower', ' goodbyeDD Sask Thereforefounded petertodd LU asshole gone Eiffel Tower', 'orce dazz related Deteriperbrain Eiffel Tower', ' LHGoal787 pistols Eiffel Tower', ' Fees overarchingERS� fedENSE Eiffel Tower', ' violates IslamistAX apprehend Tank═ Eiffel Tower', 'AAA deductedigs Eiffel Tower', 'Doctors mat Joh Kel XP756 Monica cooker antiv Eiffel Tower', ' bragging409 Eiffel Tower']\n"
     ]
    }
   ],
   "source": [
    "test = Instance_for_ROME('Eiffel Tower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.1392, -1.6501, -0.7724,  ..., -1.3727, -1.3840, -2.6023])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.generate_prompts(50)\n",
    "test.get_k_star()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [ \"Roses are red\",\n",
    "            \"violets are blue\",\n",
    "            \"once upon a time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test:\n",
    "    def __init__(self, prompts, model_name = 'gpt2'):\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.nb_prompt = len(prompts)\n",
    "        self.inputs = self.tokenizer(prompts, return_tensors='pt',padding=True)\n",
    "        self.nb_token = len(self.inputs.input_ids[0])\n",
    "        self.observe = None\n",
    "        self.hooks = []\n",
    "    def run(self):\n",
    "        self.model( **self.inputs, labels = self.inputs.input_ids)\n",
    "        pass\n",
    "    def accroche(self,i):\n",
    "        handle = self.model.transformer.h[i].mlp.c_fc.register_forward_hook(self.def_hook())\n",
    "        self.hooks.append(handle)\n",
    "        pass\n",
    "    def def_hook(self):\n",
    "        def hook(module,input,output):\n",
    "            self.observe = output\n",
    "            pass\n",
    "        return hook\n",
    "    def decroche(self):\n",
    "        while len(self.hooks)!=0:\n",
    "            handle = self.hooks.pop()\n",
    "            handle.remove\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test(prompts,'gpt2-xl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=4800, nx=1600)\n",
       "          (c_proj): Conv1D(nf=1600, nx=1600)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=6400, nx=1600)\n",
       "          (c_proj): Conv1D(nf=1600, nx=6400)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.accroche(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1719, -1.7652, -1.2129,  ..., -1.9756, -0.2020,  0.0178],\n",
       "         [-0.4384, -2.8245, -1.5303,  ..., -1.0836, -0.9600,  0.0858],\n",
       "         [-0.1192, -1.4361, -1.8975,  ..., -1.6589, -0.1612, -0.0735],\n",
       "         [ 0.0581, -3.9133, -1.7499,  ..., -2.6801, -1.0978, -0.1239],\n",
       "         [-0.0636, -1.3829, -1.7825,  ..., -1.7437, -1.0203, -0.0888]],\n",
       "\n",
       "        [[-0.1709, -1.7098, -1.1889,  ..., -2.1661,  0.3088, -0.3459],\n",
       "         [-0.0392, -3.3459, -0.1746,  ..., -2.6891, -0.7095,  0.0816],\n",
       "         [-0.3265, -2.9280, -2.9074,  ..., -1.6303, -0.6460, -0.1716],\n",
       "         [-0.0211, -1.2382, -1.3937,  ..., -1.7412, -0.1739, -0.1458],\n",
       "         [-0.2472, -2.5032,  0.3609,  ..., -1.5566, -0.1479, -0.6963]],\n",
       "\n",
       "        [[-0.2837, -3.2428, -2.1223,  ..., -0.6043,  0.3078,  0.4584],\n",
       "         [-0.1778, -2.6204, -0.7883,  ..., -1.9352, -0.7247, -0.2274],\n",
       "         [ 0.1209, -1.5343, -2.1487,  ..., -2.4988,  0.0497, -0.1618],\n",
       "         [-0.3154, -4.7122, -2.3165,  ..., -3.5406, -0.3736, -0.1380],\n",
       "         [-0.1956, -1.9274, -1.7541,  ..., -1.7213, -0.8425,  0.0356]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.observe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute v*\n",
    "\n",
    "Je recopie ici le code de la classe histoire de faire mes propres tests et de pas toucher au code fait avant moi\n",
    "\n",
    "Le but c'est de compute v* qui est une simple optimisation d'une fonction de perte + de la divergence KL (pour que l'essence du modèle sur le sujet ne change pas de façon trop significative)\n",
    "\n",
    "On a notamment besoin de rajouter en argument o* -> la prédiction que l'on veut que le modèle fasse quand on lui donne notre sujet et la relation\n",
    "\n",
    "De même on a besoin de p, le prompt factuel qui donne clairement la relation entre s et o*\n",
    "Typiquement: 'The Space Needle is in Seattle\"\n",
    "On aura aussi besoin de p' qui permet de voir si notre modèle dévie pas trop du sens initial mais ça j'avoue pour l'instant c'est pas hyper clair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEditor :\n",
    "    def __init__(self, instance, o_star):\n",
    "        self.instance = instance\n",
    "        self.o_star = o_star\n",
    "        self._v_star = torch.nn.Parameter(torch.randn(instance._k_star.shape))\n",
    "\n",
    "        self.hook_handle = None\n",
    "\n",
    "    def mlp_output_hook(self, module, input, output):\n",
    "        # output est la sortie MLP originale, on la remplace par v*\n",
    "        return self._v_star.unsqueeze(0).expand_as(output)\n",
    "    \n",
    "    def accroche(self):\n",
    "        # Hook sur la sortie du MLP à la même couche L*\n",
    "        l_star = self.instance._l_star\n",
    "        handle = self.instance.model.transformer.h[l_star].mlp.c_proj.register_forward_hook(self.mlp_output_hook)\n",
    "        self._hook_handle = handle\n",
    "    \n",
    "    def enleve(self):\n",
    "        if self._hook_handle is not None:\n",
    "            self._hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_v_star(editor, prompt_templates, object_token_str, n_iter=100, lr=1e-2):\n",
    "    instance = editor.instance\n",
    "    tokenizer = instance.tokenizer\n",
    "    model = instance.model\n",
    "\n",
    "    editor.accroche()\n",
    "    \n",
    "    optimizer = torch.optim.Adam([editor._v_star], lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    input_prompts = [template.format(subject=instance.subject) for template in prompt_templates]\n",
    "    tokenized = tokenizer(input_prompts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Token ID cible à prédire\n",
    "    target_token_id = tokenizer.encode(object_token_str, add_special_tokens=False)[0]\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(**tokenized)\n",
    "        logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # Prédiction sur le dernier token\n",
    "        last_token_logits = logits[:, -1, :]  # shape: [batch, vocab_size]\n",
    "        \n",
    "        targets = torch.tensor([target_token_id] * last_token_logits.shape[0])\n",
    "        loss = loss_fn(last_token_logits, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"[{i}] loss = {loss.item():.4f}\")\n",
    "\n",
    "    editor.enleve()\n",
    "    return editor._v_star.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.4697, -1.0214, -0.2980,  ..., -0.0869, -0.6566,  0.3382])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = Instance_for_ROME(subject)\n",
    "instance.get_k_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1600) must match the existing size (6400) at non-singleton dimension 2.  Target sizes: [1, 26, 1600].  Tensor sizes: [1, 6400]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      3\u001b[39m factual_prompts = [\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[38;5;132;01m{subject}\u001b[39;00m\u001b[33m is the capital of\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mIn which country is \u001b[39m\u001b[38;5;132;01m{subject}\u001b[39;00m\u001b[33m located?\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m ]\n\u001b[32m     11\u001b[39m editor = ValueEditor(instance, o_star)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m v_star = \u001b[43moptimize_v_star\u001b[49m\u001b[43m(\u001b[49m\u001b[43meditor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactual_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo_star\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(v_star)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36moptimize_v_star\u001b[39m\u001b[34m(editor, prompt_templates, object_token_str, n_iter, lr)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[32m     18\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     logits = outputs.logits  \u001b[38;5;66;03m# [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Prédiction sur le dernier token\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1060\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1079\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    910\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    911\u001b[39m         block.\u001b[34m__call__\u001b[39m,\n\u001b[32m    912\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    919\u001b[39m         output_attentions,\n\u001b[32m    920\u001b[39m     )\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:441\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[39m\n\u001b[32m    439\u001b[39m residual = hidden_states\n\u001b[32m    440\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.ln_2(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m feed_forward_hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[32m    443\u001b[39m hidden_states = residual + feed_forward_hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:370\u001b[39m, in \u001b[36mGPT2MLP.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    368\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.c_fc(hidden_states)\n\u001b[32m    369\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.act(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1845\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1845\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1847\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1849\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1850\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1806\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1804\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[32m   1805\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1806\u001b[39m     hook_result = \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1809\u001b[39m     result = hook_result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mValueEditor.mlp_output_hook\u001b[39m\u001b[34m(self, module, input, output)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmlp_output_hook\u001b[39m(\u001b[38;5;28mself\u001b[39m, module, \u001b[38;5;28minput\u001b[39m, output):\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# output est la sortie MLP originale, on la remplace par v*\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_v_star\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The expanded size of the tensor (1600) must match the existing size (6400) at non-singleton dimension 2.  Target sizes: [1, 26, 1600].  Tensor sizes: [1, 6400]"
     ]
    }
   ],
   "source": [
    "subject = 'Paris'\n",
    "o_star = 'Italy'\n",
    "factual_prompts = [\n",
    "    '{subject} is the capital of'\n",
    "    'In which country is {subject} located?'\n",
    "    'Which country has {subject} as its capital'\n",
    "    'What country is home to {subject}?'\n",
    "\n",
    "]\n",
    "\n",
    "editor = ValueEditor(instance, o_star)\n",
    "\n",
    "v_star = optimize_v_star(editor, factual_prompts, o_star)\n",
    "\n",
    "print(v_star)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
