{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# orienté objet (copie avec modif de raphael)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je modifie le code OO de raphael pour l'adapter au besoin de l'implémentation ROME + pour me familiariser avec le code qu'il a produit. Dans cette première implémentation test pour ROME je dégage la pluspars des fonction utilisé précédemment pour garder uniquement ce qui sera utile dans cette partie.\n",
    "\n",
    "J'ai enlever le truc de batch et la mise sur GPU parce que j'y comprend R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = les prompts sur lesquels on va calculer k* et v*\n",
    "#subject le sujet pour qui il faut determiner k*,v*\n",
    "class Instance_for_ROME :\n",
    "    def __init__(self, subject, inputs= None, l_star = 18, model_name = 'gpt2-xl', nb_prompt=50):\n",
    "        self.model_name = model_name\n",
    "        self.subject = subject\n",
    "        self._l_star = l_star\n",
    "\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if inputs == None:\n",
    "            self.generate_prompts(50)\n",
    "        else:\n",
    "            self.prompts = inputs\n",
    "\n",
    "        self._subject_mask = self.compute_subject_mask()\n",
    "        self._last_subject_indices= (self._subject_mask * torch.arange(1, self._subject_mask.shape[1] + 1, device=self._subject_mask.device)).argmax(dim=1)\n",
    "\n",
    "        self._ks = None\n",
    "        self._k_star=None\n",
    "        self._hooks = []\n",
    "        self._logits = None\n",
    "        self.output = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Instance of {self.model.config.architectures[0]} model'\n",
    "    \n",
    "    def tokenize(self,batch,offsetsMapping=False):\n",
    "        inputs=self.tokenizer(batch,return_tensors='pt',padding=True,return_offsets_mapping=offsetsMapping)\n",
    "        return inputs\n",
    "    \n",
    "    def compute_subject_mask(self, prompts = None, subject = None):\n",
    "        res =[]\n",
    "\n",
    "        if prompts == None:\n",
    "            prompts = self.prompts\n",
    "        if subject == None:\n",
    "            subject = self.subject\n",
    "\n",
    "        input = self.tokenize(prompts,offsetsMapping=True)\n",
    "        mask=[]\n",
    "        for j, prompt in enumerate(prompts):\n",
    "            map = torch.zeros_like(input.input_ids[j], dtype=torch.int)\n",
    "            for i,t in enumerate(input.offset_mapping[j]):\n",
    "                if (prompts[j].find(subject)-1<=t[0]) and (t[1]<=prompts[j].find(subject)+len(subject)) and (prompts[j].find(subject) !=-1):\n",
    "                    map[i] = 1\n",
    "            mask.append(map)\n",
    "        subject_mask = torch.stack(mask)\n",
    "        subject_mask = torch.logical_and(subject_mask, input.attention_mask).int()\n",
    "        return subject_mask\n",
    "     \n",
    "    def get_ks_hook(self, last_subject_indices=None):\n",
    "    # Use the last_subject_indices from the class if not provided\n",
    "        if last_subject_indices is None:\n",
    "            last_subject_indices = self._last_subject_indices\n",
    "\n",
    "    # Define the hook function\n",
    "        def hook(module, input, output):\n",
    "            # Ensure input is a 2D tensor with shape (batch_size, num_features)\n",
    "            res = input\n",
    "            self._ks = res\n",
    "            pass\n",
    "        \n",
    "        return hook\n",
    "\n",
    "    def accroche(self, l_star = None):\n",
    "        if l_star == None:\n",
    "            l_star = self._l_star\n",
    "        hook = self.get_ks_hook()\n",
    "        handle = self.model.transformer.h[l_star].mlp.act.register_forward_hook(self.get_ks_hook())\n",
    "        self._hooks.append(handle)\n",
    "        pass\n",
    "    \n",
    "    def enleve(self):\n",
    "        for handle in self._hooks:\n",
    "            handle.remove()\n",
    "        self._hooks = []\n",
    "        pass\n",
    "    \n",
    "    def run(self, conserve_logits = False,conserve_output = False):\n",
    "        input = self.tokenize(self.prompts)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**input, labels = input.input_ids) \n",
    "        if self._ks != None:\n",
    "            self._k_star = torch.mean(self._ks, dim=0)\n",
    "        if conserve_logits:\n",
    "            self._logits = output.logits \n",
    "        if conserve_output:\n",
    "            self._output = output\n",
    "        pass\n",
    "\n",
    "    def generate_prompts(self, nb_prompt, min_len = 2, max_len = 11):\n",
    "        vocab_size = self.tokenizer.vocab_size\n",
    "        nb_token = torch.randint(min_len, max_len, (nb_prompt,))\n",
    "        max_tokens = nb_token.max() \n",
    "        tokens = torch.randint(0, vocab_size, (nb_prompt, max_tokens))\n",
    "        padded_tokens = F.pad(tokens, (0, max_tokens - nb_token.max().item()), value=vocab_size)\n",
    "        decoded_sequences = [self.tokenizer.decode(seq[:nb_token[i].item()]) for i, seq in enumerate(padded_tokens)]\n",
    "        res = [x + ' ' + self.subject for x in decoded_sequences]\n",
    "        self.__init__(self.subject, res, self._l_star,self.model_name)\n",
    "        pass\n",
    "\n",
    "    def get_k_star(self,l_star = None):\n",
    "        self.accroche(l_star)\n",
    "        self.run()\n",
    "        self.enleve()\n",
    "        return self._k_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Instance_for_ROME('Eiffel Tower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.generate_prompts(50)\n",
    "test.get_k_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test._ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute v*\n",
    "\n",
    "Je crée ici une nouvelle classe histoire de faire mes propres tests et de pas toucher au code fait avant moi\n",
    "\n",
    "Le but c'est de compute v* qui est une simple optimisation d'une fonction de perte + de la divergence KL (pour que l'essence du modèle sur le sujet ne change pas de façon trop significative)\n",
    "\n",
    "On a notamment besoin de rajouter en argument o* -> la prédiction que l'on veut que le modèle fasse quand on lui donne notre sujet et la relation\n",
    "\n",
    "De même on a besoin de p, le prompt factuel qui donne clairement la relation entre s et o*\n",
    "Typiquement: 'The Space Needle is in Seattle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEditor :\n",
    "    def __init__(self, instance, o_star):\n",
    "        self.instance = instance\n",
    "        self.o_star = o_star\n",
    "        self._v_star = torch.nn.Parameter(torch.randn([1,1600]))\n",
    "\n",
    "        self.hook_handle = None\n",
    "\n",
    "    def mlp_output_hook(self, module, input, output): #Simple hook pour insérer v* à la bonne couche.\n",
    "        return self._v_star.unsqueeze(0).expand_as(output)\n",
    "    \n",
    "    def accroche(self):\n",
    "        l_star = self.instance._l_star\n",
    "        handle = self.instance.model.transformer.h[l_star].mlp.c_proj.register_forward_hook(self.mlp_output_hook)\n",
    "        self._hook_handle = handle\n",
    "    \n",
    "    def enleve(self):\n",
    "        if self._hook_handle is not None:\n",
    "            self._hook_handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction qui suit cherche à optimiser v* par itérations successives sur des prompts qui lui donnent le contexte.\n",
    "\n",
    "[A FAIRE] Définir la loss correctement pour matcher celle qu'on a dans le papier, en prenant en compte les xj notamment ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_v_star(editor, factual_prompts, o_star, n_iter=300, lr=0.1, early_stop_threshold=0.01, lambda_kl=30):\n",
    "    \"\"\"\n",
    "    Optimise v* pour forcer le modèle à prédire o* juste après le prompt,\n",
    "    tout en contrôlant l'essence du sujet avec la KL divergence.\n",
    "    \"\"\"\n",
    "    instance = editor.instance\n",
    "    tokenizer = instance.tokenizer\n",
    "    model = instance.model\n",
    "\n",
    "    editor.accroche()\n",
    "    optimizer = torch.optim.Adam([editor._v_star], lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    input_prompts = [template.format(subject=instance.subject) for template in factual_prompts]\n",
    "    tokenized = tokenizer(input_prompts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_ids = tokenized.input_ids  # (batch_size, seq_len)\n",
    "    attention_mask = tokenized.attention_mask\n",
    "\n",
    "    # Move bizarre de ChatGPT pour essayer de prendre en compte que notre token cible sera pas forcément celui prédit en premier.\n",
    "    # A voir si ça change vraiment qq chose, ou même si c'est pas un peu contreproductif...\n",
    "    # Quelle est la longueur du prompt ?\n",
    "    seq_len = input_ids.shape[1]\n",
    "\n",
    "    # Token ID cible (premier token de o_star)\n",
    "    target_token_id = tokenizer.encode(o_star, add_special_tokens=False)[0]\n",
    "\n",
    "    # Stocker les logits originaux pour le KL divergence\n",
    "    with torch.no_grad():\n",
    "        outputs_original = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits_original = outputs_original.logits\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits_modified = outputs.logits\n",
    "\n",
    "        # === 1. CrossEntropy Loss ===\n",
    "        # Chercher les logits pour la première position libre\n",
    "        target_logits = logits_modified[:, seq_len-1, :]\n",
    "\n",
    "        targets = torch.full((target_logits.size(0),), target_token_id, dtype=torch.long, device=target_logits.device)\n",
    "        ce_loss = loss_fn(target_logits, targets)\n",
    "\n",
    "        # === 2. KL Divergence Loss ===\n",
    "        logits_modified_flat = logits_modified.view(-1, logits_modified.size(-1))\n",
    "        logits_original_flat = logits_original.view(-1, logits_original.size(-1))\n",
    "\n",
    "        probs_modified = torch.nn.functional.softmax(logits_modified_flat, dim=-1)\n",
    "        probs_original = torch.nn.functional.softmax(logits_original_flat, dim=-1)\n",
    "\n",
    "        kl_loss = torch.nn.functional.kl_div(probs_modified.log(), probs_original, reduction=\"batchmean\")\n",
    "\n",
    "        # === 3. Loss totale ===\n",
    "        loss = ce_loss + lambda_kl * kl_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0 or loss.item() < early_stop_threshold:\n",
    "            print(f\"[{i}] Total Loss = {loss.item():.6f} | CE = {ce_loss.item():.6f} | KL = {kl_loss.item():.6f}\")\n",
    "\n",
    "        # Early stopping si la loss totale est très faible\n",
    "        if loss.item() < early_stop_threshold:\n",
    "            print(f\"\\nEarly stopping at iteration {i} with loss {loss.item():.6f}\")\n",
    "            break\n",
    "\n",
    "    editor.enleve()\n",
    "    return editor._v_star.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir défini tout ça, on le test en essayant d'apprendre le fait: Paris is the capital of Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1776, -0.1835,  0.0786,  ..., -0.7899, -0.1964, -0.3447])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = 'Paris'\n",
    "instance = Instance_for_ROME(subject)\n",
    "instance.get_k_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Total Loss = 9.887506 | CE = 9.887506 | KL = -0.000000\n",
      "[10] Total Loss = 8.395196 | CE = 7.804498 | KL = 0.019690\n",
      "[20] Total Loss = 7.500482 | CE = 6.440556 | KL = 0.035331\n",
      "[30] Total Loss = 6.720286 | CE = 5.108779 | KL = 0.053717\n",
      "[40] Total Loss = 6.047701 | CE = 3.830797 | KL = 0.073897\n",
      "[50] Total Loss = 5.565105 | CE = 3.028305 | KL = 0.084560\n",
      "[60] Total Loss = 5.222683 | CE = 2.673275 | KL = 0.084980\n",
      "[70] Total Loss = 5.049441 | CE = 2.605066 | KL = 0.081479\n",
      "[80] Total Loss = 4.953030 | CE = 2.658812 | KL = 0.076474\n",
      "[90] Total Loss = 4.884614 | CE = 2.575024 | KL = 0.076986\n",
      "[100] Total Loss = 4.844116 | CE = 2.518240 | KL = 0.077529\n",
      "[110] Total Loss = 4.812892 | CE = 2.575865 | KL = 0.074568\n",
      "[120] Total Loss = 4.781309 | CE = 2.514036 | KL = 0.075576\n",
      "[130] Total Loss = 4.759818 | CE = 2.486676 | KL = 0.075771\n",
      "[140] Total Loss = 4.751710 | CE = 2.424177 | KL = 0.077584\n",
      "[150] Total Loss = 4.738081 | CE = 2.542277 | KL = 0.073193\n",
      "[160] Total Loss = 4.718220 | CE = 2.501337 | KL = 0.073896\n",
      "[170] Total Loss = 4.716710 | CE = 2.386987 | KL = 0.077657\n",
      "[180] Total Loss = 4.700091 | CE = 2.406425 | KL = 0.076456\n",
      "[190] Total Loss = 4.698211 | CE = 2.511364 | KL = 0.072895\n",
      "[200] Total Loss = 4.687056 | CE = 2.490070 | KL = 0.073233\n",
      "[210] Total Loss = 4.672782 | CE = 2.410093 | KL = 0.075423\n",
      "[220] Total Loss = 4.687841 | CE = 2.370791 | KL = 0.077235\n",
      "[230] Total Loss = 4.670605 | CE = 2.444082 | KL = 0.074217\n",
      "[240] Total Loss = 4.663866 | CE = 2.384101 | KL = 0.075992\n",
      "[250] Total Loss = 4.651463 | CE = 2.396485 | KL = 0.075166\n",
      "[260] Total Loss = 4.670160 | CE = 2.507172 | KL = 0.072100\n",
      "[270] Total Loss = 4.650672 | CE = 2.364596 | KL = 0.076203\n",
      "[280] Total Loss = 4.641552 | CE = 2.383980 | KL = 0.075252\n",
      "[290] Total Loss = 4.641331 | CE = 2.349817 | KL = 0.076384\n",
      "tensor([[-3.5089, -0.4344,  1.6164,  ..., -4.1744, -4.0996, -1.5488]])\n"
     ]
    }
   ],
   "source": [
    "subject = 'Paris'\n",
    "o_star = 'Italy'\n",
    "factual_prompts = [\n",
    "    '{subject} is the capital of',\n",
    "    'In which country is {subject} located?',\n",
    "    'Where is {subject}?',\n",
    "    \"The country that governs {subject} is\",\n",
    "    \"The location of {subject} is in\",\n",
    "    \"{subject} is situated in the country of\",\n",
    "    \"Which nation does {subject} belong to?\",\n",
    "    \"In which country is {subject} found?\",\n",
    "    \"{subject} is part of the country called\",\n",
    "    \"The city of {subject} is a part of\",\n",
    "    \"The famous city {subject} is located in\",\n",
    "    \"You can find {subject} in the country of\",\n",
    "    \"The administrative country for {subject} is\"\n",
    "]\n",
    "\n",
    "editor = ValueEditor(instance, o_star)\n",
    "\n",
    "v_star = optimize_v_star(editor, factual_prompts, o_star)\n",
    "\n",
    "print(v_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertion (k,v) -> Update de W_proj\n",
    "\n",
    "Pour l'instant on élude complètement la question de la covariance empirique des clés k sur le corpus de wikipédia en remplacant la matrice de covariance (C) per l'identité.\n",
    "On regarde si ça fonctionne déjà comme ça et puis on se penchera dessus après"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rank_one_update(instance, v_star, C_inv=None):\n",
    "    l_star = instance._l_star\n",
    "    k_star = instance._k_star\n",
    "    W_proj = instance.model.transformer.h[l_star].mlp.c_proj.weight  \n",
    "\n",
    "    if C_inv is None:\n",
    "        C_inv = torch.eye(k_star.size(0), device=k_star.device)  # Pour l'instant on pose C = Id\n",
    "\n",
    "    # 1. Calculer Lambda\n",
    "    numerator = v_star - W_proj.t() @ k_star\n",
    "    denominator = (C_inv @ k_star).dot(k_star)\n",
    "    Lambda = numerator / denominator\n",
    "\n",
    "    # 2. Calculer delta_W\n",
    "    delta_W = Lambda.view(-1, 1) @ (C_inv @ k_star).view(1, -1)\n",
    "\n",
    "    # 3. Appliquer la mise à jour\n",
    "    with torch.no_grad():\n",
    "        W_proj.data += delta_W.t()\n",
    "\n",
    "    print(\"Mise à jour appliquée avec succès sur W_proj.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mise à jour appliquée avec succès sur W_proj.\n"
     ]
    }
   ],
   "source": [
    "apply_rank_one_update(instance, v_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: \"Paris is the capital of\"\n",
      "Top 1: France (0.6203)\n",
      "Top 2: Italy (0.1514)\n",
      "Top 3: the (0.0481)\n",
      "Top 4: Europe (0.0206)\n",
      "Top 5: a (0.0063)\n",
      "\n",
      "Prompt: \"The administrative country for Paris is\"\n",
      "Top 1: the (0.1429)\n",
      "Top 2: a (0.0290)\n",
      "Top 3: Italy (0.0274)\n",
      "Top 4: not (0.0262)\n",
      "Top 5: now (0.0196)\n"
     ]
    }
   ],
   "source": [
    "def test_new_fact(instance, subject, prompt_template, top_k=5):\n",
    "    tokenizer = instance.tokenizer\n",
    "    model = instance.model\n",
    "    model.eval()\n",
    "\n",
    "    prompt = prompt_template.format(subject=subject)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        top_probs, top_indices = probs.topk(top_k)\n",
    "        top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
    "\n",
    "    print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n",
    "    for rank, (token, prob) in enumerate(zip(top_tokens, top_probs), 1):\n",
    "        print(f\"Top {rank}: {token.strip()} ({prob.item():.4f})\")\n",
    "\n",
    "# Tester sur quelques prompts :\n",
    "test_new_fact(instance, subject, \"{subject} is the capital of\")\n",
    "test_new_fact(instance, subject, \"The administrative country for {subject} is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pourquoi ça ne fonctionne pas\n",
    "\n",
    "-> On a un arbitrage fondamental dans le processus d'optimisation de v* entre l'apprentissage de nouvelles connaissances et la fidélité au modèle original (Dkl)\n",
    "Cela crée notamment une oscillation qui vient faire stagner l'optimisation de v* à un certain endroit.\n",
    "\n",
    "Deux hypothèses:\n",
    "- Soit l'association Paris -> France est beaucoup trop ancré dans l'apprentissage du modèle, et en fait ROME ne marchera tout simplement pas dessus ne modifie pas assez en profondeur\n",
    "- Soit le problème vient d'autre part et on peut encore améliorer l'optimisation.\n",
    "\n",
    "### On en retire deux pistes\n",
    "\n",
    "1. On va essayer de tester les mêmes fonctions sur un fait beaucoup moins établi (pour voir si c'est vraiment ça le pb)\n",
    "\n",
    "2. (Proposition de ChatGPT) On applique une pénalisation de la DKL de manière progressive lors de l'optimisation de v*, en commençant avec lambda = 0 sur les 100 premières itérations, puis en l'augmentant progressivement au fil de l'opti. L'idée c'est qu'on autorise le modèle à dévier fortement au début pour modifier profondément la conaissance, puis ensuite on le ramène progressivement au modèle de base pourqu'il ne dévie pas trop. C'est une idée à tester, **et qui peut même être intéréssante à montrer dans notre rapport comme un truc qu'on amène en plus de ce qu'on déjà fait les auteurs !!**\n",
    "\n",
    "#### 1. ROME sur un fait moins ancré (Mont Everest -> Nepal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject2 = \"Mount Everest\"\n",
    "o_star2 = \"Tibet\"\n",
    "\n",
    "factual_prompts_2 = [\n",
    "    \"{subject2} is located in\",\n",
    "    \"The location of {subject2} is\",\n",
    "    \"Which country is {subject2} situated in?\",\n",
    "    \"{subject2} is found in\",\n",
    "    \"Where can you find {subject2}?\",\n",
    "    \"{subject2} lies in the country of\",\n",
    "    \"The famous peak {subject2} is part of\",\n",
    "    \"{subject2} belongs to\",\n",
    "    \"In which country is {subject2}?\",\n",
    "    \"The region hosting {subject2} is\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1228,  0.2350, -0.3844,  ..., -0.6715, -0.5512,  0.9980])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance2 = Instance_for_ROME(\"Mount Everest\")\n",
    "instance2.get_k_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Total Loss = 6.779407 | CE = 6.779407 | KL = -0.000000\n",
      "[10] Total Loss = 5.231492 | CE = 4.633796 | KL = 0.019923\n",
      "[20] Total Loss = 4.454584 | CE = 3.407744 | KL = 0.034895\n",
      "[30] Total Loss = 4.009721 | CE = 2.753810 | KL = 0.041864\n",
      "[40] Total Loss = 3.831625 | CE = 2.596739 | KL = 0.041163\n",
      "[50] Total Loss = 3.721290 | CE = 2.522736 | KL = 0.039952\n",
      "[60] Total Loss = 3.668720 | CE = 2.465712 | KL = 0.040100\n",
      "[70] Total Loss = 3.654763 | CE = 2.404272 | KL = 0.041683\n",
      "[80] Total Loss = 3.616385 | CE = 2.414395 | KL = 0.040066\n",
      "[90] Total Loss = 3.596866 | CE = 2.423718 | KL = 0.039105\n",
      "[100] Total Loss = 3.584001 | CE = 2.406402 | KL = 0.039253\n",
      "[110] Total Loss = 3.610426 | CE = 2.529819 | KL = 0.036020\n",
      "[120] Total Loss = 3.581926 | CE = 2.395793 | KL = 0.039538\n",
      "[130] Total Loss = 3.567891 | CE = 2.435115 | KL = 0.037759\n",
      "[140] Total Loss = 3.557625 | CE = 2.399783 | KL = 0.038595\n",
      "[150] Total Loss = 3.563760 | CE = 2.349854 | KL = 0.040464\n",
      "[160] Total Loss = 3.548687 | CE = 2.402713 | KL = 0.038199\n",
      "[170] Total Loss = 3.547964 | CE = 2.422522 | KL = 0.037515\n",
      "[180] Total Loss = 3.544499 | CE = 2.371168 | KL = 0.039111\n",
      "[190] Total Loss = 3.547628 | CE = 2.388122 | KL = 0.038650\n",
      "[200] Total Loss = 3.555834 | CE = 2.344294 | KL = 0.040385\n",
      "[210] Total Loss = 3.545805 | CE = 2.351679 | KL = 0.039804\n",
      "[220] Total Loss = 3.536790 | CE = 2.373413 | KL = 0.038779\n",
      "[230] Total Loss = 3.533280 | CE = 2.385707 | KL = 0.038252\n",
      "[240] Total Loss = 3.530733 | CE = 2.382430 | KL = 0.038277\n",
      "[250] Total Loss = 3.603833 | CE = 2.534680 | KL = 0.035638\n",
      "[260] Total Loss = 3.554670 | CE = 2.430705 | KL = 0.037466\n",
      "[270] Total Loss = 3.542311 | CE = 2.424396 | KL = 0.037264\n",
      "[280] Total Loss = 3.534354 | CE = 2.360358 | KL = 0.039133\n",
      "[290] Total Loss = 3.528110 | CE = 2.358314 | KL = 0.038993\n",
      "tensor([[-0.0953,  0.4081,  1.6404,  ..., -2.7436,  0.0386,  1.7774]])\n"
     ]
    }
   ],
   "source": [
    "editor2 = ValueEditor(instance2, o_star2)\n",
    "\n",
    "v_star2 = optimize_v_star(editor2, factual_prompts_2, o_star2)\n",
    "\n",
    "print(v_star2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mise à jour appliquée avec succès sur W_proj.\n",
      "\n",
      "Prompt: \"Mount Everest is located in\"\n",
      "Top 1: the (0.5760)\n",
      "Top 2: Nepal (0.2083)\n",
      "Top 3: a (0.0310)\n",
      "Top 4: Tibet (0.0258)\n",
      "Top 5: an (0.0129)\n",
      "\n",
      "Prompt: \"The region hosting Mount Everest is\"\n",
      "Top 1: home (0.0807)\n",
      "Top 2: the (0.0757)\n",
      "Top 3: a (0.0652)\n",
      "Top 4: also (0.0626)\n",
      "Top 5: one (0.0422)\n"
     ]
    }
   ],
   "source": [
    "apply_rank_one_update(instance2, v_star2)\n",
    "\n",
    "subject = subject2 #Comportement bizarre de la fonction de test qui utliise une variable globale -> a fix pour plus tard\n",
    "\n",
    "test_new_fact(instance2, subject, \"{subject} is located in\")\n",
    "test_new_fact(instance2, subject, \"The region hosting {subject} is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon là visiblement en fait on a encore un autre problème, c'est que l'association est tellement faible sur ces prompts que ça prédit des phrases plus diverses encore. Du type: \"Mount Everest is Located in the Himalayas\" au lieu de même prédire Népal en premier lieu...\n",
    "Donc nos prompts sont de fait pas pertinents de base et l'optimisation ne marche pas forcément mieux, il faudrait alors soit prendre un autre exemple, soit prendre en compte plus de contexte\n",
    "Mais là je vais aller me coucher mdr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calcule de C = Esperance(k*kT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explication du code :\n",
    "\n",
    "- Calcul_C est une classe qu'on initialise avec plusieurs prompts au format prompts =[ prompt1,prompt2, ..., promptn]\n",
    "- get_ks_hook crée un hook qui récupére pour chaque token de chaque prompt qui n'est pas un token de padding la valeur $k$ \n",
    "- pour accrocher le hook crée par get_ks_hook on utilise accroche(l_star) ou l_star est le block transformer pour lequel on veut récupérer les $k$ s\n",
    "- deccroche permet de décroccher le hook\n",
    "- run récupère les $k$ s, calcule pour chaque $k$ la valeur $k*k^T$ puis fait la moyenne de cette valeurs run renvoie donc la valeurs $mean(k*k^T) \\approx C = E[k*k^T]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn.functional as Fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calcul_C:\n",
    "    def __init__(self, prompts=None, l_star=18, model_name='gpt2-xl',device = 'cpu'):\n",
    "        self.model_name = model_name\n",
    "        self._l_star = l_star\n",
    "\n",
    "        # Check for GPU availability\n",
    "        if device =='cuda':\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        # Load the model and tokenizer\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.prompts = prompts\n",
    "\n",
    "        self.ks = None\n",
    "        self.hook_handles = []\n",
    "        self.C = None\n",
    "\n",
    "        if prompts is not None:\n",
    "            encodings = self.tokenizer(\n",
    "                prompts,\n",
    "                return_tensors='pt',\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            )\n",
    "\n",
    "            self.input_ids = encodings['input_ids'].to(self.device)  # Move to the correct device\n",
    "            self.attention_mask = encodings['attention_mask'].to(self.device)  # Move to the correct device\n",
    "        else:\n",
    "            self.input_ids = None\n",
    "            self.attention_mask = None\n",
    "\n",
    "        if self.attention_mask is not None:\n",
    "            self.nb_ks = self.attention_mask.sum().item()\n",
    "\n",
    "    def get_ks_hook(self):\n",
    "        mask = self.attention_mask.bool()\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            self.ks = output[mask]\n",
    "            return None  # Return None to stop the forward pass\n",
    "\n",
    "        return hook\n",
    "\n",
    "    def accroche(self, l_star=None):\n",
    "        if l_star is None:\n",
    "            l_star = self._l_star\n",
    "        handle = self.model.transformer.h[l_star].mlp.act.register_forward_hook(self.get_ks_hook())\n",
    "        self.hook_handles.append(handle)\n",
    "\n",
    "    def deccroche(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "    def run(self, l_star=None):\n",
    "        self.accroche(l_star)\n",
    "        with torch.no_grad():\n",
    "            # Forward pass on the model (no gradients needed)\n",
    "            self.model(input_ids=self.input_ids, attention_mask=self.attention_mask)\n",
    "        self.deccroche()\n",
    "\n",
    "        # Compute the kkT_matrices and C\n",
    "        kkT_matrices = torch.bmm(self.ks.unsqueeze(2), self.ks.unsqueeze(1))  # Batch matrix multiplication\n",
    "        self.C = kkT_matrices.mean(dim=0)  # Compute the mean over the batch dimension\n",
    "        return self.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Calcul_C(prompts=['Je suis un romain','vite'],device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0039,  0.0053, -0.0002,  ...,  0.0032,  0.0045,  0.0040],\n",
       "        [ 0.0053,  0.0188,  0.0101,  ...,  0.0115,  0.0105,  0.0204],\n",
       "        [-0.0002,  0.0101,  0.0173,  ...,  0.0094,  0.0019,  0.0149],\n",
       "        ...,\n",
       "        [ 0.0032,  0.0115,  0.0094,  ...,  0.0103,  0.0055,  0.0157],\n",
       "        [ 0.0045,  0.0105,  0.0019,  ...,  0.0055,  0.0109,  0.0115],\n",
       "        [ 0.0040,  0.0204,  0.0149,  ...,  0.0157,  0.0115,  0.0290]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En paramètre d'intéret:\n",
    " - nb_ks le nombre de ks que l'on pourra extraire des prompts  \n",
    " étant donnée que l'on veut calculer C sur $\\approx 100 \\,000$ $ k $ s on peut fonctionner par 'batch' : calculer C sur un premier ensemble de prompt ce qui nous donne $nb\\_ks_1$, $C_1 = \\frac{1}{nb\\_ks_1} \\sum k$, puis faire de même sur un autre ensemble de prompt; enfin on as plus qu'à calculer $C = \\frac{1}{N}\\sum_i nb\\_ks_i * C_i$ où $N = \\sum_i nb\\_ks_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe les données de wikipédia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_name = 'wikitext'\n",
    "\n",
    "raw_ds = load_dataset(ds_name, dict(wikitext=\"wikitext-103-raw-v1\", wikipedia=\"20200501.en\")[ds_name])\n",
    "raw_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sélectionne aléatoirement n ligne de texte et les nettois XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX pas encore bien nettoyer\n",
    "n = 10000\n",
    "text_data = raw_ds['train'].shuffle()[:n]['text']\n",
    "cleaned_text_data = []\n",
    "\n",
    "for line in text_data:\n",
    "\n",
    "    line = line.replace('@-@', '-')\n",
    "    line = line.replace(' @,@ ', ',')\n",
    "    line = line.replace(' @.@ ', '.')\n",
    "    line = re.sub(r'\\s+', ' ', line).strip()\n",
    "    line = line.replace(\"\\\\'\", \"'\") # ne marche pas je veux remplacer les \\' par ' mais j'y arrive pas\n",
    "    \n",
    "    # 3. Avoid adding empty lines\n",
    "    if line:  # Only add non-empty lines\n",
    "        cleaned_text_data.append(line)\n",
    "cleaned_text_data = [ line for line in cleaned_text_data \n",
    "                        if not (line.startswith('=') and line.endswith('='))\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On as pris au hasard tant de ligne de texte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4727"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien qu'on ait les cpu implémenter, on ne peut pas faire tourner les cpu avec le model gpt2 xl car on as des problème de mémoire (même lorsqu'on ne mets qu'une ligne) donc on calcule C par batch comme dans la méthode décrite précédemment via le CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11839"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Calcul_C(cleaned_text_data[:100])\n",
    "test.nb_ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text_data = cleaned_text_data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "étape 1 ; nombre de k dans l'étape : 383;  N = 383; Execution time: 0 minute(s) and 16 second(s)\n",
      "étape 2 ; nombre de k dans l'étape : 569;  N = 952; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 3 ; nombre de k dans l'étape : 518;  N = 1470; Execution time: 0 minute(s) and 13 second(s)\n",
      "étape 4 ; nombre de k dans l'étape : 1239;  N = 2709; Execution time: 0 minute(s) and 48 second(s)\n",
      "étape 5 ; nombre de k dans l'étape : 584;  N = 3293; Execution time: 0 minute(s) and 45 second(s)\n",
      "étape 6 ; nombre de k dans l'étape : 471;  N = 3764; Execution time: 0 minute(s) and 22 second(s)\n",
      "étape 7 ; nombre de k dans l'étape : 613;  N = 4377; Execution time: 0 minute(s) and 26 second(s)\n",
      "étape 8 ; nombre de k dans l'étape : 523;  N = 4900; Execution time: 0 minute(s) and 22 second(s)\n",
      "étape 9 ; nombre de k dans l'étape : 726;  N = 5626; Execution time: 0 minute(s) and 26 second(s)\n",
      "étape 10 ; nombre de k dans l'étape : 678;  N = 6304; Execution time: 0 minute(s) and 25 second(s)\n",
      "étape 11 ; nombre de k dans l'étape : 480;  N = 6784; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 12 ; nombre de k dans l'étape : 292;  N = 7076; Execution time: 0 minute(s) and 14 second(s)\n",
      "étape 13 ; nombre de k dans l'étape : 399;  N = 7475; Execution time: 0 minute(s) and 15 second(s)\n",
      "étape 14 ; nombre de k dans l'étape : 672;  N = 8147; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 15 ; nombre de k dans l'étape : 672;  N = 8819; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 16 ; nombre de k dans l'étape : 623;  N = 9442; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 17 ; nombre de k dans l'étape : 598;  N = 10040; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 18 ; nombre de k dans l'étape : 703;  N = 10743; Execution time: 0 minute(s) and 20 second(s)\n",
      "étape 19 ; nombre de k dans l'étape : 400;  N = 11143; Execution time: 0 minute(s) and 14 second(s)\n",
      "étape 20 ; nombre de k dans l'étape : 696;  N = 11839; Execution time: 0 minute(s) and 19 second(s)\n",
      "étape 21 ; nombre de k dans l'étape : 975;  N = 12814; Execution time: 0 minute(s) and 32 second(s)\n",
      "étape 22 ; nombre de k dans l'étape : 431;  N = 13245; Execution time: 0 minute(s) and 16 second(s)\n",
      "étape 23 ; nombre de k dans l'étape : 618;  N = 13863; Execution time: 0 minute(s) and 19 second(s)\n",
      "étape 24 ; nombre de k dans l'étape : 534;  N = 14397; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 25 ; nombre de k dans l'étape : 517;  N = 14914; Execution time: 0 minute(s) and 16 second(s)\n",
      "étape 26 ; nombre de k dans l'étape : 502;  N = 15416; Execution time: 0 minute(s) and 14 second(s)\n",
      "étape 27 ; nombre de k dans l'étape : 696;  N = 16112; Execution time: 0 minute(s) and 20 second(s)\n",
      "étape 28 ; nombre de k dans l'étape : 1036;  N = 17148; Execution time: 0 minute(s) and 33 second(s)\n",
      "étape 29 ; nombre de k dans l'étape : 525;  N = 17673; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 30 ; nombre de k dans l'étape : 1165;  N = 18838; Execution time: 0 minute(s) and 39 second(s)\n",
      "étape 31 ; nombre de k dans l'étape : 428;  N = 19266; Execution time: 0 minute(s) and 15 second(s)\n",
      "étape 32 ; nombre de k dans l'étape : 475;  N = 19741; Execution time: 0 minute(s) and 14 second(s)\n",
      "étape 33 ; nombre de k dans l'étape : 385;  N = 20126; Execution time: 0 minute(s) and 14 second(s)\n",
      "étape 34 ; nombre de k dans l'étape : 881;  N = 21007; Execution time: 0 minute(s) and 25 second(s)\n",
      "étape 35 ; nombre de k dans l'étape : 767;  N = 21774; Execution time: 0 minute(s) and 22 second(s)\n",
      "étape 36 ; nombre de k dans l'étape : 640;  N = 22414; Execution time: 0 minute(s) and 21 second(s)\n",
      "étape 37 ; nombre de k dans l'étape : 980;  N = 23394; Execution time: 0 minute(s) and 34 second(s)\n",
      "étape 38 ; nombre de k dans l'étape : 900;  N = 24294; Execution time: 0 minute(s) and 28 second(s)\n",
      "étape 39 ; nombre de k dans l'étape : 462;  N = 24756; Execution time: 0 minute(s) and 13 second(s)\n",
      "étape 40 ; nombre de k dans l'étape : 516;  N = 25272; Execution time: 0 minute(s) and 16 second(s)\n",
      "étape 41 ; nombre de k dans l'étape : 866;  N = 26138; Execution time: 0 minute(s) and 27 second(s)\n",
      "étape 42 ; nombre de k dans l'étape : 589;  N = 26727; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 43 ; nombre de k dans l'étape : 660;  N = 27387; Execution time: 0 minute(s) and 21 second(s)\n",
      "étape 44 ; nombre de k dans l'étape : 468;  N = 27855; Execution time: 0 minute(s) and 14 second(s)\n",
      "étape 45 ; nombre de k dans l'étape : 767;  N = 28622; Execution time: 0 minute(s) and 24 second(s)\n",
      "étape 46 ; nombre de k dans l'étape : 627;  N = 29249; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 47 ; nombre de k dans l'étape : 1105;  N = 30354; Execution time: 0 minute(s) and 37 second(s)\n",
      "étape 48 ; nombre de k dans l'étape : 703;  N = 31057; Execution time: 0 minute(s) and 21 second(s)\n",
      "étape 49 ; nombre de k dans l'étape : 282;  N = 31339; Execution time: 0 minute(s) and 13 second(s)\n",
      "étape 50 ; nombre de k dans l'étape : 645;  N = 31984; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 51 ; nombre de k dans l'étape : 271;  N = 32255; Execution time: 0 minute(s) and 10 second(s)\n",
      "étape 52 ; nombre de k dans l'étape : 900;  N = 33155; Execution time: 0 minute(s) and 29 second(s)\n",
      "étape 53 ; nombre de k dans l'étape : 312;  N = 33467; Execution time: 0 minute(s) and 11 second(s)\n",
      "étape 54 ; nombre de k dans l'étape : 739;  N = 34206; Execution time: 0 minute(s) and 24 second(s)\n",
      "étape 55 ; nombre de k dans l'étape : 324;  N = 34530; Execution time: 0 minute(s) and 11 second(s)\n",
      "étape 56 ; nombre de k dans l'étape : 548;  N = 35078; Execution time: 0 minute(s) and 16 second(s)\n",
      "étape 57 ; nombre de k dans l'étape : 673;  N = 35751; Execution time: 0 minute(s) and 23 second(s)\n",
      "étape 58 ; nombre de k dans l'étape : 594;  N = 36345; Execution time: 0 minute(s) and 16 second(s)\n",
      "étape 59 ; nombre de k dans l'étape : 650;  N = 36995; Execution time: 0 minute(s) and 19 second(s)\n",
      "étape 60 ; nombre de k dans l'étape : 620;  N = 37615; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 61 ; nombre de k dans l'étape : 575;  N = 38190; Execution time: 0 minute(s) and 21 second(s)\n",
      "étape 62 ; nombre de k dans l'étape : 723;  N = 38913; Execution time: 0 minute(s) and 23 second(s)\n",
      "étape 63 ; nombre de k dans l'étape : 844;  N = 39757; Execution time: 0 minute(s) and 29 second(s)\n",
      "étape 64 ; nombre de k dans l'étape : 473;  N = 40230; Execution time: 0 minute(s) and 15 second(s)\n",
      "étape 65 ; nombre de k dans l'étape : 600;  N = 40830; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 66 ; nombre de k dans l'étape : 731;  N = 41561; Execution time: 0 minute(s) and 22 second(s)\n",
      "étape 67 ; nombre de k dans l'étape : 719;  N = 42280; Execution time: 0 minute(s) and 24 second(s)\n",
      "étape 68 ; nombre de k dans l'étape : 449;  N = 42729; Execution time: 0 minute(s) and 14 second(s)\n",
      "étape 69 ; nombre de k dans l'étape : 855;  N = 43584; Execution time: 0 minute(s) and 29 second(s)\n",
      "étape 70 ; nombre de k dans l'étape : 530;  N = 44114; Execution time: 0 minute(s) and 16 second(s)\n",
      "étape 71 ; nombre de k dans l'étape : 867;  N = 44981; Execution time: 0 minute(s) and 30 second(s)\n",
      "étape 72 ; nombre de k dans l'étape : 1083;  N = 46064; Execution time: 0 minute(s) and 38 second(s)\n",
      "étape 73 ; nombre de k dans l'étape : 802;  N = 46866; Execution time: 0 minute(s) and 28 second(s)\n",
      "étape 74 ; nombre de k dans l'étape : 930;  N = 47796; Execution time: 0 minute(s) and 30 second(s)\n",
      "étape 75 ; nombre de k dans l'étape : 931;  N = 48727; Execution time: 0 minute(s) and 31 second(s)\n",
      "étape 76 ; nombre de k dans l'étape : 610;  N = 49337; Execution time: 0 minute(s) and 23 second(s)\n",
      "étape 77 ; nombre de k dans l'étape : 833;  N = 50170; Execution time: 0 minute(s) and 28 second(s)\n"
     ]
    }
   ],
   "source": [
    "N = 0\n",
    "i=0\n",
    "nb_ks = []\n",
    "Cs = []\n",
    "model_name = 'gpt2-xl'\n",
    "while N <= 50000:\n",
    "    t1 = time.time()\n",
    "    instance = Calcul_C(prompts=cleaned_text_data[i*5:i*5+5], model_name= model_name)\n",
    "    nbk = instance.nb_ks\n",
    "    nb_ks.append(nbk)\n",
    "    instance.run()\n",
    "    Cs.append(instance.C)\n",
    "    N+=nbk\n",
    "    i+=1\n",
    "    t2 = time.time()\n",
    "    elapsed = t2 - t1\n",
    "    minutes = int(elapsed // 60)\n",
    "    seconds = int(elapsed % 60)\n",
    "    print(f\"étape {i} ; nombre de k dans l'étape : {nbk};  N = {N}; Execution time: {minutes} minute(s) and {seconds} second(s)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sum = sum(C * nbk for C, nbk in zip(Cs, nb_ks))\n",
    "C_gpt_xl = weighted_sum / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enregistre ce C qui utilise 50466 ks puis on va calculer un autre C sur envire 50000 ks pour enfin calculer un C qui utilise 100000 ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'model_name': model_name,\n",
    "    'number_of_ks': N}\n",
    "torch.save({'C': C_gpt_xl, 'Metadata': metadata }, 'C.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "étape 78 ; nombre de k dans l'étape : 757;  N = 757; Execution time: 0 minute(s) and 21 second(s)\n",
      "étape 79 ; nombre de k dans l'étape : 436;  N = 1193; Execution time: 0 minute(s) and 15 second(s)\n",
      "étape 80 ; nombre de k dans l'étape : 510;  N = 1703; Execution time: 0 minute(s) and 16 second(s)\n",
      "étape 81 ; nombre de k dans l'étape : 805;  N = 2508; Execution time: 0 minute(s) and 23 second(s)\n",
      "étape 82 ; nombre de k dans l'étape : 489;  N = 2997; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 83 ; nombre de k dans l'étape : 636;  N = 3633; Execution time: 0 minute(s) and 20 second(s)\n",
      "étape 84 ; nombre de k dans l'étape : 539;  N = 4172; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 85 ; nombre de k dans l'étape : 624;  N = 4796; Execution time: 0 minute(s) and 21 second(s)\n",
      "étape 86 ; nombre de k dans l'étape : 444;  N = 5240; Execution time: 0 minute(s) and 15 second(s)\n",
      "étape 87 ; nombre de k dans l'étape : 716;  N = 5956; Execution time: 0 minute(s) and 21 second(s)\n",
      "étape 88 ; nombre de k dans l'étape : 667;  N = 6623; Execution time: 0 minute(s) and 19 second(s)\n",
      "étape 89 ; nombre de k dans l'étape : 608;  N = 7231; Execution time: 0 minute(s) and 19 second(s)\n",
      "étape 90 ; nombre de k dans l'étape : 702;  N = 7933; Execution time: 0 minute(s) and 22 second(s)\n",
      "étape 91 ; nombre de k dans l'étape : 620;  N = 8553; Execution time: 0 minute(s) and 20 second(s)\n",
      "étape 92 ; nombre de k dans l'étape : 349;  N = 8902; Execution time: 0 minute(s) and 11 second(s)\n",
      "étape 93 ; nombre de k dans l'étape : 1046;  N = 9948; Execution time: 0 minute(s) and 35 second(s)\n",
      "étape 94 ; nombre de k dans l'étape : 652;  N = 10600; Execution time: 0 minute(s) and 19 second(s)\n",
      "étape 95 ; nombre de k dans l'étape : 859;  N = 11459; Execution time: 0 minute(s) and 29 second(s)\n",
      "étape 96 ; nombre de k dans l'étape : 630;  N = 12089; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 97 ; nombre de k dans l'étape : 818;  N = 12907; Execution time: 0 minute(s) and 25 second(s)\n",
      "étape 98 ; nombre de k dans l'étape : 487;  N = 13394; Execution time: 0 minute(s) and 13 second(s)\n",
      "étape 99 ; nombre de k dans l'étape : 438;  N = 13832; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 100 ; nombre de k dans l'étape : 740;  N = 14572; Execution time: 0 minute(s) and 27 second(s)\n",
      "étape 101 ; nombre de k dans l'étape : 625;  N = 15197; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 102 ; nombre de k dans l'étape : 777;  N = 15974; Execution time: 0 minute(s) and 23 second(s)\n",
      "étape 103 ; nombre de k dans l'étape : 834;  N = 16808; Execution time: 0 minute(s) and 25 second(s)\n",
      "étape 104 ; nombre de k dans l'étape : 576;  N = 17384; Execution time: 0 minute(s) and 15 second(s)\n",
      "étape 105 ; nombre de k dans l'étape : 302;  N = 17686; Execution time: 0 minute(s) and 10 second(s)\n",
      "étape 106 ; nombre de k dans l'étape : 384;  N = 18070; Execution time: 0 minute(s) and 14 second(s)\n",
      "étape 107 ; nombre de k dans l'étape : 577;  N = 18647; Execution time: 0 minute(s) and 14 second(s)\n",
      "étape 108 ; nombre de k dans l'étape : 706;  N = 19353; Execution time: 0 minute(s) and 21 second(s)\n",
      "étape 109 ; nombre de k dans l'étape : 590;  N = 19943; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 110 ; nombre de k dans l'étape : 514;  N = 20457; Execution time: 0 minute(s) and 16 second(s)\n",
      "étape 111 ; nombre de k dans l'étape : 251;  N = 20708; Execution time: 0 minute(s) and 11 second(s)\n",
      "étape 112 ; nombre de k dans l'étape : 563;  N = 21271; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 113 ; nombre de k dans l'étape : 974;  N = 22245; Execution time: 0 minute(s) and 33 second(s)\n",
      "étape 114 ; nombre de k dans l'étape : 445;  N = 22690; Execution time: 0 minute(s) and 13 second(s)\n",
      "étape 115 ; nombre de k dans l'étape : 820;  N = 23510; Execution time: 0 minute(s) and 27 second(s)\n",
      "étape 116 ; nombre de k dans l'étape : 686;  N = 24196; Execution time: 0 minute(s) and 25 second(s)\n",
      "étape 117 ; nombre de k dans l'étape : 596;  N = 24792; Execution time: 0 minute(s) and 21 second(s)\n",
      "étape 118 ; nombre de k dans l'étape : 653;  N = 25445; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 119 ; nombre de k dans l'étape : 786;  N = 26231; Execution time: 0 minute(s) and 25 second(s)\n",
      "étape 120 ; nombre de k dans l'étape : 916;  N = 27147; Execution time: 0 minute(s) and 31 second(s)\n",
      "étape 121 ; nombre de k dans l'étape : 1088;  N = 28235; Execution time: 0 minute(s) and 38 second(s)\n",
      "étape 122 ; nombre de k dans l'étape : 622;  N = 28857; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 123 ; nombre de k dans l'étape : 955;  N = 29812; Execution time: 0 minute(s) and 29 second(s)\n",
      "étape 124 ; nombre de k dans l'étape : 520;  N = 30332; Execution time: 0 minute(s) and 15 second(s)\n",
      "étape 125 ; nombre de k dans l'étape : 1098;  N = 31430; Execution time: 0 minute(s) and 40 second(s)\n",
      "étape 126 ; nombre de k dans l'étape : 684;  N = 32114; Execution time: 0 minute(s) and 22 second(s)\n",
      "étape 127 ; nombre de k dans l'étape : 355;  N = 32469; Execution time: 0 minute(s) and 11 second(s)\n",
      "étape 128 ; nombre de k dans l'étape : 926;  N = 33395; Execution time: 0 minute(s) and 28 second(s)\n",
      "étape 129 ; nombre de k dans l'étape : 845;  N = 34240; Execution time: 0 minute(s) and 26 second(s)\n",
      "étape 130 ; nombre de k dans l'étape : 792;  N = 35032; Execution time: 0 minute(s) and 26 second(s)\n",
      "étape 131 ; nombre de k dans l'étape : 554;  N = 35586; Execution time: 0 minute(s) and 21 second(s)\n",
      "étape 132 ; nombre de k dans l'étape : 844;  N = 36430; Execution time: 0 minute(s) and 27 second(s)\n",
      "étape 133 ; nombre de k dans l'étape : 390;  N = 36820; Execution time: 0 minute(s) and 14 second(s)\n",
      "étape 134 ; nombre de k dans l'étape : 343;  N = 37163; Execution time: 0 minute(s) and 11 second(s)\n",
      "étape 135 ; nombre de k dans l'étape : 409;  N = 37572; Execution time: 0 minute(s) and 14 second(s)\n",
      "étape 136 ; nombre de k dans l'étape : 508;  N = 38080; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 137 ; nombre de k dans l'étape : 834;  N = 38914; Execution time: 0 minute(s) and 27 second(s)\n",
      "étape 138 ; nombre de k dans l'étape : 428;  N = 39342; Execution time: 0 minute(s) and 14 second(s)\n",
      "étape 139 ; nombre de k dans l'étape : 694;  N = 40036; Execution time: 0 minute(s) and 22 second(s)\n",
      "étape 140 ; nombre de k dans l'étape : 998;  N = 41034; Execution time: 0 minute(s) and 34 second(s)\n",
      "étape 141 ; nombre de k dans l'étape : 152;  N = 41186; Execution time: 0 minute(s) and 9 second(s)\n",
      "étape 142 ; nombre de k dans l'étape : 962;  N = 42148; Execution time: 0 minute(s) and 34 second(s)\n",
      "étape 143 ; nombre de k dans l'étape : 604;  N = 42752; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 144 ; nombre de k dans l'étape : 592;  N = 43344; Execution time: 0 minute(s) and 21 second(s)\n",
      "étape 145 ; nombre de k dans l'étape : 1090;  N = 44434; Execution time: 0 minute(s) and 39 second(s)\n",
      "étape 146 ; nombre de k dans l'étape : 705;  N = 45139; Execution time: 0 minute(s) and 22 second(s)\n",
      "étape 147 ; nombre de k dans l'étape : 191;  N = 45330; Execution time: 0 minute(s) and 9 second(s)\n",
      "étape 148 ; nombre de k dans l'étape : 592;  N = 45922; Execution time: 0 minute(s) and 17 second(s)\n",
      "étape 149 ; nombre de k dans l'étape : 444;  N = 46366; Execution time: 0 minute(s) and 15 second(s)\n",
      "étape 150 ; nombre de k dans l'étape : 565;  N = 46931; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 151 ; nombre de k dans l'étape : 399;  N = 47330; Execution time: 0 minute(s) and 15 second(s)\n",
      "étape 152 ; nombre de k dans l'étape : 450;  N = 47780; Execution time: 0 minute(s) and 13 second(s)\n",
      "étape 153 ; nombre de k dans l'étape : 771;  N = 48551; Execution time: 0 minute(s) and 26 second(s)\n",
      "étape 154 ; nombre de k dans l'étape : 535;  N = 49086; Execution time: 0 minute(s) and 18 second(s)\n",
      "étape 155 ; nombre de k dans l'étape : 998;  N = 50084; Execution time: 0 minute(s) and 35 second(s)\n"
     ]
    }
   ],
   "source": [
    "N = 0\n",
    "nb_ks = []\n",
    "Cs = []\n",
    "while N <= 50000:\n",
    "    t1 = time.time()\n",
    "    instance = Calcul_C(prompts=cleaned_text_data[i*5:i*5+5], model_name= model_name)\n",
    "    nbk = instance.nb_ks\n",
    "    nb_ks.append(nbk)\n",
    "    instance.run()\n",
    "    Cs.append(instance.C)\n",
    "    N+=nbk\n",
    "    i+=1\n",
    "    t2 = time.time()\n",
    "    elapsed = t2 - t1\n",
    "    minutes = int(elapsed // 60)\n",
    "    seconds = int(elapsed % 60)\n",
    "    print(f\"étape {i} ; nombre de k dans l'étape : {nbk};  N = {N}; Execution time: {minutes} minute(s) and {seconds} second(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sum = sum(C * nbk for C, nbk in zip(Cs, nb_ks))\n",
    "C_gpt_xl = weighted_sum / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata2 = {\n",
    "    'model_name': model_name,\n",
    "    'number_of_ks': N\n",
    "}\n",
    "checkpoint = torch.load('C.pt')\n",
    "checkpoint['C2'] = C_gpt_xl\n",
    "checkpoint['Metadata2'] = metadata2\n",
    "torch.save(checkpoint, 'C.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('C.pt')\n",
    "C100 = (checkpoint['Metadata']['number_of_ks']*checkpoint['C']+checkpoint['Metadata2']['number_of_ks']*checkpoint['C2'])/(checkpoint['Metadata']['number_of_ks']+checkpoint['Metadata2']['number_of_ks'])\n",
    "dico1 = {'C':C100, 'model_name': model_name, 'number_of_ks': checkpoint['Metadata']['number_of_ks']+checkpoint['Metadata2']['number_of_ks'] }\n",
    "dico2 = {'C':checkpoint['C'], 'model_name': model_name, 'number_of_ks': checkpoint['Metadata']['number_of_ks'] }\n",
    "dico3 = {'C':checkpoint['C2'], 'model_name': model_name, 'number_of_ks': checkpoint['Metadata2']['number_of_ks'] }\n",
    "data = {1 : dico1, 2: dico2 , 3 : dico3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, 'C.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On as ainsi calculé la matrice C. On peut accéder à C comme suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'C': tensor([[0.0299, 0.0021, 0.0013,  ..., 0.0018, 0.0012, 0.0019],\n",
       "          [0.0021, 0.0178, 0.0044,  ..., 0.0039, 0.0018, 0.0044],\n",
       "          [0.0013, 0.0044, 0.0311,  ..., 0.0040, 0.0023, 0.0043],\n",
       "          ...,\n",
       "          [0.0018, 0.0039, 0.0040,  ..., 0.0302, 0.0033, 0.0042],\n",
       "          [0.0012, 0.0018, 0.0023,  ..., 0.0033, 0.0400, 0.0019],\n",
       "          [0.0019, 0.0044, 0.0043,  ..., 0.0042, 0.0019, 0.0137]]),\n",
       "  'model_name': 'gpt2-xl',\n",
       "  'number_of_ks': 100254},\n",
       " 2: {'C': tensor([[0.0292, 0.0023, 0.0014,  ..., 0.0017, 0.0011, 0.0020],\n",
       "          [0.0023, 0.0184, 0.0043,  ..., 0.0038, 0.0018, 0.0044],\n",
       "          [0.0014, 0.0043, 0.0303,  ..., 0.0039, 0.0023, 0.0044],\n",
       "          ...,\n",
       "          [0.0017, 0.0038, 0.0039,  ..., 0.0319, 0.0035, 0.0042],\n",
       "          [0.0011, 0.0018, 0.0023,  ..., 0.0035, 0.0372, 0.0021],\n",
       "          [0.0020, 0.0044, 0.0044,  ..., 0.0042, 0.0021, 0.0133]]),\n",
       "  'model_name': 'gpt2-xl',\n",
       "  'number_of_ks': 50170},\n",
       " 3: {'C': tensor([[0.0306, 0.0020, 0.0011,  ..., 0.0019, 0.0013, 0.0018],\n",
       "          [0.0020, 0.0172, 0.0044,  ..., 0.0040, 0.0017, 0.0044],\n",
       "          [0.0011, 0.0044, 0.0319,  ..., 0.0041, 0.0024, 0.0043],\n",
       "          ...,\n",
       "          [0.0019, 0.0040, 0.0041,  ..., 0.0285, 0.0031, 0.0043],\n",
       "          [0.0013, 0.0017, 0.0024,  ..., 0.0031, 0.0429, 0.0018],\n",
       "          [0.0018, 0.0044, 0.0043,  ..., 0.0043, 0.0018, 0.0141]]),\n",
       "  'model_name': 'gpt2-xl',\n",
       "  'number_of_ks': 50084}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.load('C.pt')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0299, 0.0021, 0.0013,  ..., 0.0018, 0.0012, 0.0019],\n",
       "        [0.0021, 0.0178, 0.0044,  ..., 0.0039, 0.0018, 0.0044],\n",
       "        [0.0013, 0.0044, 0.0311,  ..., 0.0040, 0.0023, 0.0043],\n",
       "        ...,\n",
       "        [0.0018, 0.0039, 0.0040,  ..., 0.0302, 0.0033, 0.0042],\n",
       "        [0.0012, 0.0018, 0.0023,  ..., 0.0033, 0.0400, 0.0019],\n",
       "        [0.0019, 0.0044, 0.0043,  ..., 0.0042, 0.0019, 0.0137]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]['C']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
